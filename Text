 ECE 610 Machine Learning &
Artificial Intelligence
Spring 2024 David Han
Electrical and Computer Engineering Drexel University

 ECE 610 Machine Learning & AI
â€¢ Instructor: David Han (dkh42@drexel.edu)
â€¢ Lecture Time: Tue. & Thr. 3:30 â€“ 4:50 PM
â€¢ Location:Papadakisroom104
â€¢ Office Hour: Wed. 11:00 AM â€“ 12:00 PM
â€¢ Location:BossoneHallroom513b
   
 University Academic Policies
â€¢ Course Change Policy: The instructor reserves the right to modify the course, as necessary, during the term: including policies, evaluations, due dates, course content, schedule, assignments or requirements. All changes will be communicated in lecture and/or via the course DrexelLearn page.
â€¢ Please review the following relevant university policies.
â€¢ Academic Integrity, Plagiarism, Dishonesty and Cheating Policy:
â€¢ http://drexel.edu/provost/policies/absence/
â€¢ Disability Resources: Students requesting accommodations due to a disability at Drexel University need to request a current Accommodations Verification Letter (AVL) in the ClockWork database before accommodations can be made. These requests are received by Disability Resources (DR), who then issues the AVL to the appropriate contacts. For additional information, visit the DR website at http://drexel.edu/oed/disabilityResources/overview/, or contact DR for more information by phone at 215.895.1401, or by email at disability@drexel.edu.
â€¢ Course Add/Drop Policy: http://www.drexel.edu/provost/policies/course-add-drop
â€¢ Course WithdrawalPolicy: http://drexel.edu/provost/policies/course-withdrawal
     
 Purpose of this course
â€¢ To gain in depth conceptual understanding of modern machine learning methods
â€¢ To gain hands on knowledge of writing basic python-based machine learning codes for solving some representative problems.
â€¢ To obtain key knowledge in machine learning in preparation for graduate level research
â€¢ Required background: basic concepts in probability, linear algebra, and elementary python

   Course Textbook
â€¢ There is no single source for the course material
â€¢ Earlier part of the course will be from â€œPattern Classificationâ€ by Duda, Hart, and Stork
â€¢ Not necessary to purchase, mostly will be in lecture notes
â€¢ Some material will be from â€œHands-on Machine Learning with Scikit-Learn, Keras & TensorFlow,â€ by Aurelien Geron
â€¢ Available online from Drexel Library
â€¢ Select material will be from â€œDeep Learning,â€ by Ian Goodfellow, Yoshua Gengio, and Aaron Courville
â€¢ Available online from Drexel Library
â€¢ Additionally, each lecture will be accompanied with lecture slides, which serve as the primary resource for the course
â€¢ Lecture slides will be posted on blackboard afterwards
 
 Course Syllabus
â€¢ We will use the BBLearn system to deliver all information regarding the course, including assignment descriptions, links to online materials, and grades as they are calculated.
â€¢ Your assignments are to be submitted through this system in a manner consistent with each assignment description. However, emails should not be sent through this system, but rather directly to the instructor at the email addresses above.
â€¢ Assessment Overview â€¢ 5% - Class attendance â€¢ 25% - Assignments
â€¢ 20% - Midterm Exam
â€¢ 25% - Final Exam
â€¢ 25% - Paper report presentation

 Course Outline
 â€¢ AI landscape
â€¢ Linear Algebra & Probability review â€¢ NaÃ¯ve Bayes Classifier
â€¢ Learning Loss
â€¢ Artificial Neural Network
â€¢ Multi-layer Perceptron
â€¢ Convolutional Neural Network
â€¢ Sequence Models
â€¢ Attention Mechanism
â€¢ Generative Models
â€¢ Reviews of contemporary ML papers

 Machine Learning is every where!
â€¢ Languagetranslationservices(Google)
â€¢ Speech translation (Google Translate)
â€¢ Songrecognition(Shazam)
â€¢ Facerecognition(Recognizr)
â€¢ Question answering (Google Home, Amazon Alexa, Apple Siri, IBM Watson)
â€¢ Board games (IBM Deep Blue, Google DeepMind AlphaGo)
â€¢ Driverless cars (Uber, Tesla, Google)
â€¢ Deep Fake
          
 Semantic Segmentation for Driverless Car Application
Zhao, et. al, â€œICNet for Real-Time Semantic Segmentation on High-Resolution Imagesâ€ https://hszhao.github.io/projects/icnet/

 Also some filters
 YouTube, Guardian News, â€˜Iâ€™m not a catâ€™: lawyer gets stuck on Zoom kitten filter during court case, https://www.youtube.com/watch?v=lGOofzZOyl8

 Some Exciting New Development
â€¢ ChatGPT: Large Language Model (LLM)
â€¢ Example:
â€¢ User: Explain how a combustion engine works
â€¢ ChatGPT: â€œA combustion engine works by converting the energy stored in fuel into mechanical energy. This is typically done through a four-stroke process in which fuel is mixed with air and then ignited in the combustion chamber. The resulting explosion drives a piston, which moves the engineâ€™s crankshaft and converts the linear motion of the piston into the rotational motion of the crankshaft. This rotational motion can then be used to power a vehicle or other machinery.â€
â€¢ User: Who won the F1 championship in 2021?
â€¢ ChatGPT: â€œIâ€™m sorry, but I am a large language model trained by OpenAI and I do not have access to current information. My training only goes up until 2021, so I am unable to provide information about events that have occurred after that. My knowledge is limited to what I was trained on and I do not have the ability to browse the internet or access other sources of information. Is there anything else I may be able to help you with?

 ChatGPT
â€¢ ChatGPT task examples:
â€¢ Speech and text analysis
â€¢ Translations
â€¢ Explanations of complex issues â€¢ Writing stories and essays
â€¢ Learn coding
â€¢ Debugging code

 What is Machine Learning?
â€¢ Science (and art) of programming computers so they can learn from data.
â€¢ A study that gives computers the ability to learn without being explicitly programmed. â€”Arthur Samuel, 1959
â€¢ A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. â€”Tom Mitchell, 1997

 What is Machine Learning?
â€¢ Science (and art) of programming computers so they can learn from data.
â€¢ A study that gives computers the ability to learn without being explicitly programmed. â€”Arthur Samuel, 1959
â€¢ A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. â€”Tom Mitchell, 1997

 What is Machine Learning?
â€¢ Science (and art) of programming computers so they can learn from data.
â€¢ A study that gives computers the ability to learn without being explicitly programmed. â€”Arthur Samuel, 1959
â€¢ A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. â€”Tom Mitchell, 1997

 AI vs Machine Learning vs Deep Learning
Atul 2021, AI vs Machine Learning vs Deep Learning, https://www.edureka.co/blog/ai-vs-machine-learning-vs-deep-learning/

 Artificial Intelligence Timeline
Irina Nik, 2020, â€œWhat is artificial intelligence? A simple explanation for designersâ€

 Good Old Fashioned Artificial Intelligence (GOFAI) vs Machine Learning
 Rule Based Approach

 Good Old Fashioned Artificial Intelligence (GOFAI) vs Machine Learning
    Rule Based Approach

 Good Old Fashioned Artificial Intelligence (GOFAI) vs Machine Learning
     Rule Based Approach
Learning Based Approach

 Good Old Fashioned Artificial Intelligence (GOFAI) vs Machine Learning
      Rule Based Approach
Learning Based Approach

 Rule based approach vs ML approach
Rule Based Approach (Good Old Fashion AI)
â€¢ Deterministic
â€¢ Can operate with simple basic
information
â€¢ Hard to scale as problem complexity grows
â€¢ Rules are implemented manually â”Knowledge Engineering
Learning Based Approach
â€¢ Probabilistic
â€¢ Constantly evolve, develop and adapt its production in accordance with training information streams
â€¢ Learnsrulesfromdata
â€¢ Easily scaled
â€¢ Requires more data for better performance

 Rule Based AI: Expressing human knowledge in the
form of rules for problem solving
â€¢ Rule can be defined as an IF-THEN structure that relates given information or facts in the IF part to some action in the THEN part. A rule provides a soluttion to a problem
â€¢ Any rule consists of two parts: the IF part, called the antecedent (premise or condition) and the THEN part called the consequent (conclusion or action).

 Rules as knowledge representation
IF the â€™traffic lightâ€™ is green THEN the action is go
     IF the â€™traffic lightâ€™ is red THEN the action is stop
Netflix, Red Light, Green Light Game â€“ Squid Game (2021)

 Learning Based Approach
Cars
Motorcycles
Motordynasty (2020), â€œTop of the line Cruisers in September 2020,â€ https://motordynasty.com/2020/10/28/top-of-the-line-cruisers-in- september-2020/
 How can a machine tell the difference between cars and motocycles?
   By extracting distinct features of each class
Motor Trend (2019), â€œThe 15 Most Important Cars of the Decade,â€ https://www.motortrend.com/features/the-15-most-important-cars-of- the-decade/
 Traditional ML based Classification
  Image/Video/ Pixels
Hand-crafted feature extraction
Classifier trained on data containing extracted features
   Object class

 ML Process Before Deep Learning (BDL)
â€¢ Components of a basic machine learning system:
â€¢ Sensor(s)
â€¢ Preprocessor & Feature extraction (manual or automated)
â€¢ Classification algorithm
â€¢ Training set examples already classified or described
 Role of Machine
Classification algorithm
    Role of Human domain expert knowledge
   Rotation
Output Estimation
 
 ML Process Before Deep Learning (BDL)
â€¢ Components of a basic machine learning system:
â€¢ Sensor(s)
â€¢ Preprocessor & Feature extraction (manual or automated)
â€¢ Classification algorithm
â€¢ Training set examples already classified or described
 Role of Machine
Classification algorithm
    Role of Human domain expert knowledge
   Rotation
Output Estimation
 
 ML Process Before Deep Learning (BDL)
â€¢ Components of a basic machine learning system:
â€¢ Sensor(s)
â€¢ Preprocessor & Feature extraction (manual or automated)
â€¢ Classification algorithm
â€¢ Training set examples already classified or described
 Role of Machine
Classification algorithm
    Role of Human domain expert knowledge
   Rotation
Output Estimation
 
 ML Process Before Deep Learning (BDL)
â€¢ Components of a basic machine learning system:
â€¢ Sensor(s)
â€¢ Preprocessor & Feature extraction (manual or automated)
â€¢ Classification algorithm
â€¢ Training set examples already classified or described
 Role of Machine
Classification algorithm
    Role of Human domain expert knowledge
   Rotation
Output Estimation
 
  Classical to Modern AI
â€¢ Differences and similarities of AI approaches
â€¢ Note shaded components learn from data
Goodfellow, et. al. (2016), â€œDeep Learning,â€ MIT press

  Classical to Modern AI
â€¢ Differences and similarities of AI approaches
â€¢ Note shaded components learn from data
Goodfellow, et. al. (2016), â€œDeep Learning,â€ MIT press

  Classical to Modern AI
â€¢ Differences and similarities of AI approaches
â€¢ Note shaded components learn from data
Goodfellow, et. al. (2016), â€œDeep Learning,â€ MIT press

 Features
â€¢ Feature is any distinctive aspect, quality or characteristic â€¢ Can be categorical (i.e., color) or numeric (i.e., height)
â€¢ Definitions
â€¢ Thecombinationofğ‘‘featuresisağ‘‘-dimcolumnvectorcalledafeature
vector
â€¢ Theğ‘‘-dimensionalspacedefinedbythefeaturevectoriscalledthefeature space
â€¢ Objectsarerepresentedaspointsinfeaturespace;theresultisascatter plot
  
 Features
â€¢ What makes a â€œgoodâ€ feature vector?
â€¢ The quality of a feature vector is related to its ability to discriminate
examples from different classes
â€¢ Examples from the same class should have similar feature values â€¢ Examples from different classes have different feature values
â€¢ More feature properties
  
 Machine Learning: Classification Task
 Sorting incoming fish on a conveyor according to species (salmon or sea bass) using optical sensing
 Salmon or sea bass? (2 categories or classes)
Duda, et. al. (1973), Pattern Classification, 2nd Ed.

 Binary Classification
â€¢ Use both width and average lightness features for classification. Use a boundary to discriminate given feature space x = (x x )
  Width
12
  Width
    sea bass
Duda, et. al. (1973), Pattern Classification, 2nd Ed.
sea bass
  salmon
salmon
  Lightness
Lightness

 Binary Classification
â€¢ Decision boundary with good generalization
  Duda, et. al. (1973), Pattern Classification, 2nd Ed.

  3+1 Types of Machine Learning
  Supervised learning
                                    Unsupervised learning
                                                                                Reinforcement learning
                                          Semi-supervised learning
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 12

 Credit: Univ. Virginia CS4501
Classification (Recognition)
(Supervised Classification)
Clustering
(Unsupervised Classification)

 Credit: Univ. Virginia CS4501
Classification (Recognition)
(Supervised Classification)
Clustering
(Unsupervised Classification)

 Supervised Learning
apple
pear
tomato cow
dog horse
f(   ) = â€œappleâ€ f(   ) = â€œtomatoâ€ f(   ) = â€œcowâ€
 y = f(x) prediction
   output
Image function
Training set
â€¢ Training: given a training set of labeled examples {(x1,y1), ..., (xN,yN)}, estimate the prediction
function f by minimizing the prediction error on the training set
â€¢ Testing: apply f to a never before seen test example x and output the predicted value y = f(x) â€¢ Supervised learning is expensive as it requires both input and the label
Machine Learning Crash Course Photo CMU Machine Learning, https://slidetodoc.com/machine-learning-crash-course-photo-cmu-machine-learning/

 Supervised Training Steps
Training
Training Images
Training Labels
Learning model
       Image Features
Learned model
Testing
    Test Image
Image Learned Features model
Prediction
Machine Learning Crash Course Photo CMU Machine Learning, https://slidetodoc.com/machine-learning-crash-course-photo-cmu-machine-learning/

 Generalization
  apple
pear
tomato cow
dog horse
â€¢ How well does a learned model generalize from the data it was trained on to a new test set?
Training set
Test set (previously unseen)
Machine Learning Crash Course Photo CMU Machine Learning, https://slidetodoc.com/machine-learning-crash-course-photo-cmu-machine-learning/

  Unsupervised
â€œWeaklyâ€ supervised Fully supervised
Definition depends on task
 Machine Learning Crash Course Photo CMU Machine Learning, https://slidetodoc.com/machine-learning-crash-course-photo-cmu-machine-learning/

 Self Supervised Learning
â€¢ A machine learning process where the model trains itself to learn one part of the input from another part of the input.
â€¢ Also known as predictive or pretext learning.
â€¢ Examples:
â€¢ Grab a text from some existing source
â€¢ Predict any part of the input from any other part
â€¢ Predict the future from the past
â€¢ Predict the future from the recent past
â€¢ Predict the past from the present
â€¢ Predict the top from the bottom
â€¢ Predict the occluded from the visible
Men in sandals push bicycles overloaded with bags of coal down the highway, while on the back roads close to Hazaribagh, women carry heads. of the stuff on their
Men in sandals push bicycles overloaded with bags of coal down the highway, while on the back roads close to Hazaribagh, women carry heads. of the stuff on their
Output: buckets
 buckets
  buckets
  Input

Machine Learning Algorithms
NaÃ¯ve Bayes
Bayesian network
K-nearest neighbor
SVM
Logistic regression
Neural networks Reinforcement Learning Randomized Forests Boosted Decision TreesRBMs Etc.
             Suggested reading for entertainment: â€œThe Master Algorithmâ€ by Pedro Domingos
 Pedro Domingo (2015), â€œMaster Algorithmâ€
 
 The Five Tribes by Pedro Domingos
Pedro Domingo (2015), â€œMaster Algorithmâ€

 Symbolist: Decision Trees
Should I wait at this restaurant?
  
 Analogizer: Nearest Neighbor Approach
â€¢ If it walks like a duck, quacks like a duck, then itâ€™s probably a duck
Training Records
Choose k of the â€œnearestâ€ records
Compute Distance
Test Record

 Analogizer: Kth Nearest Neighbor (KNN)
â€¢ Majority vote within the kth Nearest Neighbors (KNN)
new
    K= 1: blue K= 3: green

 Analogizer: Kth Nearest Neighbor (KNN)
1-nearest neighbor
3-nearest neighbor
  xx xx
x oxx x oxx xx
o+o xx o+o xx oo+ oo+
oo x2o x2o
x1 x1
     
 Analogizer: Suport Vector Machine (SVM)
r=wTxi +b â€¢ Distance from example xi to the separator is w
â€¢ Examples closest to the hyperplane are support vectors.
â€¢ Margin Ï of the separator is the distance between support vectors.
Ï r
         
 Evolutionaries
â€¢ Algorithm selected by an evolutionary process (Genetic Algorithm)
  â€¢ The process as a whole is typically slow.
GLOWZYME, , â€œSELEX Optimization with Machine Learning,â€ http://2017.igem.org/Team:McMasterU/Genetic

 Evolutionaries
â€¢ Algorithm selected by an evolutionary process (Genetic Algorithm)
  â€¢ The process as a whole is typically slow.
GLOWZYME, , â€œSELEX Optimization with Machine Learning,â€ http://2017.igem.org/Team:McMasterU/Genetic

 Evolutionaries
â€¢ Algorithm selected by an evolutionary process (Genetic Algorithm)
  â€¢ The process as a whole is typically slow.
GLOWZYME, , â€œSELEX Optimization with Machine Learning,â€ http://2017.igem.org/Team:McMasterU/Genetic

 Evolutionaries
â€¢ Algorithm selected by an evolutionary process (Genetic Algorithm)
  â€¢ The process as a whole is typically slow.
GLOWZYME, , â€œSELEX Optimization with Machine Learning,â€ http://2017.igem.org/Team:McMasterU/Genetic

 Bayesian: Probabilistic approach
â€¢ Bayesâ€™ theorem: Outputs the most probable hypothesis wâˆˆW, given the data x and knowledge about prior probabilities of hypotheses in w
p (w | x ) = p (x |w )p (w ) p(x)
â€¢ p(w|x) : posterior probability of w. probability that w holds given data x; confidence that w holds given x
â€¢ p(w) : prior probability of w. Background knowledge we have about that w is a correct hypothesis
â€¢ p(x) : prior probability that training data x will be observed
â€¢ p(x|w) : likelihood of x. probability of observing x given w holds
Thomas Bayes 1701 â€“ 7 April 1761
  Terence O'Donnell, History of Life Insurance in Its Formative Years (Chicago: American Conservation Co:, 1936), p. 335 (caption "Rev. T. Bayes: Improver of the Columnar Method developed by Barrett."

Bayesian
â€¢ Bayes theorem allows converting a priori estimate to posteriori (e.g. measurement conditioned) probability density function (pdf) of state of nature via:
P(salmon | width) = [ p(width | salmon)P(salmon)] p(width)
   From the training set, Posterior Probability of salmon for a given width can be generated
P(salmon | width)
x
x

   â€¢ A neuron has
â€¢ Branchinginput(dendrites) â€¢ Branchingoutput(theaxon)
â€¢ Information moves from the dendrites to the axon via the cell body
â€¢ Axon connects to dendrites via synapses â€¢ Synapsesvaryinstrength
â€¢ Synapses may be excitatory or inhibitory
Neves, et. al. (2018), â€œA new approach to damage detection in bridges using machine learningâ€
Connectionist: Neuronal Network Approach

 Basic Perceptron
(Frank Rosenblatt, 1950s and early 60s)
  ï‚— An Artificial Neuron (AN) is a non-linearparameterized function with restricted output range
Ã¬ Ã¦Ã¥ O=Ã¯Ã­ 1:Ã§Ã¨
Ã¶ Ã¼ wixiÃ·Ã¸+b>0 Ã¯Ã½
Ã¯ i
Ã® 0 : otherwise Ã¾
Ã¯

 Activation Functions â€“ Sigmoid Function
 Recent successes (e.g., Baidu Deep Speech) use (clipped) Rectifier Linear Units:
f(x) = max(0,x) -- rectifier f(x) = min(max(0,x), clip)
 
 Neural Network for Classification
 â€¢ Separates different classes
â€¢ Logistic Regression (Perceptron)
 Linear Regression

 For More Complex Boundaries
â€¢ More complex boundaries can be constructed using multilayer perceptrons
â€¢ Add additional linear boundaries by adding a hidden layer
  Multi-layer Perceptron

 Deep Learning/Deep Neural Net (DNN)
â€¢ Not Possible Previously Because:
â€¢ Requires a large set of data
â€¢ Involves a large set of weight computation
â€¢ Vanishing Gradient Problem
  
 Convolutional Neural Net
Lecun, et. al. (1998), â€œGradient-based learning applied to document recognitionâ€

 Machine Learning Tasks
Classification
Regression
    â€œWhere Deep Learning Meets GISâ€ by Rohit Singh, June 2019
â€œCreating a Model for Weather Forecasting Using Linear Regressionâ€ by Ashan Lakmal

  Review of Probability and Statistics
ECE 610
David Han Drexel University
   
  Probability in our lives
     StudiousGuy, â€œ8 Real Life Examples of Probability,â€ https://studiousguy.com/8-real us?â€ https://www.theguardian.com/us-news/2020/jun/02/restaurants-reopening-eating-out-co -life-examples-of-probability/ ronavirus-safety
The Guardian (2020), â€œAs restaurants reopen, what will eating out be like in the age of coronavir

  Basic Probability Concepts
q Definitions (informal)
q Probabilities are numbers assigned to events that indicate â€œhow likelyâ€ it
is that the event will occur when a random experiment is performed q The sample space S of a random experiment is the set of all possible
outcomes
Sample space
   q Axioms of probability qAxiomI: 0Â£P[Ai]
P[S]=1
q Axiom III :if A Ã‡A =f,thenP[A ÃˆA ]=P[A]+P[A ]
q Axiom II :
ijijij
Probability Law
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Joint Probability
q Assume that there are two events, A and B, on the sample space S. q Joint probability
â–  The probability for two events which intersect in the sample space. (events that occur together)
â–  Example: a deck of cards
P(Aï‰B)= P(A)+P(B)-P(Aï•B)
Ã› P(Aï• B) = P(A) + P(B) - P(Aï‰ B)Â£ P(A) + P(B)
  S
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  More Properties of Probability
qProperty1: q Property 2 : q Property 3 : q Property 4 : q Property 5 : q Property 6 :
P[AC]=1-P[A] P[A]Â£1
P[f]=0
given{A,A,...,A},if{AÃ‡A=f,"i,j},thenP A = P[A]
12Nij Ãªï• Ã«k=1
kÃºÃ¥ Ã» k=1
k
P[A ÃˆA ]=P[A]+P[A ]-P[A Ã‡A ] 121212
if A ÃŒ A , then P[ A ] Â£ P[ A ] 1212
$
Note:
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill
! ğ´! â‰¡ ğ´# âˆª ğ´% âˆª â‹¯ .âˆª ğ´$ !"#
Ã©NÃ¹N

  Probability as a Relative Frequency
q Probability is defined as a â€œrelative frequencyâ€ of occurrence of some event
q If the â€œfairâ€ coin is flipped many times (say n) and heads shows up nH times out of the n flips, then
lim(nH /n)=P(H) nÂ®Â¥
q Here, the ratio (nH /n) is the relative frequency (or average number of successes) for this event.
q statistical regularity
â–  relative frequenciesÃ¨a fixed value (a probability) as nÃ¨
large.
â–  Based on observations
 Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill
6

  Basic Probability Concepts
q An experiment consist of obtaining a number x by spinning the pointer on a â€œfairâ€ wheel of chance that is labeled from 0 to 100
points.
S = {0 < x Â£ 100}
q Probability of the pointer falling between any two numbers
x2â‰¥x1
q Axiom 1 : Because of x2â‰¥x1 , Ok!
q Axiom 2 : Applied to x2=100 and x1=0
q Axiom 3
â–  Break the wheelâ€™s periphery into N contiguous segment An
 â– 
P(x < x Â£ x )= x - x 1 2 100
 Ã¦NÃ¶NN1
PÃ§ï•A Ã·= P(A )= N =1=P(S)
Ã¥
Ã¨n=1 Ã¸n=1 n=1
n
n
Ã¥
21
 Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Basic Probability Concepts
q Observe the sum of the number showing up two dice that is thrown.
q Sample space
â–  Total 36(=62) points.
q Definition of events
A={sum=7}, B={8<sumÂ£11}, and C={10<sum} Aij ={sumforoutcome(i,j)=i+j}
Â¢
where i is row location, j is the column location in the table. q Probability
Ã¨i=1 Ã¸ i=1 Ã¦1Ã¶ 1
  Â¢
  Â¢Ã¦6Ã¶6Ã¦1Ã¶1 P(A)=PÃ§ï•A Ã·=Ã¥P(A )=6Ã§ Ã·=
i,7-i i,7-i 36 6
P(B)=9Ã§Ã¨36Ã·Ã¸= 4, P(C)=3Ã§Ã¨36Ã·Ã¸=12
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill
Ã¨ Ã¸ Ã¦1Ã¶ 1
8

  Examples
q There are 80 resistor.
P(draw 10Î©) = 18 / 80, P(draw 22Î©) = 12 / 80 P(draw 27Î©) = 33 / 80, P(draw 47Î©) = 17 / 80
q Suppose a 22 Î© resister is drawn from the box at first, what are the probabilities of drawing a second resister of any one of the four values?
â–  P(.| Î©) : The probability on the second drawing are now conditional on the outcome of the first drawing.
P(draw 10Î© | 22Î©) = 18 / 79, P(draw 22Î©|22Î©) = 11/ 79 P(draw 27Î©|22Î©) = 33 / 79, P(draw 47Î©|22Î©) = 17 / 79
  Ohm(Î©)
10 Î©
22 Î©
27 Î©
47 Î©
Num.
18
12
33
17
 â– 
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill
9

  Conditional Probability
q If A and B are two events, the probability of event A when we already know that event B has occurred is P[A|B]
â–  the â€œconditional probability of A conditioned on Bâ€, or simply
â–  the â€œprobability of A given Bâ€
q Interpretation
â–  The new evidence â€œB has occurredâ€ has the following effects
Â¢ The original sample space S (the square) becomes B (the rightmost circle) Â¢ The event A becomes A âˆ© B
â–  P[B] simply re-normalizes the probability of events that occur jointly with B
 Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill
P[A | B]= P[A Ã‡ B] for P[B] > 0 P[B]
  
  Conditional Probability Example
q Conditional probability in a card game
q Event B: hearts
q Event A: kings
â–  The conditional prob. of drawing a king, given the cards are all
hearts
   P(A|B)o P(Aï‰B) P(B)
 S
   Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill
11
P(A B)P(B)=P(AÃ‡B)
 
  Theorem of Total Probability
q Let B1, B2, ..., BN be a partition of S (mutually exclusive that totals to S) q Any event A can be represented as
A=AÃ‡S=AÃ‡(B ÃˆB ÃˆïŒÃˆB )=(AÃ‡B)Ãˆ(AÃ‡B )ÃˆïŒÃˆ(AÃ‡B ) 12N12N
q Since Bi are mutually exclusive, then P[A]=P[AÃ‡B]+P[AÃ‡B]+ïŒ+P[AÃ‡B ]
 12N
 P(A
 B)P(B) = P(AÃ‡B)
q Recall
qAnd,therefore P[A]=P[A|B]P[B]+ïŒ+P[A|B ]P[B ]
11 NN =Ã¥N P[A|Bk]P[Bk]
k =1 Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signa l Principles,â€ McGraw Hill

   Bayes Theorem
q Problem formulation
â–  Assume {B1, B2, ..., BN } is a partition of S.
â–  Given that event A occurs.
â–  What is the probability of event Bj ?
â–  Using the definition of conditional probability and the Theorem of total probability we obtain
P[B |A]=P[AÃ‡Bj]= P[A|Bj]P[Bj]
j
q This is known as Bayes Theorem or Bayes Rule, and is (one of) the most useful relations in probability and statistics
â–  Bayes Theorem is definitely the fundamental relation in Statistical Pattern Recognition
   P[A]
k=1
kk
N
Ã¥P[A|B ]P[B ]
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Bayes Theorem and Statistical Pattern Recognition
q When used for pattern classification, BT is generally expressed as P[w |x]= P[x|wj]P[wj] =P[x|wj]P[wj]
   j
k=1
N
P[x]
â–  where Ï‰j is the j-th class (e.g., car, motorcycle, ..) and x is the
feature/observation vector (e.g., image of a car, motorcycle, ...)
q A typical decision rule is to choose class Ï‰j with highest P[Ï‰j |x] â–  Intuitively, we choose the class that is more â€œlikelyâ€ given observation x
q Each term in the Bayes Theorem has a special name
â–  P[Ï‰j]
â–  P[Ï‰j | x]
â–  P[x | Ï‰j]
â–  P[x]
: Prior probability (of class Ï‰j)
: Posterior probability (of class Ï‰j given the observation x)
: likelihood (probability of observation x given class Ï‰j) : normalization constant (does not affect the decision)
Ã¥P[x|w ]P[w ] kk
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Exercise
q Consider a clinical problem where we need to decide if a patient has a particular medical condition on the basis of an imperfect test
â–  Someone with the condition may go undetected (false-negative)
â–  Someone free of the condition may yield a positive result (false-positive)
q Nomenclature
â–  The true-negative rate P(NEG|~COND) of a test is called its SPECIFICITY
â–  The true-positive rate P(POS|COND) of a test is called its SENSITIVITY
q Problem
â–  Assume a population of 10,000 with a 1% prevalence for the condition
â–  Assume that we design a test with 98% specificity and 90% sensitivity
â–  Assume you take the test, and the result comes out POSITIVE
â–  What is the probability that you have the condition(having the disease)?
q Solution
â–  Solution A : Fill in the joint frequency table next slide, or
â–  Solution B : Apply Bayes rule
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Exercise : Solution B
q Applying Bayes rule
Assume a population of 10,000 with a 1% prevalence for the condition
Assume that we design a test with 98% specificity (True Neg) and 90% sensitivity (Tru Pos) Assume you take the test, and the result comes out POSITIVE
P[COND|POS] = P[POS|COND]P[COND] P[POS]
= P[POS | COND]P[COND]
P[POS | COND]P[COND] + P[POS |~ COND]P[~ COND]
= 0.90  Ì 0.01
0.90  Ì 0.01+ (1- 0.98)  Ì 0.99
= 0.3125
     P[w |x]= P[x|wj]P[wj] =P[x|wj]P[wj]
  j Ã¥N P[x|w]P[w]
k=1
P[x]
kk
         Test is Positive
Test is Negative
Row Total
Has Condition
True-positive P(POS|COND) 100Ã—0.90
False-negative P(NEG|COND) 100Ã—(1-0.90)
100
Free of Condition
False-positive P(POS|~COND) 9,900Ã—(1-0.98)
True-negative P(NEG|~COND) 9,900Ã—0.98
9,900
Column Total
288
9,712
10,000
    Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Joint And Conditional Probability
q EXAMPLE: Binary communication system
q Send one of two possible symbol ( 0 or 1 ) over a channel.
 q Denote ( for i = 1,2 )
â–  Bi : â€œthe symbol before the channelâ€
â–  Ai : â€œthe symbol after the channelâ€
q Assume
INPUT
B1 : 0 B2 :1
OUTPUT
A1 : 0 A2 :1
     â– 
P(B)=0.6, P(A |B)=0.9, 11112
P(A |B )=0.1 P(B )=0.4, P(A |B)=0.1, P(A |B )=0.9
22122
q Calculate rest of probability. P(A)=P(A |B)P(B)+P(A |B )P(B )
 1111122 = 0.9(0.6) + 0.1(0.4) = 0.58
P(A)=P(A |B)P(B)+P(A |B)P(B) 2211222
= 0.1(0.6) + 0.9(0.4) = 0.42
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill
channel

  Joint And Conditional Probability
q EXAMPLE
q Conditional Probability
â–  Using the Bayesâ€™ theorem, â€“
INPUT
B1 : 0 B2 : 1
OUTPUT
P(B | A ) = P(A | B )P(B ) = 0.9(0.6) = 0.54 Â» 0.931 111
1 1 P(A ) 0.58 0.58 1
P(B |A)=P(A |B)P(B)=0.9(0.4)=0.36Â»0.857 222
2 2 P(A ) 0.42 0.42 2
â€“ P(A | B )P(B ) 0.1(0.6) 0.06 P(B|A)=21 1= =Â»0.143
1 2 P(A ) 0.42 0.42 2
P(B |A)=P(A|B)P(B)=0.1(0.4)=0.04Â»0.069 122
2 1 P(A ) 0.58 0.58 1
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill
A1 : 0 A2 : 1
A1 : 0 A2 : 1
    0.6
0.9
0.1
  0.4
0.9
0.1
             
  Independent Events
q Let two events A and B with nonzero probability. q (Statistically) Independent
q Definition
â–  The probability of occurrence of one event is not affected by the occurrence of the other event.
q Mathematical representation P(A| B) = P(A)
Ã› P(B | A) = P(B)
Ã› P(Aï‰ B) = P(A)P(B)
qIftwoeventsaremutuallyexclusive,
â–  So, two events cannot be both mutually exclusive and statistically
independent.
â–  i.e. For two events to be independent, they must have an intersection
  P(A|B)o P(Aï‰B) P(B)
 ğ‘·ğ‘¨âˆ©ğ‘© =ğ’â‰ ğ‘·ğ‘¨ğ‘·(ğ‘©)
  Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Independent Events
q EXAMPLE: One card is selected from an ordinary 52-card deck. q Define events
â–  A : â€œselect a kingâ€, B : â€œselect a jack or queenâ€, C : â€œselect a heartâ€ q Probabilities
â–  P(A) = 4/52, P(B)=8/52, P(C)=13/52 q Check the independence by pairs.
 â–  P(Aï‰B)=01P(A)P(B)= 32 522
P(Aï‰C)= 1 =P(A)P(C)= 1 52 52
P(Bï‰C)= 2 =P(B)P(C)= 2 52 52
Mutually Exclusive
 â–  So,AandCareindependentasapair,asareBandC. Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Independent Events
q EXAMPLE: Draw four cards from an ordinary 52-card deck. q Define events
â–  Ai : â€œdraw an ace consecutively i timesâ€ ( for i=1,2,3,4 )
q Check the independent events in the either replaced trials or not.
q Assuming that each trial is replaced.
 â–  They are independent.
q Assuming that each trial is not replaced.
=(4/52)4 Â»3.50(10-5) â–  They are not independent events.
P(A ï‰ A ï‰ A ï‰ A ) 1234
â‰  ğ‘ƒ(ğ´#) ğ‘ƒ(ğ´%) ğ‘ƒ(ğ´&) ğ‘ƒ(ğ´')
P(A ï‰ A ï‰ A ï‰ A ) = P(A )P(A )P(A )P(A ) 12341234
=P(A)P(A ï‰A ï‰A |A) 12341
=P(A)P(A |A)P(A ï‰A |A ï‰A ) 1213412
=P(A)P(A |A)P(A |Aï‰A)P(A |Aï‰A ï‰A) 1213124123
= 4 Ã— 3 Ã— 2 Ã— 1 Â»3.69(10-6) 52 51 50 49
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Independent Events
q EXAMPLE: A pair of fair dice is rolled, the values of their faces are added, and the following events are defined.
q Define events
â–  A: â€œroll an odd sumâ€
â–  B: â€œroll a sum that is an integer multiple of 3â€
a) Are events A and B statistically independent?
           Sum of faces
Probability
1,1
2
1/36
1,2
2,1
3
2/36
1,3
2,2
3,1
4
3/36
1,4
2,3
3,2
4,1
5
4/36
1,5
2,4
3,3
4,2
5,1
6
5/36
1,6
2,5
3,4
4,3
5,2
6,1
7
6/36
2,6
3,5
4,4
5,3
6,2
8
5/36
3,6
4,5
5,4
6,3
9
4/36
4,6
5,5
6,4
10
3/36
5,6
6,5
11
2/36
6,6
12
1/36
                                                           b) Note all the events are mutually exclusive. Total probability axiom applies.
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Independent Events
q Example (cont.)
q P(A)=2/36+4/36+6/36+4/36+2/36=18/36=1/2 q P(B) = 2/36+5/36+4/36+1/36=12/36=1/3
q P(AâŒ’B)=?
q Find events of AâŒ’BÃ¨Sum = odd & multiples of 3
q P(AâŒ’B) = 2/36 + 4/36 = 6/36 =1/6 q Is P(AâŒ’B) = P(A)P(B)?
â–  1/6 = 1/2Â·1/3
q Thus A & B are independent!
q What is probability of event A Ãˆ B?
q P(A Ãˆ B) = P(A) + P(B) â€“ P(A Ã‡ B)=P(A) + P(B) - P(A)P(B)
=1/2 + 1/3 - 1/6=2/3
 Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Random Variables
 q When we perform a random experiment we are usually interested in some measurement or numerical attribute of the outcome
â–  e.g., weights in a population of subjects, execution times when benchmarking CPUs, shape parameters when performing ATR
q These examples lead to the concept of random variable
q A random variable X is a function that assigns a real number X(ğœ‰)
to each outcome ğœ‰ in the sample space of a random experiment q X(ğœ‰) maps from all possible outcomes in sample space onto the
real line
   Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Random Variables
q The function that assigns values to each outcome is fixed and deterministic, i.e., as in the rule â€œcount the number of heads in three coin tossesâ€
â–  Randomness in it is due to the underlying randomness of the outcome ğœ‰ of the experiment
q Random variables can be
q Discrete, e.g., the resulting number after rolling a dice q Continuous, e.g., the weight of a sampled individual
 Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Random Variables
 q EXAMPLE 2.1-1
â–  An experiment consist of rolling a die and flipping a coin.
â–  (1) a coin head (H) outcome corresponds to positive values of X that are equal to numbers that show up on the die.
â–  (2) a coin tail (T) outcome correspond to negative values of X that are equal in magnitude to twice the number that shows on the die.
FIGURE 2.1-1
A Random variable mapping of a sample space.
 Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill
26

  Random Variables
 q EXAMPE 2.1-2
â–  An experiment where the pointer on a wheel of chance is spun.
â–  The possible outcomes are the numbers from 0 to 12 marked on the wheel. (The sample space : {0 < s Â£ 12} .
â–  a random variable : X=X(s)=s2
â–  Points in S now map onto the real line as the set .
{0<sÂ£144}
 Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill
27
FIGURE 2.1-2
Mapping applicable to Example 2.1-2.

    Cumulative Distribution Function (
q The cumulative distribution function FX(x) of a random variable X is defined as the probability of the event {X â‰¤ x}
FX(x)=P[XÂ£x] for-Â¥<x<+Â¥ q Properties of the cdf
â–  â–  â–  â–  â– 
cdf
)
  0Â£FX(x)Â£1 limFX (x)=1
 xÂ®Â¥ limFX(x)=0
xÂ®-Â¥
FX(a)Â£FX(b) if aÂ£b
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

    Probability Density Function (
q The probability density function fX(x) of a continuous random variable X, if it exists, is defined as the derivative of FX(x)
fX (x) = dFX (x) dx
q For discrete random variables, the equivalent to the pdf is the probability mass function
fX (x) = DFX (x) Dx
q Properties of the pdf â–  ğ‘“!(ğ‘¥)â‰¥0
-Â¥
pdf
)
       â–  P[a<x<b]=Ã²b fX(x)dx Ã²x a
â–  FX(x)=-Â¥fX(x)dx â–  Ã²Â¥ fX(x)dx=1
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Probability Density Function Vs. Probability
   Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Probability Density Function Vs. Probability
   Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Probability Density Function Vs. Probability
   Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Probability Density Function Vs. Probability
   Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Statistical Characterization of Random Variables
q The cdf or the pdf are SUFFICIENT to fully characterize a RV
q However, a RV can be PARTIALLY characterized with other measures q Expectation (center of mass of a density)
E[X]=Î¼=Ã²Â¥ xfX(x)dx -Â¥
q Variance (spread about the mean)
VAR[X]=s2 =E[(X-E[X])2]=Ã²Â¥ (x-Î¼)2 f (x)dx
-Â¥
q Standard deviation
STD[X]=s =VAR[X]1/2
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill
X

   Random
q An extension of the concept of a random variable
q A random vector X is a function that assigns a vector of real
numbers to each outcome ğœ‰ in sample space S
q Denote a random vector by a column vector
q The notions of cdf and pdf are replaced by â€˜joint cdfâ€™ and â€˜joint pdfâ€™
q Given random vector X = [x1, x2, ..., xN]T we define the joint cdf as
F(x)=P[{XÂ£x}Ã‡{XÂ£x}Ã‡ïŒÃ‡{X Â£x}] XX1122 NN
q And the joint pdf as fX(x)= Â¶NFX(x)
Â¶x Â¶x ïŒÂ¶x 12N
Vectors
 Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Statistical Characterization of Random Vectors
q A random vector is also fully characterized by its joint cdf or joint pdf
q Alternatively, we can (partially) describe a random vector with measures similar to those defined for scalar random variables
q Mean vector
E[X]=Î¼=[E[X],E[X],ïŒ,E[X ]]=[Î¼,Î¼,ïŒ,Î¼]
TT 12N12N
q Covariance matrix COV[X]= Î£ = E[(X-Î¼)(X-Î¼)T ]
Ã© E[(x-Î¼)2] ïŒ E[(x-Î¼)(x -Î¼ )]Ã¹ Ã©s2 ïŒ c Ã¹ Ãª11 11NNÃºÃª1 1NÃº
=ÃªïïïÃº=ÃªïïïÃº ÃªE[(x-Î¼)(x-Î¼)]ïŒ E[(x-Î¼)2] Ãº Ãªc ïŒs2Ãº
Ã«NN11 NNÃ»Ã«N1 NÃ» Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Covariance Matrix
q The covariance matrix indicates the tendency of each pair of features (dimensions in a random vector) to vary together, i.e., to co-vary*
q The covariance has several important properties
â–  If xi and xk tend to increase together, then cik > 0
â–  If xi tends to decrease when xk increases, then cik < 0
â–  If xi and xk are uncorrelated, then cik = 0
â–  cii = Ïƒi2 = VAR[xi]
q The covariance terms can be expressed as cii = Ïƒi2 and cik = ÏikÏƒiÏƒk
â–  where Ïik is called the correlation coefficient
  Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  The Normal or Gaussian Distribution
q The multivariate Normal distribution N(Î¼, Î£) is defined as f (x)= 1 expÃ©-1(X-Î¼)TÎ£-1(X-Î¼)Ã¹
X (2p)n/2 Î£1/2 ÃªÃ« 2 ÃºÃ»
q For a single dimension, this expression is reduced to
   f (x)= 1 expÃ©-1Ã¦x-Î¼Ã¶2Ã¹ X 2ps2 Ãª 2Ã§Ã¨ s Ã·Ã¸Ãº
   Ã«Ã»
 Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Central Limit Theorem
q The probability distribution function of the sum of a large number of random variables approaches a gaussian distribution.
q Example: 500 experiments are performed using a uniform distribution â–  ğ‘=1
Â¢ One sample is drawn from the distribution and its mean is recorded (500 times)
Â¢ The histogram resembles a uniform distribution, as one would expect
â–  ğ‘=4
Â¢ Four samples are drawn and the mean of the four
samples is recorded (500 times)
Â¢ The histogram starts to look more Gaussian
â–  As ğ‘ grows, the shape of the histograms resembles a Normal distribution more closely
 Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Implementation: Central Limit Theorem
q 100000 experiments are performed using a uniform distribution
q The number of samples is increased to 10 from 1 (N = 1, 4, 7, 10)
q As ğ‘ grows, the shape of the histograms resembles a Normal distribution more closely
 Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

  Independent Events
q EXAMPLE: A pair of fair dice is rolled, the values of their faces are added, and the following events are defined.
q Define events
â–  A: â€œroll an odd sumâ€
â–  B: â€œroll a sum that is an integer multiple of 3â€
a) Are events A and B statistically independent?
           Sum of faces
Probability
1,1
2
1/36
1,2
2,1
3
2/36
1,3
2,2
3,1
4
3/36
1,4
2,3
3,2
4,1
5
4/36
1,5
2,4
3,3
4,2
5,1
6
5/36
1,6
2,5
3,4
4,3
5,2
6,1
7
6/36
2,6
3,5
4,4
5,3
6,2
8
5/36
3,6
4,5
5,4
6,3
9
4/36
4,6
5,5
6,4
10
3/36
5,6
6,5
11
2/36
6,6
12
1/36
                                                           b) Note all the events are mutually exclusive. Total probability axiom applies.
Peebles, Jr.(2001), â€œProbability, Random Variables and Random Signal Principles,â€ McGraw Hill

 Bayesian Classifier
ECE 610 David K. Han

 Bayesian Decision Theory
â€¢ A fundamental statistical approach to ML classification.
â€¢ Allows us to combine observed data and prior knowledge
â€¢ Provides practical learning algorithms
â€¢ It is a generative (model based) approach, which offers a useful conceptual framework
â€“Any kind of objects (e.g. time series, trees, etc.) can be classified, based on a probabilistic model specification
â€¢ Based on tradeoffs between
â€¢ decisions using probability and the costs
â€¢ Assumes relevant probability values are known.
    
 Bayesian Decision Theory
â€¢ Example:
â€¢ Designing a classifier to separate two kinds of fish:
 sea bass and salmon
â€¢ Let Ï‰ denote the state of nature
â€¢ Ï‰ = Ï‰1 for sea bass
â€¢ Ï‰ = Ï‰2 for salmon
â€¢ Thus, Ï‰ needs to be described probabilistically
â€¢ A priori probability: Representative of existing knowledge on the problem
â€¢ P(Ï‰1) is the probability that next fish is sea bass â€¢ P(Ï‰2) is the probability that that it is salmon
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Bayesian Decision Theory
â€¢ If forced to make a decision without observing the fish.
â€¢ Use the following decision rule
â€¢ Decide Ï‰1 if P(Ï‰1) > P(Ï‰2); otherwise decide Ï‰2.
â€¢ End up deciding the fish to be one type always unless the probabilities are the same.
â€¢ This strategy may be acceptable so long as â€¢ P(Ï‰1) >> P(Ï‰2) or P(Ï‰1) << P(Ï‰2)
â€¢ Probability of error by this strategy: â€¢ P(E) = min(P(w1),P( w2)).
â€¢ An alternative: Utilize observationÃ¨data
â€¢ Consider x (lightness of the fish) to be a continuous random variable whose distribution depends on the state of nature (salmon or sea bass)
â€¢ Expressedasp(x|Ï‰1)Ã¨class-conditionalprobabilitydensity function for state of nature being Ï‰1
 Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Bayesian Decision Theory
â€¢ Difference between p(x|Ï‰1) and p(x|Ï‰2): difference in lightness between sea bass and salmon
â€¢ Suppose both the prior probabilities P(Ï‰j) and the conditional densities p(x|Ï‰j) known. The joint probability density can be written as
p(wj,x)=P(wj     x)p(x)=p(x wj)P(wj)
   P(wj   x) =
p(x wj )P(wj ) p(x)
 â€¢ Rearranging terms lead to Bayes Theorem 2
â€¢ Where
â€¢ In plain English:
jj
p(x)=Ã¥p(xw )P(w )
j=1
posterior = likelihood  Ì prior
evidence
  Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

  Bayesian Decision Theory
â€¢ By measuring x, we can convert the prior probability, P(wj), into a posterior probability, P(wj|x).
â€¢ More specifically, for Ï‰1
 P(w x)= p(xw)P(w)= p(xw)P(w) = 11 11
1 p(x) 2
p(x
p(x w )P(w )+ p(x w )P(w ) 1 1 2 2
        Ã¥p(xw )P(w )
 j=1
jj
w )P(w ) 11
  P(w x)=p(xw2)P(w2)= p(x
w2)P(w2)
    2
p(x) p(x
w )P(w ) + p(x 11
w2)P(w2)
  â€¢ For Ï‰2
â€¢ Evidence p(x), viewed as a scale factor, is identical for both Ï‰1 and Ï‰2 , it is often ignored.
 Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

  Bayesian Decision Theory: Caution
â€¢ By measuring x, we can convert the prior probability, P(wj), into a posterior probability, P(wj|x).
â€¢ Note however that P(wj|x) is an estimate of the state being wj given the observation x.
â€¢ Recall
â€œI have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character.â€
â€¢ The observation used to estimate the state is not the actual content of the state
 Photo from https://diversity.ncsu.edu/news/2021/01/15/the-history-of-mlk-at-nc-state

 Bayesian Decision Theory
â€¢ Letâ€™s plot likelihood and posterior for a given prior probability: P(w1) = 2/3 and P(w2) = 1/3
p ğ‘¥ ğ‘¤! P ğ‘¤! ğ‘¥
â€¢ For every value of x, the posteriors sum to 1.0. Ã¨ P(Ï‰1|x) + P(Ï‰2 |x) = 1
â€¢ At x=14, the probability it is in category w2 is 0.08, and for category w1 is 0.92.
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience
     P(wj   x) = p(x wj )P(wj ) p(x)
   
 Implementing a Bayesian classifier
â€¢ Sorting Fish: incoming fish are sorted according to species using optical sensing (sea bass ğœ”! or salmon ğœ”"?)
â€¢ Collect data on sea bass and salmon according to their distinctive features
â€¢ Feature : ğ‘¥ (length or lightness)
Optical Sensor
 â€¢ Step
â€¢ Estimate likelihood ğ‘(ğ‘¥|ğœ”#) and ğ‘ ğ‘¥|ğœ”$
â€¢ Feature extraction using optical sensing.(length and lightness of fish)
â€¢ Compare
â€¢ Ifğ‘ ğ‘¥|ğœ”! ğ‘ƒ ğœ”! >ğ‘ ğ‘¥|ğœ”" ğ‘ƒ ğœ”" , fish=Seabass â€¢Ifğ‘ğ‘¥|ğœ”!ğ‘ƒğœ”! <ğ‘ğ‘¥|ğœ”" ğ‘ƒğœ”",fish=Salmon
from observation data.
(a) Salmon
(b) Sea bass
     Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Maximum A Prosteriori (MAP) Inference
P(Ï‰1|x)
 â€¢ Bayesâ€™ Decision Rule
â€¢ If P(Ï‰1|x) > P(Ï‰2 |x), choose Ï‰1 â€¢ If P(Ï‰2 |x) > P(Ï‰1 |x), choose Ï‰2
â€¢ This is called Maximum A Prosteriori (MAP) inference â€¢ More compactly:
â€¢ To justify this, letâ€™s compute probability of error for such decisions
P(Ï‰ |x) 2
 â€¢ P(error|x) = P(Ï‰1|x) if we decide Ï‰2 â€¢ P(error|x) = P(Ï‰2 |x) if we decide Ï‰1
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Maximum A Prosteriori (MAP) Inference
â€¢ Again
â€¢ P(error|x) = P(Ï‰1|x) if we decide Ï‰2 â€¢ P(error|x) = P(Ï‰2 |x) if we decide Ï‰1
â€¢ Will this rule minimize the average probability of error?
â€¢ Average probability of error
â€¢ By choosing Ï‰ as
â€¢ When P(Ï‰1|x) > P(Ï‰2 |x), choose Ï‰1 â€¢ When P(Ï‰2 |x) > P(Ï‰1 |x), choose Ï‰2
â€¢ errorisminimizedeverywhereinx.Thus,the average error is minimized by Bayesâ€™ decision rule.
P(Ï‰1|x)
  P(Ï‰2|x)
  decision in error
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Bayesian Decision in Multidimensions
â€¢ Letâ€™s consider more general cases
â€¢ Cases with more than one feature
â€¢ Replacethescalarxwiththefeaturevectorxwherex isinad-dimensionalEuclideanspace Rd , called feature space.
â€¢ A feature vector x is a d-component vector-valued random variable
â€¢ Let p(x|Ï‰j) be the probability density function for x conditioned on Ï‰j being the
true state of nature.
â€¢ P(Ï‰j ) describes the prior probability that nature is in state Ï‰j.
â€¢ The posterior probability P(Ï‰j |x) can be computed from p(x|Ï‰j) by Bayesâ€™ rule: ğ‘ƒğœ”#ğ’™ =ğ‘ğ’™ğœ”# ğ‘ƒğœ”#
   â€¢ where
ğ‘ªğ‘ğ’™
ğ‘ ğ’™ = * ğ‘ ğ’™ ğœ”# ğ‘ƒ ğœ”#
ğ’‹%! Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience
 
 Multivariate Gaussian Distribution for Classification
â€¢ Letâ€™s model the likelihood functions using multivariate Gaussian distribution
   ï²
1 Ã© 1 ï² ï² ï€¥ -1 ï² ï² Ã¹ exp- (x-Î¼)TÃ©SÃ¹ (x-Î¼)
p(xw)= j
â€¢ d = dimensions of x
â€¢ N = number of data points
â€¢ For a single dimension, this expression is reduced to
d 12 Ãª2 Ã«Ã» Ãº ()2ï€¥Ã« Ã»
 2pS w â€¢ The parameters can be estimated as
j
Î¼ = N Ã¥xn n=1
ï²ï²ï²ï²
ï€¥
x=[x,x,....,x ] 12N
    ï²1N
1Nï²ï²ï²ï²T S=N-1 xn -Î¼ xn -Î¼
ï€¥ Ã¥( )( ) n=1
   p(xw )= 1 expÃ©-1Ã¦x-Î¼Ã¶2Ã¹ j Ãª2Ã§sÃ·Ãº
   2ps Ãª Ã¨ Ã¸Ãº Ã«Ã»
 1N
Î¼ = N Ã¥xn n=1
 1NT
s2 = Ã¥(x -Î¼)(x -Î¼)
 N -1 n=1
nn
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Multivariate Gaussian Distribution for Classification
â€¢ Mean vector ï²TT
E [ x ] = Î¼Ë† = [ E [ x ] , E [ x ] , ïŒ , E [ x ] ] = [ Î¼ , Î¼ , ïŒ , Î¼ ] 12d12d
â€¢ Covariance matrix ï²ï€¥ï²ï²
C O V [ x ] = Î£ = E Ã© ( x - Î¼Ë† ) ( x - Î¼Ë† ) T Ã¹ Ã«Ã»
Ã© E[(x-Î¼)2] ïŒ E[(x-Î¼)(x-Î¼)]Ã¹ Ã©s2 Ãª11 11ddÃºÃª1 1dÃº
=ÃªïïïÃº=ÃªïïïÃº
ÃªE[(x-Î¼)(x-Î¼)]ïŒ E[(x-Î¼)2] Ãº Ãªc ïŒs2Ãº Ã«dd11 ddÃ»Ã«d1 dÃ»
ïŒ cÃ¹
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

Multivariate Gaussian Distribution for Classification
â€¢ For 2-D Gaussian density function
   Ã¬Ã©2 2Ã¹Ã¼ p(x,x )= 1 expÃ¯ -1 Ãª(x1 -X1) -2r(x1 -X1)(x2 -X2)+(x2 -X2) ÃºÃ¯
12 2pss1-r2 Ã­2(1-r2)Ãªs2 ss s2 ÃºÃ½
      Ã¯Ã¯
Ã«Ã»
Ã®xxxxÃ¾
xx1122
12
 â€¢ where
X2 =E[X2]
X1 =E[X1]
s2 =E[(X -X )2]
x11 1
s2 =E[(X -X )2] x2 22
r = E[(X1 - X1)(X2 - X2 )]
sx sx 12
ï²Ã©xÃ¹ x=Ãª 1Ãº
Ã«x2 Ã»
 
 Maximum Likelihood (ML) Inference
â€¢ Ignoring p(x) as stated previously, Bayesâ€™ decision rule can also be rewritten as:
â€¢ Decide Ï‰1 if p(x| Ï‰1)P(Ï‰1) > p(x| Ï‰2)P(Ï‰2); otherwise decide Ï‰2. â€¢ If P(Ï‰1) =P(Ï‰2) then,
â€¢ Choose Ï‰1 if p(x| Ï‰1) > p(x| Ï‰2); otherwise decide Ï‰2. â€¢ This is called Maximum Likelihood (ML) inference
â€¢ More compactly
 Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Implementation: ML Bayesian classifier
â€¢ Step1
â€¢ Collect data ğœ”! and ğœ”" (= Sample observation)
  ğœ”"
 Distribution of species Ï‰1 and Ï‰2 in a 2-D feature space
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience
ğœ”#

 Implementation: ML Bayesian classifier
â€¢ Step2
â€¢ Compute likelihood ğ‘ ğ’™|ğœ”# ğ‘ğ‘›ğ‘‘ ğ‘ ğ’™|ğœ”$ assuming multivariate Gaussian distribution from observation data.
â€¢ To obtain ğ‘ ğ’™|ğœ”# ğ‘ğ‘›ğ‘‘ ğ‘ ğ’™|ğœ”$ , compute mean and covariance from each data.(Assume Gaussian distribution)
ğğŸ
       ï€¥ 1 Ã©1 ï€¥-1 Ã¹ p(x,q)= exp (x-Î¼Ë†)T Ã©SÃ¹ (x-Î¼Ë†)
ï€¥1/2 Ãª2 Ã«Ã» Ãº (2p)n/2 S Ã« Ã»
     ğğŸ
1N
Î¼Ë† = N Ã¥ x n n=1
1NT S = N - 1 x n - Î¼Ë† x n - Î¼Ë†
ï€¥ Ã¥( )( ) n=1
 Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Implementation: ML Bayesian classifier
â€¢ Step 3
â€¢ For a given observation x* â€¢
 1 Ã©1 ï€¥-1 Ã¹ p(xw)= exp (x-Î¼Ë†)TÃ©SÃ¹ (x-Î¼Ë†)
 * 1 ï€¥1/2 Ãª2* 1 Ã«Ã» * 1Ãº
 (2p)n/2 S Ã« Ã» 1
   1 Ã©1 ï€¥-1 Ã¹ p(xw)= exp (x-Î¼Ë†)TÃ©SÃ¹ (x-Î¼Ë†)
 *2 ï€¥1/2 Ãª2*2Ã«Ã»*2Ãº
 (2p)n/2 S
2
Ã« Ã»
   ğğŸ
Observation (x*)
  Compare ğ‘ xâˆ—|ğœ”" ğ‘ğ‘›ğ‘‘ ğ‘ xâˆ—|ğœ”#
Ifğ‘ xâˆ—|ğœ”" >ğ‘ xâˆ—|ğœ”# , fish=Seabass(ğœ”") If ğ‘ xâˆ—|ğœ”" < ğ‘ xâˆ—|ğœ”# , fish = Salmon (ğœ”#)
      ğğŸ
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Likelihood Ratio Test (LRT)
â€¢ Decision Rule for binary classification if the priors are the same â€¢ Decide Ï‰1 if p(x|Ï‰1) P(Ï‰1) > p(x|Ï‰2) P(Ï‰2); otherwise decide Ï‰2
â€¢ Rearrange the expression as â€¢ Choose Ï‰1 if
â€¢ And
â€¢ The decision rule based on the Likelihood Test (LRT)
  Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

   Likelihood Ratio Test (LRT)
Likelihood Ratio Test: an example
  ô°€ Given a classification problem with the following class conditional densities, derive a decision rule based on the Likelihood Ratio Test (assume equal priors)
p(x|ô°…1 )ô°ƒ 1 eô°‚21(xô°‚4)2 p(x|ô°…2 )ô°ƒ 1 eô°‚21(xô°‚10)2 2ô°„ 2ô°„
ô°€ Solution
ô° Substituting the given likelihoods and priors into the LRT expression:
    ô°…1
ô°ˆ(x)ô°ƒ p(x|ô°…1 )ô°‡P(ô°…2 )ô°Šô°ˆ(x)ô°ƒ
p(x|ô°…2 )ô°† P(ô°…1 ) ô°…2
1 eô°‚21(xô°‚4)2 ô°…1 2ô°„ ô°‡1
1 ô°‚1(xô°‚10)2 ô°† 2ô°„ e 2 ô°…2
      ô° Simplifying the LRT expression:
ô°‚1(xô°‚4)2 ô°‰1(xô°‚10)2
ô°…ô°‡ ô°ˆ(x)ô°ƒe 2 2 ô°†1
ô°…2
1
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 22
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience
 
   Likelihood Ratio Test (LRT)
Likelihood Ratio Test: an example
  ô°€ Given a classification problem with the following class conditional densities, derive a decision rule based on the Likelihood Ratio Test (assume equal priors)
p(x|ô°…1 )ô°ƒ 1 eô°‚21(xô°‚4)2 p(x|ô°…2 )ô°ƒ 1 eô°‚21(xô°‚10)2 2ô°„ 2ô°„
ô°€ Solution
ô° Substituting the given likelihoods and priors into the LRT expression:
     ô°…1
ô°ˆ(x)ô°ƒ p(x|ô°…1 )ô°‡P(ô°…2 )ô°Šô°ˆ(x)ô°ƒ
p(x|ô°…2 )ô°† P(ô°…1 ) ô°…2
1 eô°‚21(xô°‚4)2 ô°…1 2ô°„ ô°‡1
1 ô°‚1(xô°‚10)2 ô°† 2ô°„ e 2 ô°…2
      ô° Simplifying the LRT expression:
ô°‚1(xô°‚4)2 ô°‰1(xô°‚10)2
ô°…ô°‡ ô°ˆ(x)ô°ƒe 2 2 ô°†1
ô°…2
1
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 22
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience
 
   Likelihood Ratio Test (LRT)
Likelihood Ratio Test: an example
  ô°€ Given a classification problem with the following class conditional densities, derive a decision rule based on the Likelihood Ratio Test (assume equal priors)
p(x|ô°…1 )ô°ƒ 1 eô°‚21(xô°‚4)2 p(x|ô°…2 )ô°ƒ 1 eô°‚21(xô°‚10)2 2ô°„ 2ô°„
ô°€ Solution
ô° Substituting the given likelihoods and priors into the LRT expression:
   ô°…1
ô°ˆ(x)ô°ƒ p(x|ô°…1 )ô°‡P(ô°…2 )ô°Šô°ˆ(x)ô°ƒ
p(x|ô°…2 )ô°† P(ô°…1 ) ô°…2
1 eô°‚21(xô°‚4)2 ô°…1 2ô°„ ô°‡1
1 ô°‚1(xô°‚10)2 ô°† 2ô°„ e 2 ô°…2
      ô° Simplifying the LRT expression:
ô°‚1(xô°‚4)2 ô°‰1(xô°‚10)2
ô°…ô°‡ ô°ˆ(x)ô°ƒe 2 2 ô°†1
ô°…2
1
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 22
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience
 
   Likelihood Ratio Test (LRT)
Likelihood Ratio Test: an example
Note
   ô° Changing signs and taking logs (xô°‚4)2 ô°‚(xô°‚10)2 ô°‡0
ô° Which yields ô°…2
ô°…1
ô°…2 ô°†
 x ô°†ô°‡ 7 ô°…1
ô° This LRT result makes sense from an intuitive point of view since the likelihoods are identical and differ only in their mean value
Thus, the decision boundary is determined by the LRT at x = 7 ô°€ How would the LRT decision rule change if, say, the priors were
such that P(ô°‹1 ) = 2P(ô°‹2 ) ?
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 23
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience
x2 -8x+16-x2 +20x-100 =12x-84
=12(x-7)
 
   Likelihood Ratio Test (LRT)
Likelihood Ratio Test: an example
Note
   ô° Changing signs and taking logs (xô°‚4)2 ô°‚(xô°‚10)2 ô°‡0
ô° Which yields ô°…2
ô°…1
ô°…2 ô°†
 x ô°†ô°‡ 7 ô°…1
ô° This LRT result makes sense from an intuitive point of view since the likelihoods are identical and differ only in their mean value
Thus, the decision boundary is determined by the LRT at x = 7 ô°€ How would the LRT decision rule change if, say, the priors were
such that P(ô°‹1 ) = 2P(ô°‹2 ) ?
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 23
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience
x2 -8x+16-x2 +20x-100 =12x-84
=12(x-7)
 
   Likelihood Ratio Test (LRT)
Likelihood Ratio Test: an example
Note
   ô° Changing signs and taking logs (xô°‚4)2 ô°‚(xô°‚10)2 ô°‡0
ô° Which yields ô°…2
ô°…1
ô°…2 ô°†
 x ô°†ô°‡ 7 ô°…1
ô° This LRT result makes sense from an intuitive point of view since the likelihoods are identical and differ only in their mean value
Thus, the decision boundary is determined by the LRT at x = 7 ô°€ How would the LRT decision rule change if, say, the priors were
such that P(ô°‹1 ) = 2P(ô°‹2 ) ?
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 23
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience
x2 -8x+16-x2 +20x-100 =12x-84
=12(x-7)
 
 Risk
â€¢ A teenager knows that she will be grounded if she chooses to invite friends over after school instead of doing her homework, but also knows that the likelihood of her parents finding out she did so is slight. If the teenager chooses to invite her friends over, she is taking a risk of getting in trouble with her parents.
â€¢ A gambler decides to take all of his winnings from the night and attempt a bet of "double or nothing." The gambler's choice is a risk in that he could lose all that he won in one bet.
â€¢ A woman gets into her car in the morning and notices that the gas level is low. She chooses to drive to work, regardless, without stopping at a gas station. By making this choice she is risking that she will run out of gas in her car on the way to work.
â€¢ A person taking a new Covid-19 vaccine. What risk?
Risk = Probability x Consequence (Loss) Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience
Consequence
 Probability

 Bayesian Decision Theory (Bayesian Risk)
â€¢ Suppose we have an observation of x and take action Î±i.
â€¢ The loss function Î»(Î± i|Ï‰j) describes the loss incurred (consequence) for
taking action Î± i when the state of nature is Ï‰j.
â€¢ Since P(Ï‰j |x ) is the probability that the true state of nature is Ï‰j, the
expected total loss associated with taking action Î±i is
â€¢ R(Î±i |x) is defined as conditional risk
â€¢ A point value function for x and ğ›¼+
â€¢ Thus, with an observation x, select an action that minimizes the conditional risk.Ã¨Bayes Decision ProcedureÃ¨optimal performance on overall risk
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Bayesian Decision Theory (Bayesian Risk)
â€¢ Suppose we have an observation of x and take action Î±i.
â€¢ The loss function Î»(Î± i|Ï‰j) describes the loss incurred (consequence) for
taking action Î± i when the state of nature is Ï‰j.
â€¢ Since P(Ï‰j |x ) is the probability that the true state of nature is Ï‰j, the
expected total loss associated with taking action Î±i is xx
â€¢ R(Î±i |x) is defined as conditional risk
â€¢ A point value function for x and ğ›¼+
â€¢ Thus, with an observation x, select an action that minimizes the conditional risk.Ã¨Bayes Decision ProcedureÃ¨optimal performance on overall risk
 Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Bayesian Decision Theory (Bayesian Risk)
â€¢ Suppose we have an observation of x and take action Î±i.
â€¢ The loss function Î»(Î± i|Ï‰j) describes the loss incurred (consequence) for
taking action Î± i when the state of nature is Ï‰j.
â€¢ Since P(Ï‰j |x ) is the probability that the true state of nature is Ï‰j, the
expected total loss associated with taking action Î±i is xx
â€¢ R(Î±i |x) is defined as conditional risk
â€¢ A point value function for x and ğ›¼+
â€¢ Thus, with an observation x, select an action that minimizes the conditional risk.Ã¨Bayes Decision ProcedureÃ¨optimal performance on overall risk
 Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Bayesian Decision Theory (Bayesian Risk)
â€¢ Our goal is to find a decision rule that minimizes the overall risk.
â€¢ The decision function Î±(x): assumes one of the Î± values Î±1, ..., Î±a for
every x.
â€¢ The overall risk R: expected loss associated with a given decision rule.
â€¢ Since R(Î±i|x) is the conditional risk associated with action Î±i, and since the decision rule specifies the action, the overall risk is given by
â€¢ where dx is a d-dimensional volume element and the integral extends over the entire feature space.
 Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Bayesian Decision Theory (Bayesian Risk)
â€¢ Clearly, if Î±(x) is chosen so that R(Î±i|x) is as small as possible for every x, then the overall risk will be minimized.
â€¢ To minimize the overall risk, compute the conditional risk â€¢ for i=1, ... , a and select Î±i for which R(Î±i|x) is minimum.
 Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Two Category Classification by Bayesian Risk
â€¢ For two-category classification problems.
â€¢ Action Î±1 corresponds to deciding that the true state of nature is Ï‰1 â€¢ Action Î±2 corresponds to deciding that it is Ï‰2.
â€¢ Recall the loss function Î»(Î± i|Ï‰j) describes the loss incurred for taking action Î±i when the state of nature is Ï‰j.
â€¢ The conditional risk
â€¢ R(Î±1|x) = Î»11P(Ï‰1|x) + Î»12P(Ï‰2|x) â€¢ R(Î±2|x) = Î»21P(Ï‰1|x) + Î»22P(Ï‰2|x)
â€¢ Decide Ï‰1
â€¢ if R(Î±1|x) < R(Î±2|x)
â€¢ Decide Ï‰2 otherwise
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Two Category Classification by Bayesian Risk
â€¢ R(Î±1|x) = Î»11P(Ï‰1|x) + Î»12P(Ï‰2|x) â€¢ R(Î±2|x) = Î»21P(Ï‰1|x) + Î»22P(Ï‰2|x)
â€¢ Decide Ï‰1
â€¢ if R(Î±1|x) < R(Î±2|x)
Î»11P(Ï‰1|x) + Î»12P(Ï‰2|x) < Î»21P(Ï‰1|x) + Î»22P(Ï‰2|x) â€¢ Collect like terms
Î»12P(Ï‰2|x) - Î»22P(Ï‰2|x) < Î»21P(Ï‰1|x) - Î»11P(Ï‰1|x) (Î»12 - Î»22)P(Ï‰2|x) < (Î»21 - Î»11) P(Ï‰1|x)
(Î»21 - Î»11) P(Ï‰1|x) > (Î»12 - Î»22)P(Ï‰2|x)
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Two Category Classification by Bayesian Risk
â€¢ Recall Î»21 = Loss incurred by choosing Ï‰2 when it was actually Ï‰1
â€¢ and Î»11 = Loss incurred by choosing Ï‰1 when it was actually Ï‰1
â€¢ Loss incurred when choosing correctly should be less than the loss when incorrect choice was made.
â€¢ Thus (Î»21 - Î»11) > 0 and (Î»12 â€“ Î»22) > 0
â€¢ Again, decide Ï‰1 if (Î»21 - Î»11) P(Ï‰1|x) > (Î»12 - Î»22)P(Ï‰2|x)
â€¢ Substitute P(Ï‰1|x) by Bayes rule
(Î»21 - Î»11) p(x| Ï‰1)P(Ï‰1) > (Î»12 â€“ Î»22) p(x| Ï‰2)P(Ï‰2)
â€¢ Alternatively, decide Ï‰1 if
 â€¢ Choose Ï‰2 otherwise
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

T
   he Bayes Risk Example
â€¢ â€¢
Assume P(Ï‰ ) = P(Ï‰ ) = 0.5, Î» = Î» = 0, Î» = 1, Î»
1 2 11 22 12 21
The Bayes Risk: an example
 ô°€ Consider a classification problem with two classes defined by the following likelihood functions
 23 pô°Œx|ô°… ô°ô°ƒ 1 eô°‚1 x2
1 2ô°„31 2 pô°Œx|ô°…2ô°ô°ƒ 1 eô°‚2ô°Œxô°‚2ô°
   2ô°„
ô° Sketch the two densities
ô° What is the likelihood ratio?
 =
ô°‚1x2ô°‰1ô°Œxô°‚2ô°2ô°‡0 2 3 2 ô°† ô°…2
ô°…1 2 ô°‡
= 0, C Determdineeciasdioencisriounlerutlhe athtamt mininiimizeessthteherispkrobability of error
= 3
Determine a
ô° Assume P(ô°‹ ) = P(ô°‹ ) = 0.5, C = C
1 2 11 22 12 21
1
2ô°„ 3
ô°…
= 1 and C
1â„2
 eô°‚1 x2
2 3 ô°…1
ô°…2
ô°‚1ô°Œxô°‚2ô°2 ô°† e2 ô°…2
 ô°‡1 1 ô°‚1ô°Œxô°‚2ô°2 ô°† 3
1
 ô°ˆô°Œxô°ô°ƒ
ô°ƒ e23 ô°‡1
 2ô°„ e 2 ô°‚1x2 ô°…1
2xô°‚12xô°‰12ô°†0ô°xô°ƒ4.73,1.27 ô°…2
      Intelligent Information & Signal Processing Lab.
School of EE, Korea University
31
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Inte
rscience
 
T
   he Bayes Risk Example
The Bayes Risk: an example
 ô°€ Consider a classification problem with two classes defined by the following likelihood functions
Determine a
 23 pô°Œx|ô°… ô°ô°ƒ 1 eô°‚1 x2
1 2ô°„31 2 pô°Œx|ô°…2ô°ô°ƒ 1 eô°‚2ô°Œxô°‚2ô°
   2ô°„
ô° Sketch the two densities
ô° What is the likelihood ratio?
ô° Assume P(ô°‹1) = P(ô°‹2) = 0.5, C11 = C22 = 0, C12 = 1 and C21 = 31â„2
decision rule that minimizes the probability of error
 eô°‚1 x2
2 3 ô°…1
ô°…2
ô°‚1ô°Œxô°‚2ô°2 ô°† e2 ô°…2
1
2ô°„ 3
ô°…
ô°‚1x2ô°‰1ô°Œxô°‚2ô°2ô°‡0 2 3 2 ô°† ô°…2
ô°…1 2 ô°‡
 ô°‡1 1 ô°‚1ô°Œxô°‚2ô°2 ô°† 3
1
 ô°ˆô°Œxô°ô°ƒ
ô°ƒ e23 ô°‡1
 2ô°„ e 2 ô°‚1x2 ô°…1
2xô°‚12xô°‰12ô°†0ô°xô°ƒ4.73,1.27 ô°…2
      Intelligent Information & Signal Processing Lab.
School of EE, Korea University
31
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Inte
rscience
 
T
   he Bayes Risk Example
The Bayes Risk: an example
 ô°€ Consider a classification problem with two classes defined by the following likelihood functions
Determine a
 23 pô°Œx|ô°… ô°ô°ƒ 1 eô°‚1 x2
1 2ô°„31 2 pô°Œx|ô°…2ô°ô°ƒ 1 eô°‚2ô°Œxô°‚2ô°
   2ô°„
ô° Sketch the two densities
ô° What is the likelihood ratio?
ô° Assume P(ô°‹1) = P(ô°‹2) = 0.5, C11 = C22 = 0, C12 = 1 and C21 = 31â„2
decision rule that minimizes the probability of error
 eô°‚1 x2
2 3 ô°…1
ô°…2
ô°‚1ô°Œxô°‚2ô°2 ô°† e2 ô°…2
1
2ô°„ 3
ô°…
ô°‚1x2ô°‰1ô°Œxô°‚2ô°2ô°‡0 2 3 2 ô°† ô°…2
ô°…1 2 ô°‡
 ô°‡1 1 ô°‚1ô°Œxô°‚2ô°2 ô°† 3
1
 ô°ˆô°Œxô°ô°ƒ
ô°ƒ e23 ô°‡1
 2ô°„ e 2 ô°‚1x2 ô°…1
2xô°‚12xô°‰12ô°†0ô°xô°ƒ4.73,1.27 ô°…2
      Intelligent Information & Signal Processing Lab.
School of EE, Korea University
31
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Inte
rscience
 
T
   he Bayes Risk Example
The Bayes Risk: an example
 ô°€ Consider a classification problem with two classes defined by the following likelihood functions
Determine a
 23 pô°Œx|ô°… ô°ô°ƒ 1 eô°‚1 x2
1 2ô°„31 2 pô°Œx|ô°…2ô°ô°ƒ 1 eô°‚2ô°Œxô°‚2ô°
   2ô°„
ô° Sketch the two densities
ô° What is the likelihood ratio?
ô° Assume P(ô°‹1) = P(ô°‹2) = 0.5, C11 = C22 = 0, C12 = 1 and C21 = 31â„2
decision rule that minimizes the probability of error
 eô°‚1 x2
2 3 ô°…1
ô°…2
ô°‚1ô°Œxô°‚2ô°2 ô°† e2 ô°…2
1
2ô°„ 3
ô°…
ô°‚1x2ô°‰1ô°Œxô°‚2ô°2ô°‡0 2 3 2 ô°† ô°…2
ô°…1 2 ô°‡
 ô°‡1 1 ô°‚1ô°Œxô°‚2ô°2 ô°† 3
1
 ô°ˆô°Œxô°ô°ƒ
ô°ƒ e23 ô°‡1
 2ô°„ e 2 ô°‚1x2 ô°…1
2xô°‚12xô°‰12ô°†0ô°xô°ƒ4.73,1.27 ô°…2
      Intelligent Information & Signal Processing Lab.
School of EE, Korea University
31
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Inte
rscience
 
T
   he Bayes Risk Example
The Bayes Risk: an example
 ô°€ Consider a classification problem with two classes defined by the following likelihood functions
Determine a
 23 pô°Œx|ô°… ô°ô°ƒ 1 eô°‚1 x2
1 2ô°„31 2 pô°Œx|ô°…2ô°ô°ƒ 1 eô°‚2ô°Œxô°‚2ô°
   2ô°„
ô° Sketch the two densities
ô° What is the likelihood ratio?
ô° Assume P(ô°‹1) = P(ô°‹2) = 0.5, C11 = C22 = 0, C12 = 1 and C21 = 31â„2
decision rule that minimizes the probability of error
 eô°‚1 x2
2 3 ô°…1
ô°…2
ô°‚1ô°Œxô°‚2ô°2 ô°† e2 ô°…2
1
2ô°„ 3
ô°…
ô°‚1x2ô°‰1ô°Œxô°‚2ô°2ô°‡0 2 3 2 ô°† ô°…2
ô°…1 2 ô°‡
 ô°‡1 1 ô°‚1ô°Œxô°‚2ô°2 ô°† 3
1
 ô°ˆô°Œxô°ô°ƒ
ô°ƒ e23 ô°‡1
 2ô°„ e 2 ô°‚1x2 ô°…1
2xô°‚12xô°‰12ô°†0ô°xô°ƒ4.73,1.27 ô°…2
      Intelligent Information & Signal Processing Lab.
School of EE, Korea University
31
Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Inte
rscience
 
 Discriminant Function
â€¢ Allthedecisionrulessofarhavethesamestructure â€¢ AteachpointxinfeaturespacechooseclassÏ‰iwhich
maximizes (or minimizes) some measure gi(x)
â€¢ Thisstructurecanbeformalizedwithasetofdiscriminat functions gi(x), i=1,....., C, and the following decision rule:
â€¢ â€assignxtoclassÏ‰iifgi(x)>gi(x)foreveryjâ‰ iâ€
â€¢ Therefore,wecanvisualizethedecisionruleasa network or machine that computes C discriminat functions and selects the category corresponding to the largest discriminant. Such network is depicted as
â€¢ Summarizethethreedecisionrules:BayesRisk,MAP, and ML
      Decision Rule
Discriminant Function
Bayes Risk
gi(x) = R(Î±i |x)
MAP
gi(x) = P(Ï‰i |x)
ML
gi(x) = p(x| Ï‰i)
        Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

 Bayesian Decision Theory
â€¢ In summary
â€¢ MAP
â€¢ Maximum A Posteriori determined by â€¢ P(Ï‰j |x ) directly if possible
â€¢ Or by using Bayes rule
â€¢ Maximum Likelihood (ML)
â€¢ Determined by the maximum likelihood: P(x | Ï‰j ) â€¢ Bayes Risk Based choosing the minimum of
   Duda, et. al. (2001), â€œPattern Classification,â€ Wiley Interscience

   Bayes classifiers for Gaussian distributed classes
Bayes classifiers for Gaussian distributed classes
 ô°€ General expression for Gaussian densities
ô° The multivariate Normal density function was defined as
f (x)ô°ƒ 1 expô°•ô°‚1(xô°‚Î¼)T Î£ô°‚1(xô°‚Î¼)ô°’ x ( 2 ô°„ ) n 2 Î£ 1 2 ô°“ô°” 2 ô°ô°‘
   ô° Using Bayes rule, the MAP discriminant function becomes
g(x)ô°ƒP(ô°… |x)ô°ƒp(x|ô°…i )P(ô°…i )ô°ƒ 1 expô°•ô°‚1(xô°‚Î¼ )TÎ£ô°‚1(xô°‚Î¼ )ô°’P(ô°… ) 1
i i p(x) (2ô°„)n2Î£12 ô°“ô°”2 i i iô°ô°‘ip(x)
     ô° Eliminating constant terms
Sine the MAP picks the largest of gis letâ€™s eliminate common terms
g(x)ô°ƒÎ£ ô°‚12expô°•ô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°’P(ô°…) i i ô°“ô°”2ii iô°ô°‘i
i
  Aô°gaiWn,easttahkeeMnAaPtupircaklsltohegslasrginescteofthgesleotâ€™gsatarkitehnmatuirsalalomgsosninocteolnogicaariltlhyminiscareasing
monfoutoncictaiollyn increasing function
i
g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…)) i2iii2ii
    ô° This expression is called a quadratic discriminant function
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 35
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
   Bayes classifiers for Gaussian distributed classes
Bayes classifiers for Gaussian distributed classes
 ô°€ General expression for Gaussian densities
ô° The multivariate Normal density function was defined as
f (x)ô°ƒ 1 expô°•ô°‚1(xô°‚Î¼)T Î£ô°‚1(xô°‚Î¼)ô°’ x ( 2 ô°„ ) n 2 Î£ 1 2 ô°“ô°” 2 ô°ô°‘
   ô° Using Bayes rule, the MAP discriminant function becomes
g(x)ô°ƒP(ô°… |x)ô°ƒp(x|ô°…i )P(ô°…i )ô°ƒ 1 expô°•ô°‚1(xô°‚Î¼ )TÎ£ô°‚1(xô°‚Î¼ )ô°’P(ô°… ) 1
i i p(x) (2ô°„)n2Î£12 ô°“ô°”2 i i iô°ô°‘ip(x)
     ô° Eliminating constant terms
Sine the MAP picks the largest of gis letâ€™s eliminate common terms
g(x)ô°ƒÎ£ ô°‚12expô°•ô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°’P(ô°…) i i ô°“ô°”2ii iô°ô°‘i
g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…)) i2iii2ii
i
  Aô°gaiWn,easttahkeeMnAaPtupircaklsltohegslasrginescteofthgesleotâ€™gsatarkitehnmatuirsalalomgsosninocteolnogicaariltlhyminiscareasing
monfoutoncictaiollyn increasing function
i
    ô° This expression is called a quadratic discriminant function
Duda,et.al.I(n2t0e0ll1ig)e,nâ€œtPIanttfoermnaCtliaosnsi&ficSaitgionanl,â€PWrocilesysiIngteLrascbi.ence SchoolofEE, KoreaUniversity 35 From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
     
   Bayes classifiers for Gaussian distributed classes
Bayes classifiers for Gaussian distributed classes
 ô°€ General expression for Gaussian densities
ô° The multivariate Normal density function was defined as
f (x)ô°ƒ 1 expô°•ô°‚1(xô°‚Î¼)T Î£ô°‚1(xô°‚Î¼)ô°’ x ( 2 ô°„ ) n 2 Î£ 1 2 ô°“ô°” 2 ô°ô°‘
ô° Using Bayes rule, the MAP discriminant function becomes
g(x)ô°ƒP(ô°… |x)ô°ƒp(x|ô°…i )P(ô°…i )ô°ƒ 1 expô°•ô°‚1(xô°‚Î¼ )TÎ£ô°‚1(xô°‚Î¼ )ô°’P(ô°… ) 1
   i i p(x) (2ô°„)n2Î£12 ô°“ô°”2 i i iô°ô°‘ip(x)
     ô° Eliminating constant terms
Sine the MAP picks the largest of gis letâ€™s eliminate common terms
g(x)ô°ƒÎ£ ô°‚12expô°•ô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°’P(ô°…) i i ô°“ô°”2ii iô°ô°‘i
i
  Aô°gaiWn,easttahkeeMnAaPtupircaklsltohegslasrginescteofthgesleotâ€™gsatarkitehnmatuirsalalomgsosninocteolnogicaariltlhyminiscareasing
monfoutoncictaiollyn increasing function
i
g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…)) i2iii2ii
    ô° This expression is called a quadratic discriminant function
    Intelligent Information & Signal Processing Lab. School of EE, Korea University
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
35
 
   Bayes classifiers for Gaussian distributed classes
Bayes classifiers for Gaussian distributed classes
 ô°€ General expression for Gaussian densities
ô° The multivariate Normal density function was defined as
f (x)ô°ƒ 1 expô°•ô°‚1(xô°‚Î¼)T Î£ô°‚1(xô°‚Î¼)ô°’ x ( 2 ô°„ ) n 2 Î£ 1 2 ô°“ô°” 2 ô°ô°‘
   ô° Using Bayes rule, the MAP discriminant function becomes
g(x)ô°ƒP(ô°… |x)ô°ƒp(x|ô°…i )P(ô°…i )ô°ƒ 1 expô°•ô°‚1(xô°‚Î¼ )TÎ£ô°‚1(xô°‚Î¼ )ô°’P(ô°… ) 1
i i p(x) (2ô°„)n2Î£12 ô°“ô°”2 i i iô°ô°‘ip(x)
     ô° Eliminating constant terms
Sine the MAP picks the largest of gis letâ€™s eliminate common terms
g(x)ô°ƒÎ£ ô°‚12expô°•ô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°’P(ô°…) i i ô°“ô°”2ii iô°ô°‘i
i
  Aô°gaiWn,easttahkeeMnAaPtupircaklsltohegslasrginescteofthgesleotâ€™gsatarkitehnmatuirsalalomgsosninocteolnogicaariltlhyminiscareasing
monfoutoncictaiollyn increasing function
i
g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…)) i2iii2ii
    ô° This expression is called a quadratic discriminant function
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 35
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
  Case 1: Î£ğ‘– = ğœ2I Case ô°– : ô°—i=ô°˜2I
ô°€ This situation occurs when the features are statistically independent with the same variance for all classes*
  ô° In this case, the quadratic discriminant function becomes g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…))
  i2iii2ii
gi (x) ô°ƒ ô°‚ 12 (x ô°‚ Î¼i )T (ô°™ 2I)ô°‚1 (x ô°‚ Î¼i ) ô°‚ 12 log(ô°™ 2I ) ô°‰ log(P(ô°…i ))
  ô°ƒô°‚ 1 (xô°‚Î¼)T(xô°‚Î¼)ô°‚1Nlog(ô°™2)ô°‰log(P(ô°…)) 2ô°™2ii2 i
Expanding this expression
  ô°
dropping the second term
1
ô°ƒ ô°‚2ô°™2 (xô°‚Î¼i)T(xô°‚Î¼i)ô°‰log(P(ô°…i))
 1 ğ¶ğ‘ğ‘ ğ‘’ 1: âˆ‘$ = ğœ#ğ¼ gi(x)ô°ƒô°‚2ô°™2 (xô°‚Î¼i)T(xô°‚Î¼i)ô°‰log(P(ô°…i))
ô°ƒô°‚ 1 (xTxô°‚2Î¼Txô°‰Î¼TÎ¼)ô°‰log(P(ô°…)) 2ô°™2 iii i
      Intelligent Information & Signal Processing Lab. School of EE, Korea University 36
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[7 4]T Î¼3 ô°ƒ[2 5]T
Î£ô°ƒô°•2 0ô°’ Î£ô°ƒô°•2 0ô°’ Î£ô°ƒô°•2 0ô°’ 1 ô°“ô°”0 2ô°ô°‘ 2 ô°“ô°”0 2ô°ô°‘ 3 ô°“ô°”0 2ô°ô°‘
  
  Case 1: Î£ğ‘– = ğœ2I Case ô°– : ô°—i=ô°˜2I
ô°€ This situation occurs when the features are statistically independent with the same variance for all classes*
  ô° In this case, the quadratic discriminant function becomes g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…))
  i2iii2ii
gi (x) ô°ƒ ô°‚ 12 (x ô°‚ Î¼i )T (ô°™ 2I)ô°‚1 (x ô°‚ Î¼i ) ô°‚ 12 log(ô°™ 2I ) ô°‰ log(P(ô°…i ))
  ô°ƒô°‚ 1 (xô°‚Î¼)T(xô°‚Î¼)ô°‚1Nlog(ô°™2)ô°‰log(P(ô°…)) 2ô°™2ii2 i
Expanding this expression
  ô°
dropping the second term
1
ô°ƒ ô°‚2ô°™2 (xô°‚Î¼i)T(xô°‚Î¼i)ô°‰log(P(ô°…i))
 g(x)ô°ƒô°‚ 1 (xô°‚Î¼)T(xô°‚Î¼)ô°‰log(P(ô°…)) i 2ô°™2 i i i
ô°ƒô°‚ 1 (xTxô°‚2Î¼Txô°‰Î¼TÎ¼)ô°‰log(P(ô°…)) 2ô°™2 iii i
      Intelligent Information & Signal Processing Lab. School of EE, Korea University 36
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[7 4]T Î¼3 ô°ƒ[2 5]T
Î£ô°ƒô°•2 0ô°’ Î£ô°ƒô°•2 0ô°’ Î£ô°ƒô°•2 0ô°’ 1 ô°“ô°”0 2ô°ô°‘ 2 ô°“ô°”0 2ô°ô°‘ 3 ô°“ô°”0 2ô°ô°‘
  
  Case 1: Î£ğ‘– = ğœ2I Case ô°– : ô°—i=ô°˜2I
ô°€ This situation occurs when the features are statistically independent with the same variance for all classes*
  ô° In this case, the quadratic discriminant function becomes g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…))
  i2iii2ii
gi (x) ô°ƒ ô°‚ 12 (x ô°‚ Î¼i )T (ô°™ 2I)ô°‚1 (x ô°‚ Î¼i ) ô°‚ 12 log(ô°™ 2I ) ô°‰ log(P(ô°…i ))
  ô°ƒô°‚ 1 (xô°‚Î¼)T(xô°‚Î¼)ô°‚1Nlog(ô°™2)ô°‰log(P(ô°…)) 2ô°™2ii2 i
Expanding this expression
  ô°
dropping the second term
1
ô°ƒ ô°‚2ô°™2 (xô°‚Î¼i)T(xô°‚Î¼i)ô°‰log(P(ô°…i))
 g(x)ô°ƒô°‚ 1 (xô°‚Î¼)T(xô°‚Î¼)ô°‰log(P(ô°…)) i 2ô°™2 i i i
ô°ƒô°‚ 1 (xTxô°‚2Î¼Txô°‰Î¼TÎ¼)ô°‰log(P(ô°…)) 2ô°™2 iii i
      Intelligent Information & Signal Processing Lab. School of EE, Korea University 36
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[7 4]T Î¼3 ô°ƒ[2 5]T
Î£ô°ƒô°•2 0ô°’ Î£ô°ƒô°•2 0ô°’ Î£ô°ƒô°•2 0ô°’ 1 ô°“ô°”0 2ô°ô°‘ 2 ô°“ô°”0 2ô°ô°‘ 3 ô°“ô°”0 2ô°ô°‘
  
  Case1:Î£ =ğœ2I
 Case 1 : ô°š
i
Ïƒ
2
ğ‘–
=
I
   ô°
Eliminating the term xTx, which is constant for all classes g(x)ô°ƒô°‚ 1 (ô°‚2Î¼Txô°‰Î¼TÎ¼ )ô°‰log(P(ô°… ))ô°ƒô°…Txô°‰ô°…
whereô°›ô° !i ô°™2 ô°›ğ‘¤
ô°
If we assume equal priors
ô° This is called a minimum-distance or nearest mean classifier
ô° The loci of constant probability for each class are hyper-spheres
ô° For unit variance (ô°˜2=1), the distance becomes the Euclidean distance
From [schalkoff, 1992]
i 2ô°™2 i ii i i i0 ô° ô°…ğ‘¤ # ô°ƒ Î¼ i
  1
ô°œô°…!"ô°ƒô°‚2ô°™ Î¼TÎ¼ô°‰log(P(ô°…)) i0 2ii i
  ô°
Since the discriminant is linear, the decision boundaries gi(x)=gj(x), will be hyper-planes
  g (x)ô°ƒô°‚ 1 (xô°‚Î¼ )T(xô°‚Î¼ ) i 2ô°™2 i i
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 37
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 ğ‘¤$% ğ‘ğ‘›ğ‘‘ ğ‘¤$& are constants
 
  Case1:Î£ =ğœ2I
Case1:Î£ =ğœ2I
 Case 1 : ô°š
Ïƒ
2
I
ğ‘–= i
ğ‘–
   ô°
Eliminating the term xTx, which is constant for all classes
1
i2ô°™2iii ii$i0$&
g(x)ô°ƒô°‚
whereô°›ô° !i ô°™2
TTT% (ô°‚2Î¼ xô°‰Î¼ Î¼ )ô°‰log(P(ô°… ))ô°ƒô°… xğ‘¤ô°‰ğ‘¥ô°…+ğ‘¤
  ô°ô°…ğ‘¤# ô°ƒ Î¼i ô°›ğ‘¤
1
ô°œô°…!" ô°ƒô°‚2ô°™ Î¼TÎ¼ ô°‰log(P(ô°… )) i0 2ii i
ğ‘¤$% ğ‘ğ‘›ğ‘‘ ğ‘¤$& are const
ants
 ô°
ô° Since the discriminant is linear, the decision boundaries gi(x)=gj(x), will be hyper-planes
If we assume equal priors
ô° This is called a minimum-distance or nearest mean classifier
ô° The loci of constant probability for each class are hyper-spheres
ô° For unit variance (ô°˜2=1), the distance becomes the Euclidean distance
From [schalkoff, 1992]
  g (x)ô°ƒô°‚ 1 (xô°‚Î¼ )T(xô°‚Î¼ ) i 2ô°™2 i i
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 37
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
Depends on how far away x is from the mean
   
  Case1:Î£ =ğœ2I
Case1:Î£ =ğœ2I
 Case 1 : ô°šğ‘– = ğ‘–
i Ïƒ2I, example
 ô°€ To illustrate the previous result, we will compute the decision boundaries for a 3-class, 2-dimensional problem with the following class mean vectors and
covariance matrices and equal
priors
 Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[7 4]T Î¼3 ô°ƒ[2 5]T Î£ô°ƒô°•2 0ô°’ Î£ô°ƒô°•2 0ô°’ Î£ô°ƒô°•2 0ô°’
1 ô°“ô°”0 2ô°ô°‘ 2 ô°“ô°”0 2ô°ô°‘ 3 ô°“ô°”0 2ô°ô°‘
      Intelligent Information & Signal Processing Lab.
School of EE, Korea University 38
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
C
  ase 1: Î£ğ‘– = ğœ2I Case 1 : ô°ši=Ïƒ2I, example
ô°€ To illustrate the previous result, we will compute the decision boundaries for a 3-class, 2-dimensional problem with the following class mean vectors and
covariance matrices and equal
priors
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[7 4]T Î¼3 ô°ƒ[2 5]T
Î£ô°ƒô°•2 0ô°’ Î£ô°ƒô°•2 0ô°’ Î£ô°ƒô°•2 0ô°’ 1 ô°“ô°”0 2ô°ô°‘ 2 ô°“ô°”0 2ô°ô°‘ 3 ô°“ô°”0 2ô°ô°‘
          Intelligent Information & Signal Processing Lab. School of EE, Korea University
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
38
 
  Case 2: S = S (S diagonal)
Case ô°Ÿ : ô°— ô° ô°— ô°¡ô°— diagonalô°¢ i
i
 ô°€ The classes still have the same covariance matrix, but the features are allowed to have different variances
ô° In this case, the quadratic discriminant function becomes g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…))
  i2iii2ii
ô°‚2 ô°¦ô°‚2ô°£
ô°•ô°™ ô°’ ô°¨ô°™2 ô°¥ 1ô°“1ô°11
  ô°ƒô°‚2(xô°‚Î¼i)Tô°“ô°“ ô°« ô°”
1N (x[k]ô°‚Î¼[k])2 ô°ƒô°‚ ô°ª i
ô°ô°(xô°‚Î¼i)ô°‚2logô°¦ô°¦ô°¦ ô°« 2 ô°£ô°£ô°£ô°‰log(P(ô°…i)) ô°™ô°‚2ô°‘ ô°§ ô°™ô°‚2 ô°¤
 2kô°ƒ1 ô°™k2 2kô°ƒ1
ki 1 N
NN 1 N
ô°‚ logô°©ô°™2ô°‰log(P(ô°…))
 1 N
x[k]2 ô°‚2x[k]Î¼[k]ô°‰Î¼[k]2
ô°ƒô°‚ ô°ª 2kô°ƒ1
ô°™k2
i i
ô°‚ logô°©ô°™2ô°‰log(P(ô°…))
 2 kô°ƒ1
ki
    Intelligent Information & Signal Processing Lab.
School of EE, Korea University 39
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[5 4]T Î¼3 ô°ƒ[2 5]T
Î£ô°ƒô°•1 0ô°’ Î£ô°ƒô°•1 0ô°’ Î£ô°ƒô°•1 0ô°’ 1 ô°“ô°”0 2ô°ô°‘ 2 ô°“ô°”0 2ô°ô°‘ 3 ô°“ô°”0 2ô°ô°‘
 From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
  
  Case 2: S = S (S diagonal)
Case ô°Ÿ : ô°— ô° ô°— ô°¡ô°— diagonalô°¢ i
i
 ô°€ The classes still have the same covariance matrix, but the features are allowed to have different variances
ô° In this case, the quadratic discriminant function becomes g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…))
  i2iii2ii
ô°‚2 ô°¦ô°‚2ô°£
ô°•ô°™ ô°’ ô°¨ô°™2 ô°¥ 1ô°“1ô°11
  ô°ƒô°‚2(xô°‚Î¼i)Tô°“ô°“ ô°« ô°”
1N (x[k]ô°‚Î¼[k])2 ô°ƒô°‚ ô°ª i
ô°ô°(xô°‚Î¼i)ô°‚2logô°¦ô°¦ô°¦ ô°« 2 ô°£ô°£ô°£ô°‰log(P(ô°…i)) ô°™ô°‚2ô°‘ ô°§ ô°™ô°‚2 ô°¤
 2kô°ƒ1 ô°™k2 2kô°ƒ1
ki 1 N
NN 1 N
  ô°‚ logô°©ô°™2ô°‰log(P(ô°…))
 1 N
x[k]2 ô°‚2x[k]Î¼[k]ô°‰Î¼[k]2
ô°ƒô°‚ ô°ª 2kô°ƒ1
ô°™k2
i i
ô°‚ logô°©ô°™2ô°‰log(P(ô°…))
 2 kô°ƒ1
ki
    Intelligent Information & Signal Processing Lab.
School of EE, Korea University 39
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[5 4]T Î¼3 ô°ƒ[2 5]T
Î£ô°ƒô°•1 0ô°’ Î£ô°ƒô°•1 0ô°’ Î£ô°ƒô°•1 0ô°’ 1 ô°“ô°”0 2ô°ô°‘ 2 ô°“ô°”0 2ô°ô°‘ 3 ô°“ô°”0 2ô°ô°‘
 From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
  
  Case 2: S = S (S diagonal)
Case ô°Ÿ : ô°— ô° ô°— ô°¡ô°— diagonalô°¢ i
i
 ô°€ The classes still have the same covariance matrix, but the features are allowed to have different variances
ô° In this case, the quadratic discriminant function becomes g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…))
  i2iii2ii
ô°‚2 ô°¦ô°‚2ô°£
ô°•ô°™ ô°’ ô°¨ô°™2 ô°¥ 1ô°“1ô°11
  ô°ƒô°‚2(xô°‚Î¼i)Tô°“ô°“ ô°« ô°”
1N (x[k]ô°‚Î¼[k])2 ô°ƒô°‚ ô°ª i
ô°ô°(xô°‚Î¼i)ô°‚2logô°¦ô°¦ô°¦ ô°« 2 ô°£ô°£ô°£ô°‰log(P(ô°…i)) ô°™ô°‚2ô°‘ ô°§ ô°™ô°‚2 ô°¤
 2kô°ƒ1 ô°™k2 2kô°ƒ1
ki 1 N
NN 1 N
 ô°‚ logô°©ô°™2ô°‰log(P(ô°…))
 1 N
x[k]2 ô°‚2x[k]Î¼[k]ô°‰Î¼[k]2
ô°ƒô°‚ ô°ª 2kô°ƒ1
ô°™k2
i i
ô°‚ logô°©ô°™2ô°‰log(P(ô°…))
 2 kô°ƒ1
ki
    Intelligent Information & Signal Processing Lab.
School of EE, Korea University 39
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[5 4]T Î¼3 ô°ƒ[2 5]T
Î£ô°ƒô°•1 0ô°’ Î£ô°ƒô°•1 0ô°’ Î£ô°ƒô°•1 0ô°’ 1 ô°“ô°”0 2ô°ô°‘ 2 ô°“ô°”0 2ô°ô°‘ 3 ô°“ô°”0 2ô°ô°‘
 From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
  
 Case 2: S = S (S diagonal) Case1:Î£ =ğœ2I
 Case 2 :iô°šiô°¬ô°š (ô°š diagonal) ğ‘–
 ô° Eliminating the term x[k]2, which is constant for all classes
ô°
ô° The loci of constant probability are hyper-ellipses aligned with feature
axes
ô° Note that only difference with the previous classifier is that the distance of each axis is normalized by the variance of the axis
  1 N ô°‚2x[k]Î¼[k]ô°‰Î¼[k]2 1
N
gi(x)ô°ƒô°‚ i i ô°‚ log ô°™2ô°‰log(P(ô°…))
ô°ªô°© 2kô°ƒ1 ô°™k2 2 kô°ƒ1
ki
 This discriminant is linear, so the decision boundaries gi(x)=gj(x), will also be hyper-planes
  From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
  Case1:Î£ =ğœ2I iğ‘–
 Case 2: S = S (S diagonal)
Case 2 : ô°šiô°¬ô°š (ô°š diagonal)
 ô° Eliminating the term x[k]2, which is constant for all classes
ô°
ô° The loci of constant probability are hyper-ellipses aligned with feature
axes
ô° Note that only with the previous classifier is that the distance of each axis is
  1 N ô°‚2x[k]Î¼[k]ô°‰Î¼[k]2 1
N
gi(x)ô°ƒô°‚ i i ô°‚ log ô°™2ô°‰log(P(ô°…))
ô°ªô°© 2kô°ƒ1 ô°™k2 2 kô°ƒ1
ki
  This discriminant is linear, so the decision boundaries gi(x)=gj(x), will also be hyper-planes
difference
 normalized by the variance of the axis
 From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
  Case1:Î£ =ğœ2I
 Case 2: S = S (ğ‘–S diagonal), Example Case 2i : ô°šiô°¬ô°š (ô°š diagonal), example
 ô°€ To illustrate the previous result, we will compute the decision boundaries for a 3-class, 2-dimensional problem with the following class mean vectors and
covariance matrices and equal
 priors
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[5 4]T Î¼3 ô°ƒ[2 5]T Î£ô°ƒô°•1 0ô°’ Î£ô°ƒô°•1 0ô°’ Î£ô°ƒô°•1 0ô°’
1 ô°“ô°”0 2ô°ô°‘ 2 ô°“ô°”0 2ô°ô°‘ 3 ô°“ô°”0 2ô°ô°‘
       Intelligent Information & Signal Processing Lab.
School of EE, Korea University 41
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
  Case1:Î£ =ğœ2I
 Case 2: S = S (ğ‘–S diagonal), Example Case 2i : ô°šiô°¬ô°š (ô°š diagonal), example
 ô°€ To illustrate the previous result, we will compute the decision boundaries for a 3-class, 2-dimensional problem with the following class mean vectors and
covariance matrices and equal
 priors
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[5 4]T Î¼3 ô°ƒ[2 5]T Î£ô°ƒô°•1 0ô°’ Î£ô°ƒô°•1 0ô°’ Î£ô°ƒô°•1 0ô°’
1 ô°“ô°”0 2ô°ô°‘ 2 ô°“ô°”0 2ô°ô°‘ 3 ô°“ô°”0 2ô°ô°‘
        Intelligent Information & Signal Processing Lab.
School of EE, Korea University 41
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
  Case 1: Î£ğ‘– = ğœ2I
Case 3 : ô°š ô°¬ô°š (ô°š non-diagonal)
 Case 3: S = S (S non-diagonal)
i
i
  ô°€ In this case, all the classes have the same covariance matrix, but this is no longer diagonal
 ô°€ The quadratic discriminant becomes g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…))
  i2iii2ii ô°ƒô°‚12(xô°‚Î¼i)T Î£ô°‚1(xô°‚Î¼i)ô°‚12log(Î£)ô°‰log(P(ô°…i))
  ô°€ Eliminating the term logô°­ô°—ô°­, which is constant for all classes g (x)ô°ƒô°‚1(xô°‚Î¼ )T Î£ô°‚1(xô°‚Î¼ )ô°‰log(P(ô°… ))
i2iiii
ô° The quadratic term is called the Mahalanobis distance, a very important
 distance in Statistical PR
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 42
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
  Case 1: Î£ğ‘– = ğœ2I
Case 3 : ô°š ô°¬ô°š (ô°š non-diagonal)
 Case 3: S = S (S non-diagonal)
i
i
  ô°€ In this case, all the classes have the same covariance matrix, but this is no longer diagonal
 ô°€ The quadratic discriminant becomes g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…))
  i2iii2ii ô°ƒô°‚12(xô°‚Î¼i)T Î£ô°‚1(xô°‚Î¼i)ô°‚12log(Î£)ô°‰log(P(ô°…i))
  ô°€ Eliminating the term logô°­ô°—ô°­, which is constant for all classes g (x)ô°ƒô°‚1(xô°‚Î¼ )T Î£ô°‚1(xô°‚Î¼ )ô°‰log(P(ô°… ))
i2iiii
ô° The quadratic term is called the Mahalanobis distance, a very important
  distance in Statistical PR
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 42
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
  Case1:Î£ =ğœ2I Case 3 : ô°š ô°¬ô°š (ô°š non-diagonal)
 Case 3: S = S (S non-diagonal)
ğ‘–
ô°€ The Mahalanobis distance is a vector distance that uses a ô°—-1 norm
ô° ô°—-1 can be thought of as a stretching factor on the space
ô° Note that for an identitô°® covariance matriô°¯ô°¡ô°—ô° Iô°¢ô°° the Mahalanobis distance
becomes the familiar Euclidean distance
i
i
    Mahalanobis Distance
xô°‚y 2 ô°ƒ(xô°‚y)T Î£ô°‚1(xô°‚y) Î£ô°‚1
        Intelligent Information & Signal Processing Lab. School of EE, Korea University 43
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
  Case 3: S = S (S non-diagonal) Case 1: Î£ğ‘– = ğœ2I
 Case 3 :iô°šiô°¬ô°š (ô°š non-diagonal)
ô° Expansion of the quadratic term in the discriminant yields
 gi(x)ô°ƒô°‚12(xô°‚Î¼i)T Î£ô°‚1(xô°‚Î¼i)ô°‰log(P(ô°…i)) ô°ƒô°‚1ô°ŒxTÎ£ô°‚1xô°‚2Î¼TÎ£ô°‚1xô°‰Î¼TÎ£ô°‚1Î¼ ô°ô°‰log(P(ô°… ))
2
iiii
ô° Removing the term xT ô°—-1 x, which is constant for all classes g (x)ô°ƒô°‚1ô°Œô°‚2Î¼TÎ£ô°‚1xô°‰Î¼TÎ£ô°‚1Î¼ ô°ô°‰log(P(ô°… ))
i2iiii ô° Reorganizing terms we obtain
 g (x)ô°ƒô°…Txô°‰ô°… iii0
ô°ô°… ô°ƒÎ£ô°‚1Î¼ where ô°›ô° i 1 i
ô°›ô°œô°… ô°ƒô°‚2Î¼TÎ£ô°‚1Î¼ ô°‰log(P(ô°…)) i0iii
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 44
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[5 4]T Î¼3 ô°ƒ[2 5]T
Î£ô°ƒô°•1 0.7ô°’Î£ô°ƒô°•1 0.7ô°’Î£ô°ƒô°•1 0.7ô°’ 1 ô°“ô°”0.7 2 ô°ô°‘ 2 ô°“ô°”0.7 2 ô°ô°‘ 3 ô°“ô°”0.7 2 ô°ô°‘
  
  Case 3: S = S (S non-diagonal) Case 1: Î£ğ‘– = ğœ2I
 Case 3 :iô°šiô°¬ô°š (ô°š non-diagonal)
ô° Expansion of the quadratic term in the discriminant yields
 gi(x)ô°ƒô°‚12(xô°‚Î¼i)T Î£ô°‚1(xô°‚Î¼i)ô°‰log(P(ô°…i)) ô°ƒô°‚1ô°ŒxTÎ£ô°‚1xô°‚2Î¼TÎ£ô°‚1xô°‰Î¼TÎ£ô°‚1Î¼ ô°ô°‰log(P(ô°… ))
2
iiii
ô° Removing the term   which is constant for all classes g (x)ô°ƒô°‚1ô°Œô°‚2Î¼TÎ£ô°‚1xô°‰Î¼TÎ£ô°‚1Î¼ ô°ô°‰log(P(ô°… ))
i2iiii ô° Reorganizing terms we obtain
 g (x)ô°ƒô°…Txô°‰ô°… ğ’ˆ (ğ’™) = ğ‘¤%ğ‘¥ + ğ‘¤
$i $i $&i0
ô° ô°…ğ‘¤ # ô°ƒ Î£ ô°‚ 1 Î¼
xT ô°—-1 x,
where ô°›ô° i! 1 i
ô°›ô°œô°…ğ‘¤!" ô°ƒô°‚2Î¼TÎ£ô°‚1Î¼ ô°‰log(P(ô°…))
i0iii
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[5 4]T Î¼3 ô°ƒ[2 5]T
Î£ô°ƒô°•1 0.7ô°’Î£ô°ƒô°•1 0.7ô°’Î£ô°ƒô°•1 0.7ô°’ 1 ô°“ô°”0.7 2 ô°ô°‘ 2 ô°“ô°”0.7 2 ô°ô°‘ 3 ô°“ô°”0.7 2 ô°ô°‘
     Intelligent Information & Signal Processing Lab. School of EE, Korea University 44
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
  Case1:Î£ =ğœ2I Case 3 : ô°š ô°¬ô°š (ô°š non-diagonal)
 Case 3: S = S (ğ‘–S non-diagonal)
i
i
  This discriminant is linear, so the decision boundaries will also be hyper- planes
ô°
ô° The constant probability loci are hyper-ellipses aligned with the
eigenvectors of ô°—
ô° If we can assume equal prior
g i ( x ) ô°ƒ ô°‚ 12 ( x ô°‚ Î¼ i ) T Î£ ô°‚ 1 ( x ô°‚ Î¼ i )
ô° The classifier becomes a minimum (Mahalanobis) distance classifier
       Intelligent Information & Signal Processing Lab. School of EE, Korea University 45
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
  Case1:Î£ =ğœ2I Case 3 : ô°š ô°¬ô°š (ô°š non-diagonal)
 Case 3: S = S (ğ‘–S non-diagonal)
i
i
  This discriminant is linear, so the decision boundaries will also be hyper- planes
ô°
ô° The constant probability loci are hyper-ellipses aligned with the
eigenvectors of ô°—
ô° If we can assume equal prior
g i ( x ) ô°ƒ ô°‚ 12 ( x ô°‚ Î¼ i ) T Î£ ô°‚ 1 ( x ô°‚ Î¼ i )
ô° The classifier becomes a minimum (Mahalanobis) distance classifier
       Intelligent Information & Signal Processing Lab. School of EE, Korea University 45
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
   Case 3 : ô°šiô°¬ô°š (ô°š non-diagonal), example
ô°€ To illustrate the previous result, we will compute the decision boundaries for a 3-class, 2-dimensional problem with the following class mean vectors and
covariance matrices and equal
priors
  Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[5 4]T Î¼3 ô°ƒ[2 5]T Î£ô°ƒô°•1 0.7ô°’Î£ô°ƒô°•1 0.7ô°’Î£ô°ƒô°•1 0.7ô°’
1 ô°“ô°”0.7 2 ô°ô°‘ 2 ô°“ô°”0.7 2 ô°ô°‘ 3 ô°“ô°”0.7 2 ô°ô°‘
      Intelligent Information & Signal Processing Lab.
School of EE, Korea University 46
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
  
   Case 4 : ô°±i=ô°²i2I
ô°€ In this case, each class has a different covariance matrix, which is proportional to the identity matrix
 ô° The quadratic discriminant becomes g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…))
    i2iii2ii
ô°ƒ ô°‚ 1 (x ô°‚ Î¼ )T ô°™ ô°‚2 (x ô°‚ Î¼ ) ô°‚ 1 N log(ô°™ 2 ) ô°‰ log(P(ô°… ))
2iii2ii
ô° This expression cannot be reduced further so
ô° The decision boundaries are quadratic: hyper-ellipses
ô° The loci of constant probability are hyper-spheres aligned with the feature axis
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 47
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
  
  Case 4 : ô°±i=ô°²i2I
ô°€ In this case, each class has a different covariance matrix, which is proportional to the identity matrix
 ô° The quadratic discriminant becomes g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…))
    i2iii2ii
ô°ƒ ô°‚ 1 (x ô°‚ Î¼ )T ô°™ ô°‚2 (x ô°‚ Î¼ ) ô°‚ 1 N log(ô°™ 2 ) ô°‰ log(P(ô°… ))
2iii2ii ô°
 This expression cannot be reduced further so
ô° The decision boundaries are quadratic: hyper-ellipses
ô° The loci of constant probability are hyper-spheres aligned with the feature axis
    Intelligent Information & Signal Processing Lab. School of EE, Korea University
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
47
  
   Case 4: ô°³i=ô°˜i2I, example
ô°€ To illustrate the previous result, we will compute the decision boundaries for a 3-class, 2-dimensional problem with the following class mean vectors and
covariance matrices and equal
  priors
  Zoom out
       Intelligent Information & Signal Processing Lab.
School of EE, Korea University 48
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
  
  Case5: Si 1Sj
Case 5 : ô°±iô°´ô°±j general case
  ô°€ We already derived the expression for the general case at the beginning of this discussion
g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…)) i2iii2ii
  Case5: Si 1Sj
ô° Reorganizing terms in a quadratic form yields
 TT
ğ‘”g ( ğ’™x ) =ô°ƒ xğ‘¥ % ğ‘¤W ğ’™x +ô°‰ ğ‘¤ô°… % ğ’™x +ô°‰ ğ‘¤ô°…
$i
$ i $i $i&0 ô°W ô°ƒô°‚1Î£ô°‚1
ô°›ô°›i 2i where ô°ô°…ğ‘¤# ô°ƒ Î£ô°‚1 Î¼
ô°› !i i 1 i 1
ô°… ô°ƒô°‚ Î¼TÎ£ô°‚1Î¼ô°‚ log(Î£)ô°‰log(P(ô°…)) ô°›ğ‘¤!i"0 2i i i 2 i i
  ô°œ
ô° The decision boundaries are again quadratic : hyper-ellipses or hyper-parabolloids
ô° Notice that the quadratic expression in the discriminant is proportional to the Mahalanobis distance using the class-conditional covariance ô°—i
ô° The loci of constant probability for each class are hyper-ellipses, oriented with the eigenvectors of ô°—i for that class
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 49
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[5 4]T Î¼3 ô°ƒ[2 5]T
Î£ ô°ƒô°•1 ô°‚1ô°’ Î£ ô°ƒô°•1 ô°‚1ô°’ Î£ ô°ƒô°•0.5 0.5ô°’ 1 ô°“ô°” ô°‚ 1 2 ô°ô°‘ 2 ô°“ô°” ô°‚ 1 7 ô°ô°‘ 3 ô°“ô°” 0 . 5 3 ô°ô°‘
  
  Case5: Si 1Sj
Case 5 : ô°±iô°´ô°±j general case
   ô°€ We already derived the expression for the general case at the beginning of this discussion
g(x)ô°ƒô°‚1(xô°‚Î¼)TÎ£ô°‚1(xô°‚Î¼)ô°‚1log(Î£ )ô°‰log(P(ô°…)) i2iii2ii
  ô° Reorganizing terms in a quadratic form yields
 TT
ğ‘”g ( ğ’™x ) =ô°ƒ xğ‘¥ % ğ‘¤W ğ’™x +ô°‰ ğ‘¤ô°… % ğ’™x +ô°‰ ğ‘¤ô°…
$i
$ i $i $i&0 ô°W ô°ƒô°‚1Î£ô°‚1
ô°›ô°›i 2i where ô°ô°…ğ‘¤# ô°ƒ Î£ô°‚1 Î¼
ô°› !i i 1 i 1
ô°… ô°ƒô°‚ Î¼TÎ£ô°‚1Î¼ô°‚ log(Î£)ô°‰log(P(ô°…)) ô°›ğ‘¤!i"0 2i i i 2 i i
  ô°œ
ô° The decision boundaries are again quadratic : hyper-ellipses or hyper-parabolloids
ô° Notice that the quadratic expression in the discriminant is proportional to the Mahalanobis distance using the class-conditional covariance ô°—i
ô° The loci of constant probability for each class are hyper-ellipses, oriented with the eigenvectors of ô°—i for that class
     Intelligent Information & Signal Processing Lab. School of EE, Korea University 49
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[5 4]T Î¼3 ô°ƒ[2 5]T
Î£ ô°ƒô°•1 ô°‚1ô°’ Î£ ô°ƒô°•1 ô°‚1ô°’ Î£ ô°ƒô°•0.5 0.5ô°’ 1 ô°“ô°” ô°‚ 1 2 ô°ô°‘ 2 ô°“ô°” ô°‚ 1 7 ô°ô°‘ 3 ô°“ô°” 0 . 5 3 ô°ô°‘
  
  Case5: Si 1Sj
Case ô°µ : ô°šiô°¶ô°šj general case, example
ô°€ To illustrate the previous result, we will compute the decision boundaries for a 3-class, 2-dimensional problem with the following class mean vectors and
covariance matrices and equal
    priors
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[5 4]T Î¼3 ô°ƒ[2 5]T
Î£ ô°ƒô°•1 ô°‚1ô°’ Î£ ô°ƒô°•1 ô°‚1ô°’ Î£ ô°ƒô°•0.5 0.5ô°’
1 ô°“ô°” ô°‚ 1 2 ô°ô°‘ 2 ô°“ô°” ô°‚ 1 7 ô°ô°‘ 3 ô°“ô°” 0 . 5 3 ô°ô°‘
   Zoom out
     Intelligent Information & Signal Processing Lab.
School of EE, Korea University 50
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
Case5: Si 1Sj
Case ô°µ : ô°šiô°¶ô°šj general case, example
ô°€ To illustrate the previous result, we will compute the decision boundaries for a 3-class, 2-dimensional problem with the following class mean vectors and
covariance matrices and equal
      priors
Î¼1 ô°ƒ[3 2]T Î¼2 ô°ƒ[5 4]T Î¼3 ô°ƒ[2 5]T
Î£ ô°ƒô°•1 ô°‚1ô°’ Î£ ô°ƒô°•1 ô°‚1ô°’ Î£ ô°ƒô°•0.5 0.5ô°’
1 ô°“ô°” ô°‚ 1 2 ô°ô°‘ 2 ô°“ô°” ô°‚ 1 7 ô°ô°‘ 3 ô°“ô°” 0 . 5 3 ô°ô°‘
   Zoom out
     Intelligent Information & Signal Processing Lab.
School of EE, Korea University 50
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
  
 Numerical Example Case 1: Î£ğ‘– = ğœ2I
â€¢ Derive a linear discriminant function for a two class 3D classification problem
defined by the following Gaussian Likelihoods
 TT
Î¼1=[0 0 0];Î¼2=[1 1 1];S1=S2=Ãª0 4 0Ãº;P(w2)=2P(w1)
Ã©14 0 0Ã¹ ÃªÃº
 Ãª1Ãº Ãª001Ãº
  Ã« 1 4Ã»2 Since P(w1)+P(w2)=1, P(w1)+2P(w1)=1, thus P(w1)=3 ,P(w2)=3
â€¢Solution
Ã©x -Î¼ Ã¹T Ã©x -Î¼ Ã¹ 1 T 1 Ãª1 1ÃºÃª1 1Ãº
gi (x)=-2s2 (x-Î¼i ) (x-Î¼i )+log(P(wi ))= Ã¦1Ã¶Ãªx2 -Î¼2Ãº Ãªx2 -Î¼2Ãº+log(P(wi )) 2Ã§ Ã·Ãªx -Î¼ Ãº Ãªx -Î¼ Ãº
  ğ‘¥"âˆ’0% ğ‘¥"âˆ’0 " ğ‘”" ğ± = âˆ’2 ğ‘¥# âˆ’ 0 ğ‘¥# âˆ’ 0 + ğ‘™ğ‘œğ‘” '
ğ‘¥' âˆ’ 0 ğ‘¥' âˆ’ 0
ğ‘¥"âˆ’1% ğ‘¥"âˆ’1 # ğ‘”# ğ± = âˆ’2 ğ‘¥# âˆ’ 1 ğ‘¥# âˆ’ 1 + ğ‘™ğ‘œğ‘” '
ğ‘¥' âˆ’ 1 ğ‘¥' âˆ’ 1
Ã¨4Ã¸Ã«3 3Ã»Ã«3 3Ã»
      
 Numerical Example
w1
2
1>(222)2 g (x)=-2(x2 +x2 +x2)+log g (x)=-2 (x -1) +(x -1) +(x -1) +log
1 1233w<2 3
w2
x+x+x > 6-log2=1.32
123<4 w1
123
 â€¢ Classify a test example: xu=[0.1 0.7 0.8]T w2
> 0.1+0.7+0.8=1.6 < 1.32 => xu Ãw2
w1

 Can you predict if they will play?
â€¢ What is the probability of a couple playing tennis on a sunny and cold day given the past history of them playing at various conditions?
â€¢ Notethattherearetwoinput features(x): Outlook and Temperature
â€¢ Outlookcanhave3values: â€¢ Sunny
â€¢ Overcast â€¢ Rain
â€¢ Temperaturecanhave3values: â€¢ High
â€¢ Mild â€¢ Cold
       Outlook
Temperatur e
Play=Yes
Play=No
 Sunny
High
0
2
 Sunny
Mild
2
1
 Sunny
Cold
2
0
 Overcast
High
1
1
 Overcast
Mild
2
0
 Overcast
Cold
1
1
 Rain
High
1
2
 Rain
Mild
0
1
 Rain
Cold
0
1
 9
9
                      
 Can you predict if they will play?
â€¢ Apply Bayesian classification
P(Play Sunny,Cold) = p(Sunny,Cold Play)P(Play) p(Sunny,Cold)
           Outlook
Sunny
Sunny
Sunny
Overcast
Temperatur e
High
Mild
Cold
High
Play=Yes
2
2
Play=No
 0
2
 1
 0
 1
1
 Overcast
Overcast
Rain
Rain
Mild
Cold
Mild
Cold
2
1
1
0
 1
 Rain
High
2
 0
0
1
 1
  9
9
        By MAP
Is p(Sunny, Cold | Play)P(Play) > p(Sunny, Cold | No)P(Not Play) ? (2/9)x(1/2) > (0/9)x((1/2)
Thus they will play!
                           Note each of these are a unique combination of features

 Can you predict if they will play?
Can you predict if the person will play when: â€¢ Outlook = Sunny
â€¢ Temperature = Cool â€¢ Humidity = High
â€¢ Wind = Strong
  59
 More complicated past history of playing tennis
NaÃ¯ve Bayes classifier (example)
     PlayTennis: Trainingexamples
 Day
Outlook
ğ‘11 ğ‘ ğ‘¢ğ‘›ğ‘›ğ‘¦ ğ’™1 ô°· ğ‘12 ô°· ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘ğ‘ ğ‘¡
ğ‘13 ğ‘Ÿğ‘ğ‘–ğ‘›
  Temperature
ğ‘21 ğ»ğ‘œğ‘¡ ğ’™2 ô°· ğ‘22 ô°· ğ‘€ğ‘–ğ‘™ğ‘‘ ğ‘23 ğ¶ğ‘œğ‘œğ‘™
  Humidity
ğ’™3 ô°· ğ‘31 ô°· ğ»ğ‘–ğ‘”h ğ‘32 ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™
Wind
ğ’™4 ô°· ğ‘41 ô°· ğ‘†ğ‘¡ğ‘Ÿğ‘œğ‘›ğ‘” ğ‘42 ğ‘Šğ‘’ğ‘ğ‘˜
Play Tennis
D1
Sunny
Hot
High
Weak
No
D2
Sunny
Cool
High
Strong
No
D3
Overcast
Hot
High
Weak
Yes
D4
Rain
Mild
High
Weak
Yes
D5
Rain
Cool
Normal
Weak
Yes
D6
Rain
Cool
Normal
Strong
No
D7
Overcast
Cool
Normal
Strong
Yes
D8
Sunny
Mild
High
Weak
No
D9
Sunny
Cool
Normal
Weak
Yes
D10
Rain
Mild
Normal
Weak
Yes
D11
Sunny
Mild
Normal
Strong
Yes
D12
Overcast
Mild
High
Strong
Yes
D13
Overcast
Hot
Normal
Weak
Yes
D14
Rain
Mild
High
Strong
No
                          Intelligent Information & Signal Processing Lab. School of EE, Korea University 59
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier

  60
Can you predict if they will play?
 NaÃ¯ve Bayes classifier
 â€¢ There are now a total of 4 features
â€¢ Outlook:3possiblevalues
â€¢ Temperature:3possiblevalues â€¢ Humidity:2possiblevalues
â€¢ Wind:2possiblevalues
â€¢ Total possible feature combinations â€¢ 3x3x2x2=36
â€¢ Number of data points: 14
â€¢ Insufficient data to fully cover the
feature space!
â€¢ Apply NaÃ¯ve Bayes
â€¢ Learning Phase: Play=Yes 9 times, Play=No 5 times
  Outlook
 Play=Yes
 Play=No
Sunny
2/9
3/5
Overcast
4/9
0/5
Rain
3/9
2/5
Temperature
Play=Yes
Play=No
Hot
3/9
1/5
Mild
4/9
2/5
Cool
2/9
2/5
   Humidity
 Play=Yes
 Play=No
High
3/9
4/5
Normal
6/9
1/5
 Wind
Play=Yes
Play=No
Strong
3/9
3/5
Weak
6/9
2/5
  P(Play=Yes) = 9/14
P(Play=No) = 5/14
Prior
     Intelligent Information & Signal Processing Lab. School of EE, Korea University 60
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
  
 NaÃ¯ve Bayes Classifier
â€¢ NaÃ¯ve Bayes: a simple probabilistic classifer based on applying Bayesâ€™ rule with strong (naÃ¯ve) independence assumptions between the features
12dkk
P(C x,x ,.....,x ,)= p(x,x ,...,x C )P(C ) for1Â£kÂ£K
   k12 d
p(x,x ,...,x ) 12d
â€¢ Applying independence assumption
â€¢ Then Bayes rule becomes
 p(x,x,...,x C)P(C)=p(x C)p(x C)....p(x C)P(C) for1Â£kÂ£K 12dkk 1k2k dkk
     P(C x,x,.....,x,)=p(x C)P(C) for1Â£kÂ£K
Ck ) p(x2
Ck )....p(xd
   1kk
  k12 d
p(x1,x2,...,xd )
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier

 NaÃ¯ve Bayes Classifier Example
â€¢ Assuming priors are the same, letâ€™s apply ML
â€¢ Develop likelihoods for each feature â€¢ p(Outlook=Sunny|Play=Yes)=2/9
â€¢ p(Temperature=Cool|Play=Yes)=2/9 â€¢ p(Humidity=High|Play=Yes)=3/9
p(Outlook=Sunny|Play=No)=3/5 p(Temperature=Cool|Play=No)=2/5 p(Humidity=High|Play=No)=4/5 p(Wind=Strong|Play=No)=3/5
â€¢ Assumingpriorsfromthegivendatamaynotalwaysbethebestoptioniftherearepriorknowledgessuch as the couple has been observed on the average playing tennis in equal proportion
â€¢ Decision by the MAP rule with an assumption of equal prior
â€¢ P(Yes|x) ~ p(Sunny|Yes) p(Cool|Yes) p(High|Yes) p(Strong|Yes) P(Yes) = (2/9)(2/9)(3/9)(3/9)(9/14) = 0.003527 â€¢ P(No|x) ~ p(Sunny|No) p(Cool|No) p(High|No) p(Strong|No) P(No) = (3/5)(2/5)(4/5)(3/5)(5/14) = 0.041142
â€¢ Therefore, the prediction is No
From: J. Huang, I. H. Wittenâ€™s and E. Frankâ€™s â€œData Mining,â€ and Jeremy Wyatt
â€¢ p(Wind=Strong|Play=Yes)=3/9
â€¢ Prior probabilities: P(Play=Yes) = 9/14, P(Play=No) = 5/14

 Numerical Stability in NaÃ¯ve Bayes Classifier
â€¢ Note the numbers calculated in the previous example
â€¢ (2/9)(2/9)(3/9)(3/9)(9/14) = 0.003527
â€¢ NaÃ¯ve Bayes calculation involves multiplying numbers that are < 1 with other numbers < 1 many times over
â€¢ The example considered only involve 4 input features, but in general there are far more number of features included in the calculation
â€¢ Multiplying lots of probabilities Ã¨ floating-point underflow.
â€¢ So how do you deal with calculating small numbers?
â€¢ In NaÃ¯ve Bayes classification, we are only interested in comparing numbers rather than computing the exact values
â€¢ Recall: log(xy) = log(x) + log(y)
â€¢ Take logs of probabilities
â€¢ Comparing log values of probabilities is still valid for classification.

 Numerical Stability in NaÃ¯ve Bayes Classifier
â€¢ Class with highest final un-normalized log probability score is still the most probable.
â€¢ Instead of comparing P(Yes|X1,...,Xn) with P(No|X1,...,Xn), â€¢ Compare their logarithms
 From: J. Huang, I. H. Wittenâ€™s and E. Frankâ€™s â€œData Mining,â€ and Jeremy Wyatt

 60
 Zero Frequency Problem in NaÃ¯ve Bayes Classifier
 â€¢ Another prediction: â€¢ Outlook = Overcast â€¢ Temperature = Cool â€¢ Humidity = High
NaÃ¯ve Bayes classifier
â€¢ Learning Phase: Play=Yes 9 times, Play=No 5 times
   Outlook
 Play=Yes
 Play=No
Sunny
2/9
3/5
Overcast
4/9
0/5
Rain
3/9
2/5
Temperature
Play=Yes
Play=No
Hot
3/9
1/5
Mild
4/9
2/5
Cool
2/9
2/5
 â€¢ Wind = Strong
â€¢ But there is no data on not
playing when Outlook = Overcast! â€¢ Solution:
â€¢ Laplace Smoothing by adding a small value to the zero event category and add the same to each of the rest of categories
P(Play=Yes) = 9/14
P(Play=No) = 5/14
 Humidity
 Play=Yes
 Play=No
High
3/9
4/5
Normal
6/9
1/5
 Wind
Play=Yes
Play=No
Strong
3/9
3/5
Weak
6/9
2/5
      Intelligent Information & Signal Processing Lab. School of EE, Korea University 60
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
 60
 Zero Frequency Problem in NaÃ¯ve Bayes Classifier
NaÃ¯ve Bayes classifier
 â€¢ Adding the values is limited to the data
relevanttotheOutlookfeatureâ€¢ LearningPhase: Play=Yes 9times, Play=No 5times
     Outlook
 Play=Yes
 Play=No
Sunny
2/9
3/5
Overcast
4/9
0/5
Rain
3/9
2/5
 â€¢ Changes to the probabilities
 Temperature
 Play=Yes
 Play=No
Hot
3/9
1/5
Mild
4/9
2/5
Cool
2/9
2/5
  Humidity
 Play=Yes
 Play=No
High
3/9
4/5
Normal
6/9
1/5
 Wind
Play=Yes
Play=No
Strong
3/9
3/5
Weak
6/9
2/5
 P(Play=Yes) = 9/14 P(Play=No) = 5/14
     Intelligent Information & Signal Processing Lab. School of EE, Korea University 60
From H. Ko, â€œBayesian Learning, NaÃ¯ve Bayes Classifier
 
 Zero Frequency Problem in NaÃ¯ve Bayes Classifier
â€¢ Decision making with the MAP rule
â€¢ Given that P(Yes|xâ€™ ) < P(No|xâ€™ ), we label xâ€™ to be â€œNoâ€
 
 Reviews of Linear Algebra
ECE 610, spring 2024 David Han Drexel University

 Outline
â€¢ Scalars, Vectors, Matrices, and Tensors â€¢ Multiplying Matrices and Vectors
â€¢ Identity and Inverse Matrices
â€¢ Linear Dependence and Span
â€¢ Norms
â€¢ Special Kinds of Matrices and Vectors â€¢ Eigendecomposition
â€¢ Principal Component Analysis

 Linear Algebra Review
â€¢ Why Linear Algebra?
â€¢ Essential for understanding machine learning algorithms
â€¢ Input to ML algorithms, (x1, x2, ..., xn) are converted to outputs via a series of linear transformations
â€¢ Recall a linear discriminant function ğ’ˆ!(ğ’™) = ğ‘¤!"ğ’™ + ğ‘¤!#
 
 Linear Algebra Review â€¢ Remember NaÃ¯ve Bayes
 P(C x,x,.....,x,)=p(x C)P(C) for1Â£kÂ£K
Ck ) p(x2
Ck )....p(xd
   1kk
  k12 d
p(x1,x2,...,xd )
 ğ’ˆ!(ğ’™) = ğ‘¤!"ğ’™ + ğ‘¤!#

 Linear Algebra Review
â€¢ Scalars: a scalar is just a single number, used to define a magnitude
â€¢ Ex. volume, density, speed, energy, mass, and time
â€¢ Vectors: x vector is an array of numbers.
â€¢ numbers are arranged in order.
â€¢ First element of x is x1, the second element is x2, etc.
â€¢ d-dimensional vector: the vector has d elements, if real valuesÃ¨it resides in Rd.
Ã© x1 Ã¹
x=Ãªx2ÃºandxT=[x x ïŒx]
ÃªïÃº Ãªx Ãº Ã«dÃ»
12d
â€¢ Vectors can be interpreted as identifying points in space, with each element giving the coordinate along a different axis.

 Linear Algebra Review â€¢ Matrices: a 2-D array of numbers
â€¢ For a real valued matrix A with a height of n and a width of d â€¢ AâˆˆRnxd
â€¢ A1,1 is the upper left entry of A and An,d is the bottom right entry â€¢ An n Ã— d (rectangular) matrix can be written as
Ã©a a ïŒ a Ã¹
11 12 1d ÃªaïŒaÃº
ï€¥Ãª Ãºï€¥TÃª12 n2Ãº A=Ãªï ïïïÃºandA=ÃªïïïÃº
Ãªa a ïŒ a Ãº Ãª Ãº Ã«n1 n2 ndÃ» aïŒa
Ã©a ïŒ a Ã¹ 11 n1
Ã« 1d nd Ã»

 Linear Algebra Review
â€¢ Matrix-valued expressions can be defined by index.
â€¢ For example, f(A)i,j gives element (i, j) of the matrix computed by applying the function f to A.

 Linear Algebra Review
 â€¢ Tensor: An array of numbers arranged on a regular grid with variable number of axes.
â€¢ For a 3-D tensor A, the element of A at coordinates (i, j, k) can be by written as Ai,j,k.
â€¢ Thus:
â€¢ A scalar is a tensor with zero axis
â€¢ A vector is a tensor with only one axis â€¢ A matrix is a tensor with 2 axes
â€¢ A tensor can be in higher than 3D
â€¢ Matrix Transpose: mirror image of the matrix across its diagonal
â€¢ Transpose of a matrix A written as AT
scalar
vector
matrix
3D tensor
  Ã©a a ïŒ a Ã¹ 11 12 1d
Ã©a 11
ïŒ
a Ã¹ n1
ÃªaïŒaÃº ï€¥Ãª Ãºï€¥TÃª12 n2Ãº
A = Ãª ï ï ï ï Ãº Ãªa a ïŒ a Ãº
and
A
= Ãª ï ï ï Ãº
Ã«n1 n2 ndÃ»
aïŒa
Ãº
Ãª
Ã« 1d nd Ã»

 Linear Algebra Review â€¢ Transpose of a vector is a matrix of only one row.
Ã© x1 Ã¹
x=Ãªx2ÃºandxT=[x x ïŒx]
ÃªïÃº Ãªx Ãº Ã«dÃ»
12d
â€¢ Transpose of a scaler is the same: a = aâŠ¤
â€¢ Matrix addition: C = A + B where Ci,j = Ai,j + Bi,j
â€¢ Scalar addition and multiplication of a matrix â€¢ D = aÂ·B + c where Di,j =aÂ·Bi,j +c.
â€¢ Adding a vector to a matrix
â€¢ C = A + b, where Ci,j = Ai,j + bj â€¢ called broadcasting

 Linear Algebra Review â€¢ Matrix multiplication
â€¢ C = AB, for this to work, number of columns in A must match number of rows in B
â€¢ IF A is of shape m Ã— d and B is of shape d Ã— n, then C is of shape m Ã— n.
Ã©b ïŒ b Ã¹ Ã©c c ïŒ c Ã¹ 11 1n 11 12 1n
Ã©a a ïŒ a Ã¹
11 12 1d Ãªb ïŒ b Ãº Ãªc c ïŒ c Ãº ÃºÃª21 2nÃº Ãª21 22 2nÃº
ï€¥ï€¥ Ãª
AB=Ãªï ï ï ïÃºÃªï ï ïÃº=Ãªï ï ï ïÃº
ÃªaaïŒaÃºÃª ÃºÃª Ãº Ã«m1 m2 mdÃ»bïŒb ccïŒc
Ã« d1 dn Ã» Ã« m1 m2 mn Ã» â€¢ It can be also written out as
where, cij = Ã¥d aik bkj k=1

 Linear Algebra review
â€¢ Hadamard product: element-wise product of two matrices
â€¢ Denoted as AâŠ™ B
â€¢ WithC=AâŠ™B, ci,j =(ai,j)(bi,j)
â€¢ Matrix multiplication is distributive â€¢ A(B+C)=AB+AC
â€¢ Matrix multiplication is associative â€¢ A(BC) = (AB)C
â€¢ Matrix multiplication is Not commutative â€¢ AB â‰  BA

 Linear Algebra review
â€¢ Dot product between two vectors is commutative
â€¢ xTy = yTx
â€¢ Additional identities
 (A+B)T =AT + BT
(AB)T = BTAT
(AB)-1 = B-1A-1 if individual inverses exist (A-1)T = (AT) -1

 Linear Algebra Review â€¢ Identity and Inverse Matrices
â€¢ Identity matrix, In , preserves n- dimensional vectors as,
â€¢ In âˆˆ Rnxn such that â± x âˆˆ Rn , In x = x Ã©1 0 ... 0 0Ã¹
Ãª0 1 ... 0 0Ãº ÃªÃº
In =Ãª. 0 ... 0 .Ãº Ãª. 0 ... 1 .Ãº
â€¢ Matrix Inverse â€¢ A-1A=I
ÃªÃº
Ãª0 0 ... 0 1Ãº Ã«Ã»

 Linear Algebra Review â€¢ Consider a system of linear equations
â€¢ This can be written in a matrix equation as A x = b
â€¢ where A âˆˆ RmÃ—n is a known matrix, b âˆˆ Rm is a known vector, and x âˆˆ Rn is a vector of unknown variables to be solved
â€¢ If we know the inverse of A, we can solve the equation by premultiplying A-1 to the matrix equation as
 A -1A x = A -1b Ã¨ I x = A -1b Ã¨ x = A -1b

 Linear Algebra Review
â€¢ The following are equivalent: â€¢ A is invertible, i. e. A-1 exists
â€¢ Ax = b has a unique solution
â€¢ Columns of A are linearly independent
â€¢ det (A) â‰  0
â€¢ Ax = 0 has a unique, trivial solution: x = 0

 Machine learning application
â€¢ A linear Classifier
â€¢ An input image: picture
â€¢ Output y predicts the class category: cat, dog, ship.
â€¢ Input converted to vector x
â€¢ y = WxT + b
 Srihari, â€œLinear Algebra for Machine Learning,â€ https://cedar.buffalo.edu/~srihari/CSE676/2%20LinearAlgebra.pdf

 Linear Algebra Review
â€¢ Norm: used to measure magnitude of a vector
â€¢ Lp norm defined as
â€¢ which measures the distance between vector x
x =Ã¦Ã¥xpÃ¶1p pÃ§iÃ·
      from the origin
â€¢ Norm, f(x), satisfies the following properties:
â€¢ f(x)=0 if x = 0
â€¢ f(x+y) â‰¤ f(x) + f(y) from triangular inequality
â€¢ â±ğ›‚âˆˆR,f(ğ›‚x)=âˆ£ğ›‚âˆ£f(x)
Ã¨iÃ¸

 Linear Algebra Review â€¢ L1 norm also known as Manhattan distance
â€¢ L2 norm also called Euclidean norm â€¢ L2 of x = (xT x) 1â„2
â€¢ SquarenormofL2=xT x
â€¢ Unit vector is a vector with unit L2 norm:
â€¢ L2 inefficient when x near origin as it increases slowly â€¢ L1 norm more effective in this case.
â€¢ Lâˆ Norm: max norm defined as
x Â¥ = max xi i
        
 Linear Algebra Review
â€¢ Frobenius Norm: size of a matrix
â€¢ Similar to L2 norm for vectors
Ã¦ 2 Ã¶12 A =Ã§Ã¥A Ã·
â€¢ Defined as
â€¢ Measure the size of matrix in terms of
      F
Ã¨ i,j Ã¸ i, j
Euclidean distance
â€¢ Sum of magnitude of all columns (vectors) in the matrix
â€¢ May be used for regularization
â€¢ Minimize Frobenius norm of weight matrices over layers

Linear Algebra Review â€¢ Symmetric Matrix:
â€¢ A = AT
â€¢ Orthogonal Matrix
â€¢ AT A=AT A=I andA-1 =AT â€¢ Angle Between Vectors
â€¢ Dot product of two vectors can be written in terms of L2 norms and the angle ğœƒ between them
â€¢ aTb = âˆ£âˆ£aâˆ£âˆ£2 âˆ£âˆ£bâˆ£âˆ£ 2 cos(ğœƒ)
  
 Linear Algebra Review â€¢ Cosine Similarity
â€¢ Cosine between two vectors is a measure of their similarity:
â€¢ cos(ğœƒ)= ğ’‚&ğ’ƒ âˆ£âˆ£aâˆ£âˆ£ âˆ£âˆ£bâˆ£âˆ£
â€¢ Orthogonal Vectors: two vectors a and b are orthogonal to each other if ğ’‚ # ğ’ƒ = 0.
 â€¢ Vector Projection
â€¢ Given two vectors a and b, let
b b =     b
be the unit â€¢ Then a = aÃ—b is the projection of a onto a
Ë†
   vector in the direction of b
 Ë† 1
straight loine parallel to b, where
()Ë†
b b
a = a cos q =aÃ—b=aÃ—
     1
    
ï¿¼Linear Algebra Review
â€¢ Determinants
â€¢ Determinant of a square matrix is a mapping to a scalar
×€A×€ det(A) or â€¢
â€¢ Measures how much multiplicaton by the
matrix expands or contracts the space
â€¢ Determinant of product is the product of determinants:
â€¢ det(AB) = det(A)det(B)
A=a b=ad-bc cd
ï¿¼ï¿¼ï¿¼ï¿¼
 Linear Algebra Review â€¢ Matrix Decomposition
â€¢ Matrices can be decomposed into factors, as in integers
â€¢ Ex. 12 = 2 x 2 x 3
â€¢ Decomposition of Matrix A as
â€¢ A = V diag(ğ›Œ) V-1
â€¢ Where V is composed of eigenvectors and ğ›Œ are eigenvalues
Wkikpedia

 Linear Algebra Review
â€¢ What are eigenvectors and eigenvalues?
â€¢ For a square matrix A, we can find a vector v such that the following is true.
â€¢ A v = ğ›Œ v where ğ›Œ is a scalar called eigenvalue
 
 Linear Algebra Review
â€¢ If v is an eigenvalue of A, so is any rescaled vector sv. Additionally, sv still has the same eigenvalue.
â€¢ For more convenience, normalize eigenvector so that you have a unit eigenvector.
ğ’™ ğ’™!
ğ’™# =
â€¢ We can solve for eigenvectors and
eigenvalues as follows: â€¢Av-ğ›Œv=0
â€¢ (A-ğ›Œ)v=0
â€¢ For the equation to have non-trivial solution, det(A - ğ›Œ) = 0

 Linear Algebra Review
â€¢ Example eigenvector calculation
for l=6
Ã©4-6 2 Ã¹Ã©v(1)Ã¹ Ã©0Ã¹
A=Ã©4 2Ã¹ Ãª2 4Ãº
Ãª2 4-6ÃºÃª1(1)Ãº=Ãª0Ãº Ã« Ã»Ã«v2Ã»Ã«Ã»
Ã«Ã»
Av=lv 1212
(A-l)v = 0
For non-trivial solution, det(A - l ) = 0
4-l 2 2 4-l
(4-l)2 -4=0
l2 -8l +12 = 0, l=6 and l=2
v(1) = 1 , and similarly v(2) = 1 ÃªÃº ÃªÃº
-2v(1) + 2v(1) = 0, v(1) = v(1)
Ã©Ã¹ Ã©Ã¹
1 -1 Ã«Ã» Ã«Ã»
  =0

 Linear Algebra Review
â€¢ Eigen Decomposition of Matrix
â€¢ Every real symmetric matrix A can be decomposed into real-valued eigenvectors and eigenvalues
â€¢ A = V ğš² VT
â€¢ where V is an orthogonal matrix
composed of unit eigenvectors of A ğ‘£!!
 ğ‘£ ! = ğ‘£#! ğ‘£ ! " = ğ‘£!! ğ‘£#! ğ‘£$! ğ‘£$!
â€¢ with ğš² being a diagonal matrix of eigenvalues {ğ›Œ1, ...., ğ›Œn}
â€¢ Thus, v(1)Tv(1) = 1 while v(i)Tv(j) = 0 when i â‰  j as eigenvectors are orthogonal
 
 Linear Algebra Review
â€¢ Recall A v = ğ›Œ v
â€¢ For an eigenvector, v(i), matrix A scales the eigenvector by ğ›Œi in the direction of v(i)
â€¢ An example of 2x2 matrix
â€¢ Two orthonormal eigenvectors v(1) and v(2)
  
     Linear Algebra Review â€¢ Principal Component Analysis (PCA)
â€¢ A method of reducing dimensionality of a dataset
â€¢ Process
â€¢ Compute covariance matrix A of the
dataset
â€¢ Compute eigenvalues of A !
â€¢ Compute associated eigenvectors ğ‘£
â€¢ Sort eigenvalues and eigenvectors in order of sizes of eigenvalues
â€¢ Determine the reduction of data dimensions
â€¢ Form a matrix composed of eigenvectors according to the new dimensions
â€¢ Transform the data into a new feature space with reduced dimensions
     Programmathically, (2022), â€œPrincipal Components Analysis Explained for Dummies,â€ https://programmathically.com/principal-components-analysis-explained-for-dummies/

 Linear Algebra Review â€¢ Get the data
      Student ID
Math
English
Arts
1
90
60
90
2
90
90
30
3
60
60
60
4
60
60
90
5
30
30
30
                  â€¢ Compute covariance matrix A of the dataset
504 360 180 360 360 0 180 0 720
 â€¢ Compute eigenvalues of A
ğœ† â‰ˆ 44.81966..... ğœ† â‰ˆ 629.11039.... ğœ† â‰ˆ 910.06995....

 Linear Algebra Review
â€¢ Compute associated eigenvectors ğ’— 4
â€¢ Sort eigenvalues and eigenvectors in descending order of eigenvalues
â€¢ Determine the reduction of data dimensions: keep 2 D
â€¢ Form a matrix V composed of eigenvectors according to the new dimensions
ğ’— =
âˆ’3.75100 ... âˆ’0.50494 ... 1.05594 ... 4.28441... ğ’— = âˆ’0.67548... ğ’— = 0.69108...
111
   910.06995 ... 629.11039 ... 44.81966 ...
   ğ‘½ =
1.05594 ... 0.69108 ...
âˆ’0.50494 ...
âˆ’0.67548 ... 11

 Linear Algebra Review
â€¢ Transform the data into a new feature space with reduced dimensions
â€¢ 1st data point
(#) 90
â€¢ğ‘¥ =60 90
 â€¢ Transform the data
ğ‘» ğŸ 1.05594 0.69108 1 90
  ğ’š = ğ‘½ ğ’™ = âˆ’0.50494 âˆ’0.675481 1 60 90

 Learning Loss and Gradient Descent
ECE 610 David Han Drexel University

Linear Regression
â€¢ Simplest machine learning problem: Linear Regression â€¢ Fit a straight line for a bunch of data points
Example: Housing Prices = ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘ğ‘›ğ‘¡ + ğ‘ ğ‘™ğ‘œğ‘ğ‘’ Ã— ğ»ğ‘œğ‘¢ğ‘ ğ‘’ ğ‘†ğ‘ğ‘¢ğ‘ğ‘Ÿğ‘’ ğ‘“ğ‘œğ‘œğ‘¡ğ‘ğ‘”ğ‘’
â€¢ Housing Price is a linear function of the input feature ğ‘ ğ‘ğ‘¢ğ‘ğ‘Ÿğ‘’ ğ‘“ğ‘œğ‘œğ‘¡ğ‘ğ‘”ğ‘’
Housing Prices (Portland, OR)
Price (in 1000 $)
  Size (feet2)

 Linear Regression: a machine learning problem
 Size in feet2 (x)
Price ($) in 1000's (y)
2104 1416 1534 852 ...
460 232 315 178 ...
   Training set of housing prices
    Notation:
m = Number of training examples
xâ€™s = â€œinputâ€ variable / features
yâ€™s = â€œoutputâ€ variable / â€œtargetâ€ variable
Training Set
Learning Algorithm
  Size of house
Estimated price
h
Question : How do we describe h ?
  
 Linear Regression
â€¢ Housing Price is a linear function of the input feature ğ‘ ğ‘ğ‘¢ğ‘ğ‘Ÿğ‘’ ğ‘“ğ‘œğ‘œğ‘¡ğ‘ğ‘”ğ‘’ Housing Prices = ğœƒ! + ğœƒ"Ã— ğ»ğ‘œğ‘¢ğ‘ ğ‘’ ğ‘†ğ‘ğ‘¢ğ‘ğ‘Ÿğ‘’ ğ‘“ğ‘œğ‘œğ‘¡ğ‘ğ‘”ğ‘’
â€¢ Model Parameters to be found: ğœƒ!, ğœƒ" for 1-D case
â€¢ More generally for more than one input feature: ğ‘¥# where ğ‘› > 1
ğ‘¦5 = ğœƒ ! + ğœƒ " ğ‘¥ " + ğœƒ # ğ‘¥ # + â‹¯ ... ğœƒ $ ğ‘¥ $
â€¢ where
â€¢ ğ‘¦! is predicted value of the linear model â€¢ n is the number of features
â€¢ ğ‘¥! is the ğ‘–"# feature value
â€¢ ğœƒ% is the ğ‘—"# model parameter with ğœƒ& as the bias term and ğœƒ', ğœƒ(,......ğœƒ) as the feature weights

 Linear Regression
â€¢ O r ğ‘¦7 = h ÆŸ ( X ) = ğ›‰ Â· ğ‘¿
â€¢ Where hÆŸ is the hypothesis function with model parameters ğ›‰
â€¢ With ğ›‰ = [ğœƒ", ğœƒ#, ğœƒ$,......., ğœƒ%] and ğ‘¿ = [ğ‘¥", ğ‘¥#, ğ‘¥$,......, ğ‘¥%] and ğ‘¥"=1.
ğ‘¦7 = ğœƒ " + ğœƒ # ğ‘¥ # + ğœƒ $ ğ‘¥ $ + â‹¯ ... ğœƒ % ğ‘¥ %

 Linear Regression
â€¢ For 1D feature case
â€¢ How do you determine the model parameters: ğœƒ!, ğœƒ"?
â€¢ How do you make the machine find them for us?
       ğœƒ! = 290 ğœƒ! = 0 ğœƒ" = 0 ğœƒ" = 0.2
ğœƒ! = 100 ğœƒ" = 0.2

 Linear Regression
â€¢ Find the parameters that minimizes an error.
 â€¢ (This is what most of ML is all about!)
â€¢ Define errors or loss function For the ğ‘–th data point (X("), y ("))
Error = hÆŸ (X(")) - y (")
= ğ‘¦' ( X ( " ) ) - y ( " ) = ğ›‰ Â· X ( " ) âˆ’ y ( " )
Housing Prices (Portland, OR)
â€¢ Many different terms for the same meaning in ML: â€¢ lossfunction,costfunction,objectivefunction,etc.
â€¢ ML performance: how well the loss function is defined â€¢ Loss function notation includes (hÆŸ (X(,)),y (,))
m
Sumofsquareerrors= qï§X(i) -y(i) ,ğ‘¤hğ‘’ğ‘Ÿğ‘’ğ‘šğ‘–ğ‘ ğ‘¡hğ‘’ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿğ‘œğ‘“ğ‘‘ğ‘ğ‘¡ğ‘ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ğ‘ 
Ã¥( )2 i=1
Size (feet2)

 Some Distance Measures used as loss functions
 â€¢ Root Mean Square Error (RMSE) =
m i=1
qï§X (i) - y(i) ()
â€¢ Manhattan Distance: ğ¿ #
ğ‘›ğ‘œğ‘Ÿğ‘š= âˆ‘C AB#
1m Ã¥
ÆŸ Â· X(A) âˆ’ y (A)
2
 Ã¦m Ã¶1n
Ã¥ Ã¨ i=1
n
Ã§ qï§X(i) -y(i) Ã·
Ã¸
â€¢ğ¿ğ‘›ğ‘œğ‘Ÿğ‘š=( )
%

 Linear Regression
    Price ($) in 1000â€™s
Size in feet2 (x)
1m 1m Ã¥2Ã¥
2 J(q0,q1)=m (h (X(i) -y(i)) =m (qï§X(i) -y(i))
i=1
q
i=1
  â€¢ By minimizing the error, the best fitting line can be found

 The Normal Equation
â€¢ How do you find ğ›‰ that fits all the data points with the least amount of total error orRMSE? % & (") (") (
MinimumofMSE(ğ›‰)=ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›ğ›‰&âˆ‘"'% ÆŸÂ·X âˆ’y
For m data points with n dimensional features, error between the prediction
 vectorÆŸ Â· X(") and data y can be expressed in matrices as x (1) x (1) ... x (1) Ã© Ã¹ y (1)
 Ã© Ã¹qÃ©Ã¹ Ãª0 1 nÃºÃª0ÃºÃªÃº
Ãªx(2) x(2) ... x(2) ÃºÃªq1Ãº-Ãªy(2) Ãº=Xq-Y 01n
Ãªï ï ... ïÃºÃªïÃºÃªïÃº Ãª ÃºÃªÃºÃªÃº
Ãªx(m) x(m) ... x(m)Ãº q y(m)
Ã«0
1 n Ã»Ã«nÃ»Ã« Ã» Noteğ‘šâ‰ ğ‘› with x0 = 1 to be multiplied to q0
Note i Ã¨ data index
  
 The Normal Equation
â€¢ A closed form solution to the linear regression exists. Let MSE(ğ›‰) = J(ğ›‰)
T sumofthesquareerror=J(q)=(Xq-Y) (Xq-Y)
(T)TT
RewriteJ(q)= (Xq) -YT (Xq-Y)=(Xq) Xq-(Xq) Y-YT (Xq)+YTY
â€¢
 TT
Note (Xq) =qT XT and since Xq and Y are vectors (Xq) Y =YT (Xq)
T
J (q ) = q T X T X q - 2 ( X q ) Y + Y T Y
 Minimize the sum by finding its partial derivatives and set them to zero Â¶J Â¶(T T T T ) T T
Derivative orGradient
  Â¶q=Â¶q q X Xq-2(Xq)Y+Y Y =2X Xq-2X Y=0 Ë† Ë† -1
    XT Xq =2XTY or q =(XT X) XTY Ã theNormalEquation
Ë†
where q is the value of q that minimizes the cost function

 The Normal Equation
â€¢ A closed form solution to the linear regression exists. Let MSE(ğ›‰) = J(ğ›‰)
T sumofthesquareerror=J(q)=(Xq-Y) (Xq-Y)
(T)TT
RewriteJ(q)= (Xq) -YT (Xq-Y)=(Xq) Xq-(Xq) Y-YT (Xq)+YTY
TT
Note (Xq) =qT XT and since Xq and Y are vectors (Xq) Y =YT (Xq)
T
J (q ) = q T X T X q - 2 ( X q ) Y + Y T Y
â€¢
Minimize the sum by finding its partial derivatives and set them to zero Â¶JÂ¶( T )
  Derivative or Gradient
 = qTXTXq-2(Xq)Y+YTY =2XTXq-2XTY=0 Â¶q Â¶q
    Ë† -1
q=(XTX)XTY ÃtheNormalEquation
Ë†
XT Xq =2XTY or
Ë†
where q is the value of q that minimizes the cost function

 The Normal Equation
â€¢ A closed form solution to the linear regression exists. Let MSE(ğ›‰) = J(ğ›‰)
T sumofthesquareerror=J(q)=(Xq-Y) (Xq-Y)
(T)TT
RewriteJ(q)= (Xq) -YT (Xq-Y)=(Xq) Xq-(Xq) Y-YT (Xq)+YTY
TT
Note (Xq) =qT XT and since Xq and Y are vectors (Xq) Y =YT (Xq)
T
J (q ) = q T X T X q - 2 ( X q ) Y + Y T Y
â€¢
Minimize the sum by finding its partial derivatives and set them to zero Â¶JÂ¶( T )
 Derivative or Gradient
 = qTXTXq-2(Xq)Y+YTY =2XTXq-2XTY=0 Â¶q Â¶q
    Ë† -1
q=(XTX)XTY ÃtheNormalEquation
Ë†
XT Xq =2XTY or
Ë†
where q is the value of q that minimizes the cost function

 The Normal Equation
â€¢ A closed form solution to the linear regression exists. Let MSE(ğ›‰) = J(ğ›‰)
T sumofthesquareerror=J(q)=(Xq-Y) (Xq-Y)
(T)TT
RewriteJ(q)= (Xq) -YT (Xq-Y)=(Xq) Xq-(Xq) Y-YT (Xq)+YTY
TT
Note (Xq) =qT XT and since Xq and Y are vectors (Xq) Y =YT (Xq)
T
J (q ) = q T X T X q - 2 ( X q ) Y + Y T Y
â€¢
Minimize the sum by finding its partial derivatives and set them to zero Â¶JÂ¶( T )
 Derivative or Gradient
 = qTXTXq-2(Xq)Y+YTY =2XTXq-2XTY=0 Â¶q Â¶q
    Ë† -1
q=(XTX)XTY ÃtheNormalEquation
Ë†
XT Xq =2XTY or
Ë†
where q is the value of q that minimizes the cost function

Gradient Descent
  â€¢ In most cases, closed form solution doesnâ€™t exist. â€¢Recall ". (,)(,)
# â€¢ How do you find ğ‘ğ‘Ÿğ‘” ğ‘šğ‘–ğ‘›ğ›‰? What are ğœƒ" ğ‘ğ‘›ğ‘‘ ğœƒ#?
MinimumofMSE(ğ›‰)=ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›ğ›‰.âˆ‘,/" ÆŸÂ·X âˆ’y
â€¢ If you began anywhere on this surface, the strategy is to
 walk down hill
â€¢ ğœƒ%GHI = ğœƒJKLLG%I + âˆ†ğœƒMIGN IO POQGL RST
â€¢ Fast way to reach the minimum is with the steepest descent (gradient descent).
â€¢ ğœƒ$012 = ğœƒ34550$2 âˆ’ âˆ‡6ğ½ where âˆ‡ denotes gradient
â€¢ Unlike someone climbing down the slope, you donâ€™t see the landscape other than the parts immediately near (like being in the dark).

Gradient Descent Algorithm
How to get to the bottom?
 Which Way?

 Gradient Descent
Krzyk, â€œCoding Deep Learning for Beginners â€” Linear Regression (Part 3): Training with Gradient Descent,â€ (2018), https://towardsdatascience.com/coding-deep-learning-for-beginners-linear-regression-gradient-descent-fcd5e0fc077d

 Gradient Descent
 Krzyk, â€œCoding Deep Learning for Beginners â€” Linear Regression (Part 3): Training with Gradient Descent,â€ (2018), https://towardsdatascience.com/coding-deep-learning-for-beginners-linear-regression-gradient-descent-fcd5e0fc077d

 Gradient Descent
 Krzyk, â€œCoding Deep Learning for Beginners â€” Linear Regression (Part 3): Training with Gradient Descent,â€ (2018), https://towardsdatascience.com/coding-deep-learning-for-beginners-linear-regression-gradient-descent-fcd5e0fc077d

 Gradient Descent
 Krzyk, â€œCoding Deep Learning for Beginners â€” Linear Regression (Part 3): Training with Gradient Descent,â€ (2018), https://towardsdatascience.com/coding-deep-learning-for-beginners-linear-regression-gradient-descent-fcd5e0fc077d

 Gradient Descent
 Krzyk, â€œCoding Deep Learning for Beginners â€” Linear Regression (Part 3): Training with Gradient Descent,â€ (2018), https://towardsdatascience.com/coding-deep-learning-for-beginners-linear-regression-gradient-descent-fcd5e0fc077d

 Gradient Descent
 Krzyk, â€œCoding Deep Learning for Beginners â€” Linear Regression (Part 3): Training with Gradient Descent,â€ (2018), https://towardsdatascience.com/coding-deep-learning-for-beginners-linear-regression-gradient-descent-fcd5e0fc077d

 Gradient Descent
 Krzyk, â€œCoding Deep Learning for Beginners â€” Linear Regression (Part 3): Training with Gradient Descent,â€ (2018), https://towardsdatascience.com/coding-deep-learning-for-beginners-linear-regression-gradient-descent-fcd5e0fc077d

 Gradient Descent
 Krzyk, â€œCoding Deep Learning for Beginners â€” Linear Regression (Part 3): Training with Gradient Descent,â€ (2018), https://towardsdatascience.com/coding-deep-learning-for-beginners-linear-regression-gradient-descent-fcd5e0fc077d

Gradient Descent
â€¢ You see the error going up and down depending on the step you take.
â€¢ Observe the slope and follow the lowest downhill.
â€¢ Being able to calculate the slope is crucial here! Ã¨ calculate derivatives
  
 Gradient Descent
â€¢ Some conditions for this to work â€¢ Convex
  â€¢ Differentiable
 R. Kwiatkowski (2021)m :Gradient Descent Algorithm â€“ a deep dive,â€ toward data science, https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21

Batch Gradient Descent
 â€¢ Computing partial derivatives
Â¶qj
Ã¥( ) m i=1
j
â€¢ Gradient of MSE is computed by taking the average of the
derivatives over the entire training batch. Thus, it is called batch gradient method.
q(nextstep) =q-hÃ‘qMSE(q) where h is the learning rate
Â¶2
m
MSE(q)= qTx(i)-y(i) x(i)
 â€¢ Calculation involves the full training set
Ã©Ã¹ (x(1) , y(1) ),(x(2) , y(2) ),....,(x(m) , y(m) )
Ã«Ã»
     Ã‘q (MSE)=2XTXq-2XTY=2XT(Xq-Y)

 Gradient Descent
â€¢ In 1-D search
â€¢ How fast to the minimum is how large are the
steps
â€¢ Gradient only tells you the direction and magnitude
â€¢ Step size controlÃ¨learning rate
Small learning rate
   Large learning rate

Batch Gradient Descent
â€¢ Learning rate
  â€¢ Setting a proper learning rate is crucial in machine learning. Often it is not clear what would be the most appropriate.
â€¢ Consider grid search (chap. 2 of Aurelien Geron)

 Gradient Descent Algorithm
â€¢ Repeat until convergence
Correct: Simultaneous update
Incorrect:
         
   (for fixed   , this is a function of x) (function of the parameters   )
  
   (for fixed   , this is a function of x) (function of the parameters   )
  
   (for fixed   , this is a function of x) (function of the parameters   )
  
   (for fixed   , this is a function of x) (function of the parameters   )
  
   (for fixed   , this is a function of x) (function of the parameters   )
  
   (for fixed   , this is a function of x) (function of the parameters   )
  
   (for fixed   , this is a function of x) (function of the parameters   )
  
   (for fixed   , this is a function of x) (function of the parameters   )
  
   (for fixed   , this is a function of x) (function of the parameters   )
  
 Stochastic Gradient Descent
â€¢ Computing gradient by taking a random single instance (single data point) and update the solution value immediately.
â€¢ Takes less time to compute gradient
â€¢ Approach to the solution becomes stochastic as a random sample from the data is taken at each step.
â€¢ More efficient in approaching to optimal solution when the data set is large
Batch vs Stochastic Gradient Descent
 â€¢ Even when the algorithm approaches near an optimal solution, loss function value may
fluctuate, and may not reach the optimal values although the solution will be very close to it.
q
=q -hÃ‘q MSE(q)
â€¢ Gradual reduction of learning rate may help.
â€¢ May work better than batch gradient when the cost function is not convex.
(next step)
where h is the learning rate

 Batch, Stochastic, & Mini-Batch
 
Gradient Descent
â€¢ Convex function: no local minimum and only one global minimum â€¢ For convex functions, gradient descent is guaranteed to work.
â€¢ Feature scaling and gradient descent
Features scaled Features not scaled â€¢ Scaled featuresÃ¨More rapid descent
  
Gradient Descent
   Not so lucky
You need some luck to be in Machine Learning!
   Being lucky here!
 â€¢ Some cost functions (in fact most of the cost functions for interesting problems) are not shaped like bowls (not convex).
â€¢ Thus, there may be local minima on the path to the global minimum.
â€¢ Nonlinear problems such as neural network often results such local minima.
â€¢ Gradient descent may not work well in such cases.

 Gradient Descent
â€¢ Therefore, the path to the minimum can be quite an adventure!
     J(ô°¸0,ô°¸1)
ô°¸0
ô°¸1
    Intelligent Information & Signal Processing Lab. School of EE, Korea University 15

 Logistic Regression
ECE 610 David Han Drexel University

 Probabilistic Interpretation of Linear Regression
â€¢ Letâ€™s consider the linear regression problem in terms of probability and see why the cost function J is a reasonable choice.
â€¢ Letâ€™s assume that the target variables and the inputs are related via the equation â€¢ y(i) = Î¸T x (i) + É›(i)
â€¢ where É›(i)is an error term that captures either unmodeled effects or random noise.
â€¢ Letâ€™s also assume that É›(i)s are distributed IID (Independently and Identically Distributed) according to a Gaussian distribution with zero mean and variance of
ğœ!
p(e(i))=
â€¢ Therefore,
â€¢ Also by substitution p(y(i) x(i);q) =
Ã¦()Ã¶ 1 expÃ§- e(i) Ã·
2ps Ã§ 2s2 Ã· Ã¨Ã¸
2
  ï² 1 2ps
Ã¦ï²2Ã¶ Ã§ (y(i)-qTx(i))Ã·
exp -
Ã§ 2s2 Ã·
   Ã¨Ã¸

 Probabilistic Interpretation of Linear Regresstion
â€¢ â€œp(y(i) |x(i) ; Î¸)â€ indicates that this is the probability distribution of y(i) given x(i) and parameterized by Î¸.
â€¢ In other words, it describes the distribution of the y(i)â€™s, given X (the design matrix, which contains all the x(i)â€™s) and Î¸.
â€¢ We can also call it the likelihood function:
xï²
â€¢ as the probability will change as a function of the choice of Î¸
â€¢ L(Î¸)=L(Î¸;X,y)=p(y|X;Î¸)
â€¢ Now, by assuming that each É›(i) is independent, consider the entire
dataset pair and the likelihood of the dataset occurring
(y(i) -qTx(i)) Ã¦ï²2Ã¶
m ï² m 1
L(q)=Ã•p(y(i) x(i);q)=Ã• i=1 i=1
Ã·
   2ps
expÃ§-
Ã§ 2s2 Ã· Ã¨Ã¸

Probabilistic Interpretation of Linear Regresstion
â€¢ Now, given this probabilistic model relating the y(i)â€˜s and the x(i)â€˜s, what is a reasonable way of choosing our best guess of the parameters Î¸?
â€¢ Maximum likelihood: choose Î¸ to make the data as high probability as possible. â€¢ ChooseÎ¸tomaximizeL(Î¸)
 â€¢ Maximing L(Î¸) may be a bit messy. So letâ€™s maximize its log or max the log likelihood: Ã¦ï²2Ã¶
m 1 (y(i) -qTx(i)) ï¬(q)=logL(q)=logÃ• expÃ§-
Ã·
Ã¦ï²2Ã¶ =Ã¥m log 1 expÃ§-(y(i)-qTx(i))Ã·
i=1 2ps Ã§ 2s2 Ã· Ã¨Ã¸
i=1 2ps Ã§ 2s2 Ã· Ã¨Ã¸
    =mlog
2 - (y(i) -qTx(i))
111Ã¥m ï²
 2ps
s2 2i=1

 Probabilistic Interpretation of Linear Regresstion
â€¢ Discarding the constants, maximizing l (Î¸) then becomes maximizing Ã©1m ï²Ã¹
argmaxÃª-2Ã¥(y(i) -qTx(i))2Ãº q Ã« i=1 Ã»
â€¢ which is the same as minimizing the negative of the summation term. It is the same term to be minimized as in the linear regression!
q
Ã©Ã¥ ï² Ã¹ argmin m (y(i) -qTx(i))2
ÃªÃº Ã«i=1 Ã»
â€¢ Under the probabilistic assumptions on the data, least-squares regression corresponds to finding the maximum likelihood estimate of Î¸.
â€¢ Thus, the least-squares regression can be justified as a method for maximum likelihood estimation.

 Logistic Regression
â€¢ Binary Classification Problems
â€¢ Email: Spam or Ham?
â€¢ ATM machine currency check: Fradulent (yes or no)?
â€¢ Tumor: Malignant or Benign? â€¢ yâˆˆ {0, 1}
â€¢ 0: Negative Class (e.g. benign tumor)
â€¢ 1: Positive Class (e.g. malignant tumor)
   
 Logistic Regression
â€¢ Consider a hypothesis function for 1D feature case of
predicting cancer:
h (x)=qTx=w x+b q1
  â€¢ A threshold based classifier output hÎ¸(x) at 0.5:
â€¢ If hÎ¸(x) â‰¥ 0.5, predict â€œy=1â€ â€¢ If hÎ¸(x) < 0.5, predict â€œy=0â€

 Logistic Regression
â€¢ Linear regression model is not the most suitable one for classification â€¢ Hypothesis function now needs to predict values yâˆˆ {0, 1}.
â€¢ Logistic regression: binary prediction of {0, 1}.
â€¢ Thus, it is the right choice for a binary classification.
h(x)=gqTx = (ï²)1
ï²
 q 1+e-qT x where g(z)= 1
  1+e-z
â€¢ is called logistic function or the sigmoid function

 Logistic Regression
â€¢ An alternative way to fit the data
       Decision Boundary

 Logistic Regression
â€¢ Notice that g(z) tends towards 1 as z â†’ âˆ, and g(z) tends towards 0 as z â†’ âˆ’âˆ.
â€¢ Moreover, g(z), and hence also h(x), is always bounded between 0 and 1. As before, we are keeping the convention of letting x0 = 1, so that
as g
g'(z)= d 1 = 1 (e-z)
=
ï²n qTx=q +Ã¥q x
0
j=1
jj
â€¢ Some useful property of the derivative of the sigmoid function written
  dz 1+e-z
2
1+e
( -z)
1Ã¦1Ã¶
Ã§1- Ã· = g(z)(1- g(z))
( -z)Ã§ ( -z)Ã· 1+e Ã¨ 1+e Ã¸
  
 Logistic Regression
â€¢ Recall for N-dimensional features (x)
â€¢ Î¸Tx = [Î¸0, Î¸1, Î¸2, ...., Î¸N]T[x0, x1, x2, ...., xN] â€¢ where x0= 1
â€¢ Letâ€™s make a slight notational adjustment:
â€¢ Î¸Tx = [b, w1, w2, ...., wN]T[x0, x1, x2, ...., xN] â€¢hÎ¸(x)= h(Î¸Tx)=h(b+w1 x1 +w2 x2+....+wNxN)
â€¢ Thus, for 1-D feature â€¢hÎ¸(x) = h(w1 x1 + b)

 Logistic Regression
â€¢ How do we set Î¸ to best fit the data? This is now logistic regression, not linear regression.
â€¢ Using the same probabilistic assumption on the data, fit the parameters via Maximum Likelihood.
â€¢ Letâ€™s assume
â€¢ P(y = 1 | x; w,b) = hw,b(x)
â€¢ P(y = 0 | x; w,b)=1 âˆ’ hw,b(x)
â€¢ More compactly
â€¢ P(y | x; w,b)=(hw,b(x))y(1-hw,b(x))1-y

 Logistic Regression
â€¢ With m training examples assumed to be generated independently, we can then write down the likelihood of the parameters as
mï² L(w,b)= p(y X;w,b)=Ã•p(y(i) x(i);w,b)
i=1
The objective here is to
  maximize the likelihood by w and m ï² y(i) ï² 1-y(i) b to match the occurrence of
= h x(i) 1-h x(i) Ã•(())( ())
data composed of (x, y) pairs. â€¢ As before, it is easier to maximize the log likelihood
$
ğ‘™ ğ‘¤,ğ‘ =logğ¿ ğ‘¤,ğ‘ = + ğ‘¦(!)logh',) ğ±(!) + 1âˆ’ğ‘¦(!) log 1âˆ’h',) ğ±(!) !"#
w,b w,b i=1
   
 Logistic Regression
â€¢ How do we maximize the likelihood?
â€¢ As in linear regression, use gradient ascent.
â€¢ Written in vectorial notation, our updates will be given by
w(next step) = w+hÃ‘ ï¬(w,b) w
b(next step) =b+hÃ‘ ï¬(w,b) b
where h is the learning rate
â€¢ Note the positive rather than negative sign in the update formula, since weâ€™re maximizing, rather than minimizing, a function now.

 Logistic Regression
â€¢ Forbatchgradientascent,weneedtosumupforallthedatapointsas $
ğ‘™ ğ‘¤,ğ‘ =logğ¿ ğ‘¤,ğ‘ = + ğ‘¦(!)logh',) ğ±(!) + 1âˆ’ğ‘¦(!) log 1âˆ’h',) ğ±(!) !"#
â€¢ Alternatively, taking one training example (x, y) and immediately take derivatives to update w and b is called stochastic gradient ascent rule:
â€¢ Forstochasticgradientascent,thesummationgoesawayasweonlyhavetodealwithonedatapoint ğ‘™ ğ‘¤,ğ‘ =ğ‘¦(3)logh5,6 ğ±(3) + 1âˆ’ğ‘¦(3) log 1âˆ’h5,6 ğ±(3)
â€¢ Letâ€™ssubstitutesigmoidfunction
ğ‘™ ğ‘¤,ğ‘ =ğ‘¦(3)log 1 + 1âˆ’ğ‘¦(3) log 1âˆ’ 1 1+ğ‘’7 5!ğ±86 1+ğ‘’7 5!ğ±86
               
 Logistic Regression
â€¢ For compactness
â€¢ where g(wTx+b)=
Ã© Ã¹ Ã© Ã¹ ï¬(w,b)=y(i)log g(wTx+b)+(1-y(i))log 1-g(wTx+b)
1
1+ e-(wT x+b)
Ã«Ã»Ã«Ã»
 â€¢Computinggradient Â¶()Ã¦ 1 () 1 Ã¶Â¶( )
oftheloglikelihood ï¬ w,b =Ã§y - 1-y Ã· g wTx+b
   w.r.t. wj j
() () Ã¨Ã¸
j
â€¢ But recall the derivative of g
â€¢ Thus
Â¶w Ã§ g wTx+b 1-g wTx+b Ã·Â¶q
 g ' ( z ) = g ( z ) (1 - g ( z ) ) Â¶Ã¦11Ã¶()Â¶
(wT x+b )
ï¬ ( w, b ) = Ã§ y - (1 - y ) Ã· g (wT x+b ) 1 - g (wT x+b )
Â¶w Ã§ g(wTx+b) 1-g(wTx+b)Ã· Â¶w jÃ¨Ã¸j
    
 Logistic Regression
Â¶ Â¶wj
=(y-h (x))x w,b j
ï¬ w,b =Ã§y - 1-y Ã· g wTx+b Â¶()Ã¦1()1Ã¶Â¶()
â€¢ Continue calculating the gradient
â€¢ Similarly for b
ï¬(w,b)= y1-g(wTx+b)-(1-y)g(wTx+b)x (())
j
 Â¶b Ã§ g(wTx+b) 1-g(wTx+b)Ã·Â¶q Ã¨Ã¸j
   Ã¦1 1Ã¶
()
= Ã§ y - (1 - y ) Ã· g (wT x+b ) 1 - g (wT x+b )
Ã§ g(wTx+b) 1-g(wTx+b)Ã· Ã¨Ã¸
  =y1-gwTx+b -1-ygwTx+b (( ( ))( )( ))
=(y-hw,b(x))

 Logistic Regression
â€¢ Therefore
â€¢ And
â€¢ Thus the update rules according to stochastic gradient ascent for wj and b are
w =w +h(y-h (x))x j j w,b j
b=b+h(y-h (x)) w,b
Â¶wÂ¶ ï¬(w,b)=(y-hw,b(x))xj j
  Â¶ Â¶b
ï¬(w,b)=(y-h (x)) w,b
 
 Logistic Regression
â€¢ For 1D case
w =w +h(y-h (wx+b))x=w +hÃ¦y- 1 Ã¶x 1 1 w,b 1 1 Ã§ -(wx+b)Ã·
 b = b +h (y - h (x))= b +h Ã¦ y - w,b Ã§
Ã¨1+e1 Ã¸ 1 Ã¶
â€¢ Applying stochastic gradient ascent with updating the gradient at the ith
data
w =w +hÃ¦y(i) - 1 Ã¶x(i) Ã§Ã·
1 1 Ã§ -wx(i)+b Ã· Ã¨ 1+e ( 1 ) Ã¸
-(w x+b) Ã· Ã¨1+e1 Ã¸
  b=b+hÃ¦y(i) - 1 Ã¶ Ã§Ã·
Ã§ -wx +b Ã· Ã¨ 1+e(1(i) )Ã¸
 
 Logistic Regression
â€¢ Compared to the update rule of the linear regression, it looks identical. However this is not the same since hÎ¸(x(i) ) is a non-linear function of Î¸Tx(i)
â€¢ Now consider a logistic regression problem of multidimensional input features ï²
h (x)=g(b+wx+wx+...+wx) w,b 1122NN
â€¢ The multidimensional logistic regression in pictorial representation
  b
hÎ¸(wTx(i)+b)
Now, this looks like a neural network!

 Logistic Regression
ï²
â€¢ Not all the input feature vector may contribute equally to the target value
â€¢ Forcing the algorithm to fit the data to N-dimensional model may result in overfitting
h (x)=g(b+wx+wx+...+wx) w,b 1122NN
   
 Regularization
â€¢ Regularization: to constrain overfitting of a model â€¢ Ridge Regression (Tikhonov regularization)
n â€¢AregularizationtermaÃ¥w2 subtractedfromthe
likelihood function.
â€¢ Note that the bias term b is not regularized. It can be added but its influence is typically small, so it is not considered in regularization
â€¢ The terms are subtracted during training only. The hyperparameter ğ›¼ controls how much to regularize.
â€¢ For ğ›¼=0 the model becomes logistic regression
 i i=1
J(q)=ï¬(q)-a Ã¥w2
1n
2 i=1
i

 Regularization
â€¢ Thus the regularized log likelihood is m
ï² ï²a2
2
ï¬(w,b)=Ã¥y(i) logh
i=1
â€¢ with the regularization term, update rule for the gradient ascent is
(x(i))+(1-y(i))log(1-h (x(i)))- w,b
w
    â€¢ Turning into Batch Gradient Ascent of J(wTx+b)
â€¢ Usesentiretrainingsettotakean iteration
Ã¥
Ã«i=1 Ã»
w,b
w=w+hy(i)-h (x(i))x(i)-aw ()
j j w,b j j ( (i) (i) )
b=b+h y -h (x ) w,b
Ã©mÃ¹ wj=wj-hÃª (h (x(i))-y(i))x(i)+awÃº
w,b jj Ã©Ã¥ Ã¹
b=b-h m (h (x(i))-y(i)) Ãªw,b Ãº
Ã«i=1 Ã»

Standard Pre-processing
â€¢ Mostregularizedmodelsaresensitivetoinputfeaturescales.Thus,itis important to scale the data (e.g., using a StandardScaler) before performing Ridge Regression.
 â€¢ Rescaleeachfeaturextohavezeromeanandunitvariance 1m1m2
Î¼= Ã¥x(i) ands2= Ã¥Ã©x(i)-Î¼Ã¹ jmjjmÃ«jiÃ»
i=1 i=1
x(i) -Î¼ X(i) = j i
â€¢ Rescalingandzeromean
â€¢ Mayomitpreprocessingifthefeaturesareoncomparablescales.
 jsj x Ã[-5,5], x Ã[-3,3]
â€¢ e.g.
â€¢ However, if x Ã[0,1000], x Ã[0,1]
12
 12
â€¢ Gradientdescentwillbeveryslow

 Introduction to Artificial Neural Networks (ANN)
ECE 610 David Han Drexel University

 Artificial Neural Networks (ANN)
â€¢ Origins: Algorithms that try to mimic the brain.
â€¢ Was very widely used in 80s and early 90s; popularity diminished in
late 90s.
â€¢ Huge Comeback in 2010s: State-of-the-art technique for many applications
  Biological Neuron
Artificial Neuron

 Perceptron
â€¢ The simplest of Artificial Neural Network (ANN) architectures â€¢ Invented in 1957 by Frank Rosenblatt
â€¢ Also called Threshold Logic Unit (TLU)
â€¢ TLU takes weighted inputs and sums them
â€¢ Produces a binary output as a Heaviside step function (threshold function): z=wx+wx +.......+wx +b=xTw+b
 1122nn
 b
("%, O = *) 1 : $# âˆ‘ w i x i '& + b > 0 *-
*i*
+ 0 : otherwise . 0: otherwise

 Perceptron
â€¢ Can be used for a simple linear binary classification
â€¢ Composed of a single layer of TLUs
â€¢ Multiclass classification possible by input
connected to multiple TLUs
â€¢ Each TLU connected to all the inputs
â€¢ â€œFully connected layerâ€ or a â€œdense layerâ€ means when all the neurons in a layer are connected to every neuron in the previous layer.
Class 1
Class 2
Class 3
    
 Perceptron
â€¢ Can be used for a simple linear binary classification
â€¢ Composed of a single layer of TLUs
â€¢ Multiclass classification possible by input
connected to multiple TLUs
â€¢ Each TLU connected to all the inputs
â€¢ â€œFully connected layerâ€ or a â€œdense layerâ€ means when all the neurons in a layer are connected to every neuron in the previous layer.
Class 1
Class 2
Class 3
 
 Perceptron
â€¢ Can be used for a simple linear binary classification
â€¢ Composed of a single layer of TLUs
â€¢ Multiclass classification possible by input
connected to multiple TLUs
â€¢ Each TLU connected to all the inputs
â€¢ â€œFully connected layerâ€ or a â€œdense layerâ€ means when all the neurons in a layer are connected to every neuron in the previous layer.
Class 1
Class 2
Class 3
 
 Perceptron
â€¢ Can be used for a simple linear binary classification
â€¢ Composed of a single layer of TLUs
â€¢ Multiclass classification possible by input
connected to multiple TLUs
â€¢ Each TLU connected to all the inputs
â€¢ â€œFully connected layerâ€ or a â€œdense layerâ€ means when all the neurons in a layer are connected to every neuron in the previous layer.
Class 1
Class 2
Class 3
 
 Perceptron
z=wx+wx +b 11 22
b
â€¢ Recall an equation of line in 2D space from linear algebra.
â€¢ Thus, z=0 forms a decision boundary.
Ã¬0 if z<0
Ã­h! ğ‘§ =0ğ‘–ğ‘“ğ‘§<0
h ğ‘§=1ğ‘–ğ‘“ğ‘§â‰¥0
Ã®
!
1 if z30
       x2
or
    x1
 
  Perceptron
â€¢ Consideralineralyseparable dataset
â€¢ Aproperlydrawnlineof equation would serve as the classification decision boundary.
â€¢ Theclassifierhastoprovidea binary output while taking the input value of z.
â€¢ Thus an activation function h needs to behave as.
â€¢ Heavisidestepfunctionwas chosen
        h(z)=Ã¬0 if wx+b<0 Ã­
1 if wx+b30 Ã®
 heaviside(z)=Ã¬0 if z<0 Ã­
 1 if z30 Ã®
Decision Boundary

 Perceptron
â€¢ Perceptron can also do some logical computations â€¢ Let x1 and x2 be booleans
â€¢ Consider a perceptron with the following weights
x1
4 4
-7 1
x1
01 00
hw,b(x)
 x2
        x1
x2
z
h(z)
 0
0
-7
0
 0
1
-3
0
 1
0
-3
0
 1
1
1
1
            This is an â€œANDâ€ operation
x2

 Perceptron
â€¢ More logical computations by perceptron
x1
4 4
-3 1
x1
11 01
  x2
hw,b(x)
      x1
x2
z
h(z)
0
0
-3
0
0
1
1
1
1
0
1
1
1
1
5
1
           An â€œORâ€ operation
x2

 Perceptron
â€¢ More logical computations by perceptron
x1
-4 -4
2 1
x1
00 10
 x2
hw,b(x)
       x1
x2
z
h(z)
0
0
2
1
0
1
-2
0
1
0
-2
0
1
1
-6
0
           A â€œNORâ€ operation
x2

 Perceptron Training
â€¢ But,howdowefindtherightwandb? Howdowetrain perceptron?
â€¢ BasedonHebbianlearning
â€¢ Originated from Hebbâ€™s rule in The Organization of Behavior:
w(nextstep) = w(current) +h(y - yË†)x iii
â€¢ Thelearningrulereinforcesconnectionsbetweenconnected neurons that help reduce the error.
x1
w
 x2
b
1
w
yË† (x) w,b
1
2
 Logistic Regression Update Rule
 wj =wj +h(y-hw,b(x))xj bj =bj +h(y-hw,b (x))
where
w is the connection weight between the ith input neuron and the output neuron.
i
xi is the ith input value of the current training instance.
yË† is the output of the output neuron for the current training instance.
y is the target output of the output neuron for the current training instance. h is the learning rate.

 Perceptron
â€¢ The decision boundary is linear, so perceptron canâ€™t be applied to complex problems
â€¢ If the training instances are linearly separable, Rosenblatt demonstrated that this algorithm would converge to a solution. This is called the Perceptron convergence theorem.
â€¢ Though it looks similar, perceptron is different compared to logistic regression and least square linear regression.
â€¢ It is hard to relate its predictions with meaningful probabilistic interpretations
â€¢ Canâ€™t show it as a maximum likelihood estimation algorithm.

Perceptron
â€¢ Death of perceptron: â€œPerceptronâ€ by Marvin Minsky and Seymour Papert
â€¢ Highlighted major weaknesses â€¢ Incapable of XOR (Exclusive OR)
â€¢ Minsky and Papertâ€™s work froze any further advancement of neural network for decades!
    Images from: https://aiws.net/the-history-of-ai/aiws-house/this-week-in-the-history-of-ai-at-aiws-net-marvin-minsky-and-seymour-papert-published-an-expanded-edition-of-perceptrons/

 Perceptron
â€¢ It turned out a simple addition to perceptron wouldsolveXOR problem.
â€¢ Letâ€™s combine three of the perceptrons used for logical computations previously
x14 x1
4 4
x1 -4 x2 -4
2
   x2 4 1 -7
x2
   1 -3 1
 x1 -4 a1
x2 -4 4     a2 4 hw,b(x)
2 -7 4
1 -3
1
 4
     
 Perceptron
â€¢ XORcanbedonebya stacked 2 layer perceptron!
 x1 -4 a1
x2 -4 4     a2 4 hw,b(x)
2 -7 4
1 -3
1
 4
     x2
01
10 x1
         x1
x2
a1
a2
hw,b(x)
 0
0
1
0
1
 0
1
0
0
0
 1
0
0
0
0
 1
1
0
1
1
          
  Multi-Layer Perceptron
â€¢ Thus, by adding more layers, it is possible to draw more complex decision boundaries.
â€¢ Addingahiddenlayer
â€¢ Multi-Layer Perceptron (MLP)
   From: https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781788295758/6/ch06lvl1sec59/introduction-to-deep-learning
 hidden layer

 Perceptron Computation
  x1
w (1) 1,2
x2
w (1) 1,3
x3
+1
Layer 1
A two step process
w (1) 1,1
   (2) z1
(2) 1
Layer 2
a
z(2) =w(1)x +w(1)x +w(1)x +b(1)
1 111 122 133
1
 a(2) = f (z(2) ) 11
Where f is an activation function

 Neural Network Notations
â€¢ Multilayer Perceptrons
x (2) ..... z(D)
     1z1 1 ..... z(D)
         x2 2 x3 .....
z(3) 1
           +1
 +1 ..... +1
Layer 1 Layer 2 Layer D-2 Layer D-1 Layer D
+1
+1

 Neural Network Notations
 The term w(l) denotes the weight applied to an i, j
activation a(l) and added to z(l+1). ji
    z(d ) a(d ) 11
w(d ) 1,1
w(d ) 1,2
z(d +1) a(d +1) 11
Layer d+1
  a(l) denotes activation of unit i in layer l i
for l = 1, we use a(1) = x ii
f (..) is the activation function such as a sigmoid
  z(d ) a(d ) 22
  z(d+1) =w(d)a(d) +w(d)a(d) +w(d)a(d) +b(d)
1 111 122 133
1
  Layer d
a(d+1) =f(z(d+1)) 11

 Neural Network Details
  w(1) z(2) a(2) 1,1 11
w (1) 2,1
z(2) a(2) x2 22
ForLayers1and2
     x1
  z(2) =w(1)x +w(1)x +w(1)x +b(1) 2 211 222 233 2
a(2) = f(z(2)) 22
z(2) =w(1)x +w(1)x +w(1)x +b(1) 3 311 322 333 3
a(2) = f(z(2)) 33
     w (1) 1,3
  x3
z(2) a(2) 33
  b3(1) +1
Layer 1
Layer 2
 
 Neural Network Details
   w(1) z(2) a(2) 1,1 11
w (1) 2,1
z(2) a(2) x2 22
 z(3) =w(2)a(2) +w(2)a(2) +w(2)a(2) +b(2) ) 1 111 122 133 1
    x1
            w(1) 1,3
z(3) 1
Layer 3
b(2)
  x3
z(2) a(2) 33
  b3(1) +1
Layer 1
+1
  Layer 2
h =a(3)=f(z(3)) W,b1 1
 h (x) W,b

 Multi-Layer Perceptron Training
â€¢ Deeper layers seem to allow perceptrons to deal with more complex problems, but how do you train the deep network?
â€¢ Recall the perceptron learning rule?
w(nextstep) = w(current) +h(y - y)x
 iiË†i
â€¢ What do you do for hidden layers? No y values are available for training on hidden
layers!
â€¢ In 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams introduced the backpropagation training algorithm based on finding weights to minimize the cost function using Gradient Descent.
â€¢ This requires using a different activation function than the step function, since its gradient is mostly zero (flat).
q(nextstep) =q-hÃ‘qJ(q) where h is the learning rate

 Activation Functions
â€¢ Stepfunctionhaszeroderivativeeverywhereexceptatz=0
â€¢ Thiswouldmakeitdifficulttoperformgradientdescentinback
propagation
â€¢ Rumelhart,et.alchangedtheactivationfunctionfromstepfunctionto
 sigmoid function (logistic function) â€¢ Recall
â€¢ SigmoidFunctionProperties:
â€¢ Itsderivativeexistseverywhere!
 as zÂ®Â¥,s(z)Â®1 as z Â® -Â¥, s (z) Â® 0
s (0) = 1 2
  g'(z)= d 1 = 1 (e-z) dz 1+e-z 2
  =
g '(Â¥) = 0
1+e
( -z)
1Ã¦1Ã¶
Ã§1- Ã· = g(z)(1- g(z))
( -z)Ã§ ( -z)Ã· 1+e Ã¨ 1+e Ã¸
  g '(-Â¥) = 0
g '(0) = 1 4
 
 Activation Functions
â€¢ Hyperbolic tangent function (Tanh) â€¢ Continuous,differentiable
â€¢ Range -1 to +1
Tanh
g(z) = tanh(z) = ez -e-z ez +e-z
â€¢ HyperbolictangentProperties:
â€¢ Thederivativeexistseverywhere
 as z Â® Â¥, g(z) Â®1
as z Â® -Â¥, g(z) Â® -1
g(0) = 0
  g'(z)= d Ã¦ez -e-z Ã¶=1-(tanh(z))2 dzÃ§ez +e-z Ã·
 Ã¨Ã¸ =1-Ã¦ez -e-z Ã¶2
Ã§ez +e-z Ã· Ã¨Ã¸
 g '(Â¥) = 0 g '(-Â¥) = 0 g '(0) = 1
 
 Activation Functions
â€¢ Rectified Linear Unit (ReLU)
â€¢ Continuous,differentiableexceptatz=0
â€¢ Fasttocompute
â€¢ Derivative constant at z >> 0Ã¨alleviates vanishinggradientproblemindeepnetwork
a(z) = max(0, z) â€¢ ReLUProperties:
â€¢ Itsgradientexistseverywhere except at z=0
for z Â£ 0, a(z) = z for z < 0, a(z) = 0
forz>0, a'(z)=z forz<0, a'(z)=0
   
 Activation Functions
â€¢ Activation functions compared: all nonlinear
 â€¢ Why do we need nonlinear functions?
â€¢ Multilayered resulted nonlinear decision boundaries
â€¢ With linear activation, multilayer can be collapsed into a single layer

 Why Nonlinear Activation Functions?
â€¢ Consider linear activation function z(2) =W(1)x+b(1)
x1
x2 a(3)
     x1
x2 z(2)
x3 z(2) 3
+1
a(2) =g(2)(z(2))
Forlinearactivaton g(2)(z(2))=z(2)
z(2) 1
     2
z(3) =W(2)a(2) +b(2) a(3) =g(3)(z(3))=z(3)
Equivalent x 3
+1
A deep network with linear activation is equivalent to a single layer network!
 z(3) 1
    Sincea(2) =z(2) =W(1)x+b(1)
z(3) =W(2)a(2) +b(2) =W(2) W(1)x+b(1) +b(2)
a(3) = z(3) =W(2) W(1)x+b(1) +b(2) ()
=W W x+W b +b =Wx+b
(2) (1) (2) (1) (2)
()
  +1

   Intuition about deep representation
Intuition about deep representation
       Low Level Features
                                       ô°€ Image: Pixel Edge
Texture Motif Part Word-group Clause Sentence
Object Story
ô°€ Text: Character
Word
Mid Level Features
High Level Features
Output (e.g. outdoor, indoor)
 Trainable Classifier
    Intelligent Information & Signal Processing Lab.
School of EE, Korea University
46
 
   Intuition about deep representation
Intuition about deep representation
       Low Level Features
                                       ô°€ Image: Pixel Edge
Texture Motif Part Word-group Clause Sentence
Object Story
ô°€ Text: Character
Word
Mid Level Features
High Level Features
Output (e.g. outdoor, indoor)
 Trainable Classifier
    Intelligent Information & Signal Processing Lab.
School of EE, Korea University
46
 
 Multi-Layer Perceptron
â€¢ Deeper and deeper layers for more complex problems and for higher performance
   Isitacar or not?
 Binary Classification

 Multi-Layer Perceptron
â€¢ Multiclass Classification by MLP by adding another output node at the last layer
â€¢ Binary Classification by MLP
x (2) z(2) x1 z(2)
        1z1 1 1
   Car Car z(2) =2 z(2) z(3)
       x2z(2)+2 x21 2
   z(3)
1 (2) (#) x3 z(2) (#)
      x3 z(2) 3
+1
z3 ğ‘§! 3 ğ‘§!
   +1 +1 +1
Motorcycle
  +1

 Multi-Layer Perceptron
â€¢ Multiclass Classification by MLP
  Each of these are binary classificaiton
   h1(wTa(i)+b1) h2(wTa (i)+ b2)
h3(wTa (i)+ b3) h4(wTa (i)+ b4)
Car
  Pedestrain
  Truck
  Motorcycle
 
 Multiclass Classification by MLP
â€¢ For a multiclass classification, the output is a vector with the dimension matching to the number of the classification categories
       Ã©0.86Ã¹ Ã©0.06Ã¹ Ã©0.35Ã¹ Ã©0.15Ã¹ Ãª0.34Ãº Ãª0.79Ãº Ãª0.29Ãº Ãª0.36Ãº
 ÃªÃºÃªÃºÃªÃºÃªÃº Ãª0.56Ãº Ãª0.14Ãº Ãª0.92Ãº Ãª0.28Ãº
  Ãª0.02Ãº Ãª0.57Ãº Ãª0.45Ãº Ãª0.65Ãº Ã«Ã»Ã«Ã»Ã«Ã»Ã«Ã»
    
 Single layer MLP and NaÃ¯ve Bayes
â€¢ Consider a single layer for multiclass classification
x1 a1 x2 a2 x3 a3 x4 a4
+1
z=wx+wx+wx+wx+b 11223344
           
NaÃ¯ve Bayes Classifier
â€¢ Consider a naÃ¯ve bayes classifier for classifying cars with 4- dimensional input features (x1, x2, x3, x4)
  P(car x , x , x , x ) = p(x 1234
1
car) p(x2
car) p(x3
car) p(x4
car) P(car)
      p(x , x , x , x ) 1234
 x1 x2 x3
x4 +1
Assuming equal priors, removing evidence term, and taking log likelihood
log (P (car x , x , x , x )) 1234
=constant +logP(x car)+logP(x car)+logP(x car)+logP(x car)+logP(car) 1234
= constant +log P (x car )+ log P (x   car )+ log P (x   car )+ log P (x   car ) 1234
              
 Single layer Neural Net and NaÃ¯ve Bayes
    x1 a1 x2 a2
x3 a3 x4 a4
+1
â€¢ A single layer multiclass perceptron
z=wx+wx+wx+wx+b 11223344
h1(wTx(i)+b)
x1 a1 x2 a2
x3 a3 x4 a4
+1
â€¢ A multiclass naÃ¯ve Bayes classifier
      P(car x , x , x , x ) = p(x 1234
1
car) p(x2
car) p(x3
car) p(x4
car) P(car)
      p(x , x , x , x ) 1234
   Assuming equal priors, removing evidence term, and taking log likelihood
log (P (car x , x , x , x )) 1234
=constant +logP(x car)+logP(x car)+logP(x car)+logP(x car)+logP(car) 1234
= constant +log P (x car )+ log P (x   car )+ log P (x   car )+ log P (x   car ) 1234
      
 Multiclass Classification by MLP
                 Ideal
  â€¢ Can we turn the output to probabilities?
Ãª0.02Ãº Ãª0.57Ãº Ãª0.45Ãº Ãª0.65Ãº Ã«Ã»Ã«Ã»Ã«Ã»Ã«Ã»
Ã©0.86Ã¹ Ã©0.06Ã¹ Ã©0.35Ã¹ Ã©0.15Ã¹ Ãª0.34Ãº Ãª0.79Ãº Ãª0.29Ãº Ãª0.36Ãº
ÃªÃºÃªÃºÃªÃºÃªÃº Ãª0.56Ãº Ãª0.14Ãº Ãª0.92Ãº Ãª0.28Ãº
Actual

 Softmax Regression
â€¢ Generalizing logistic regression for multiple classes: Multinomial Logistic Regression
â€¢ For each class k, the softmax regression model computes a score zk(d)(x) for each class k.
         â€¢ Thus, for k classes, you need k classifiers: zk(d)
â€¢ Softmax score for class k:
â€¢ Softmax function then estimates the probability of each class from the scores.
z(d) =w(d-1)a(d-1) +w(d-1)a(d-1) +w(d-1)a(d-1) +b(d-1) k k,1 1 k,2 2 k,3 3 k
Softmax score computed from the last layer prior to the output

 Softmax Regression
 â€¢ Score for class k is computed by a softmax function as
exp(z(d)(x)) k
p =s(z(x)) =
Ë†k
K
Ã¥exp(z(d ) (x))
Exp function ensures that all the values are positive to conform to probability
 k
j=1
j
 where K is the number of classes
z(x) is a vector containing the scores of each class for the instance x s (z(x))k is the estimated probability that x belongs to class k given
the scores of each class for that instance

 Softmax Regression
Ã©0.86Ã¹ Ã©0.06Ã¹ Ã©0.35Ã¹ Ã©0.15Ã¹ Ãª0.34Ãº Ãª0.79Ãº Ãª0.29Ãº Ãª0.36Ãº
â€¢ Consider an example output as follows:
â€¢ Thus, : z1(d)(x)=0.86, z2(d)(x)=0.34, z3(d)(x)=0.56 , z4(d)(x)=0.02
pË†1 =
exp(0.86)
exp(0.86) + exp(0.34) + exp(0.56) + exp(0.02)
= 2.36
2.36 + 1.40 + 1.75 + 1.02
= 2.36 = 0.36 6.53
( ( )) exp(z(d)(x)) pË†k=szx= k
kK
Ã¥exp(z(d) (x))
j j=1
ÃªÃºÃªÃºÃªÃºÃªÃº Ãª0.56Ãº Ãª0.14Ãº Ãª0.92Ãº Ãª0.28Ãº
Ãª0.02Ãº Ãª0.57Ãº Ãª0.45Ãº Ãª0.65Ãº Ã«Ã»Ã«Ã»Ã«Ã»Ã«Ã»
    pË†2 =0.21 pË†3 =0.27 pË†4 =0.16
ğ‘Ì‚!+ ğ‘Ì‚" + ğ‘Ì‚# + ğ‘Ì‚$ =1
â€¢ Thus,
Softmax scores are turned into probabilities

 Softmax Regression
â€¢ Prediction by Softmax
y=argmaxs z x =argmaxz x =argmax q(k) x
(T)
â€¢ Note that the prediction returns the value of k that maximizes the
(()) () ()
Ë†kk kkk
estimated probability s (s(x)) k
â€¢ Softmax classifier predicts only one class at a time: use only for mutually exclusive classes

 Softmax Regression
â€¢ How do you compute a cost function for softmax classifier?
â€¢ Use Cross entropy loss : measures how well the predicted class probabilities
match with the target classes
â€¢ Cross entropy between two probability distributions p and q:
H(p,q)=-Ã¥x p(x)logq(x)
â€¢ Since we are computing a cost function, minimizing it is the objective.
â€¢ Consider a single example case of binary cross entropy loss â€¢ Note that H is always > 0 since log[q(x)]<0 as q(x)<1
â€¢ Forp(x)=1andq(x)=1,H(p,q)=0
â€¢ Forp(x)=0andq(x)=0,H(p,q)=0
â€¢ Forp(x)=1andq(x)=0,H(p,q) Â®Â¥ as log(0)Â®-Â¥
â€¢ Thus only when p(x)= q(x) , cross entropy loss is minimized

 Softmax Regression
â€¢ Cross Entropy Loss for multiclass classifier 1mK
 y1
y2
y3
p1
p2
p3
1
0
0
0.76
0.22
0.02
0
0
1
0.02
0.11
0.87
1
0
0
0.95
0.01
0.04
0
1
0
0.02
0.53
0.45
  J(Q)=- Ã¥Ã¥y(i)log(pË†(i))
  mi=1 k=1
where y(i) is the target probability that
kk
    k
the ith instance belongs to class k.
 â€¢ For binary classification, the above equation degenerates to the logistic regression cost function shown earlier. Thus K=2.
â€¢ Consider loss for one instance: m=1
1mK K
J(Q) = - Ã¥Ã¥y(i) log(pË†(i) )= -Ã¥y(i) log(pË†(i) )
kk
mi=1 k=1 k=1
kk

 Softmax Regression
 J(Q)=-y log(pË† )-y log(pË† ) 1122
but in binary, pË† = 1- pË† and y =1- y 2121
let y = y and pË† = pË† 11
then J(Q) = -ylog pË† -(1- y)log(1- pË†)
â€¢ Recall maximum log likelihood of logistic regression
logL(w,b)=
Ã¥ i=1
y(i) logh w,b
w,b
m
(x(i))+(1-y(i))log 1-h (x(i)) ï²(ï²)
â€¢ For a single instance with one data point: m = 1
â€¢ ğ‘™ ğ‘¤,ğ‘ =ğ‘¦ " ğ‘™ğ‘œğ‘”h#,%(ğ‘¥âƒ— " )+(1âˆ’ğ‘¦ " )ğ‘™ğ‘œğ‘”(1âˆ’h#,%(ğ‘¥âƒ— " ))
 
 Softmax Regression
â€¢ Continue calculating the gradient if sigmoid
â€¢ Similarly for b
â€¢ For multiclass, the gradient vector for the cost function becomes
Â¶wÂ¶ ï¬(w,b)=(y-hw,b(x))xj j
Â¶ ï¬(w,b)=(y-hw,b(x)) Â¶b
 Ã‘ J(Q)= pË†(i) -y(i) x(i) ()
q(k)kk

 Back Propagation
What caused?
 
 Learning Algorithm: Backpropagation
Pictures below illustrate how signal is propagating through the network, Symbols w(xm)n represent weights of connections between network input xm and neuron n in input layer. Symbols yn represents output signal of neuron n.
 
 Learning Algorithm: Backpropagation
 
 Learning Algorithm: Backpropagation
 
 Learning Algorithm: Backpropagation
Propagation of signals through the hidden layer. Symbols wmn represent weights of connections between output of neuron m and input of neuron n in the next layer.
 
 Learning Algorithm: Backpropagation
 
 Learning Algorithm: Backpropagation
 
 Learning Algorithm:
Backpropagation
Propagation of signals through the output layer.
 
 Learning Algorithm: Backpropagation
In the next algorithm step the output signal of the network y is compared with the desired output value (the target), which is found in training data set. The difference is called error signal d of output layer neuron
 
 Learning Algorithm: Backpropagation
The idea is to propagate error signal d (computed in single teaching step) back to all neurons, which output signals were input for discussed neuron.
 
 Learning Algorithm: Backpropagation
The idea is to propagate error signal d (computed in single teaching step) back to all neurons, which output signals were input for discussed neuron.
 
 Learning Algorithm: Backpropagation
 
 Learning Algorithm: Backpropagation
When the error signal for each neuron is computed, the weights coefficients of each neuron input node may be modified.
 
Multi-Layer Perceptron and Back-propagation
 w(1)
1,1 z(2)
â€¢
Consideraneuralnetworkwithtwo outputs
   x
1 w(1) 1
(2) a2(2)
a3(2) b1(2)
a
1
  x 2
w(1) 1,3
x3
+
1
Layer L1
z(2) 2
z(2) 3
â€¢
Cost function J(wi,j, bi ) used to optimize weightswi,jandbi bytakingpartial derivatives
2,1
    Z1(3) a1(3) y1
       b3(1)
Layer L2
Z2(3) a2(3) y2 L a y e r L 3
w(l) :=w(l) -a Â¶ J(w,b)
  i, j i, j
Â¶w(l) i, j
bl :=bl -a Â¶ J(w,b)
 ii
Â¶b(l) i
  +1
ğ‘§" =ğœ”$ğ‘$ +ğœ”$ğ‘$ +ğœ”$ğ‘$ +ğ‘$ ! !,! ! !,$ $ !," " !
 ()( )
a(3) =h = f z(3) = f w(2)a(2) +w(2)a(2) +w(2)a(2) +b(2)
1 1 1 1,11 1,22 1,33 1
 J(w,b;x,y)=1h (x)-y2
    2
w,b

Multi-Layer Perceptron and Back-propagation
â€¢ The goal is to minimize J(w,b) as a function of w and b.
â€¢ J(w,b) is not convex, thus the gradient descent process might get
stuck on local minima. Usually, however, gradient descent works.
â€¢ Initialization of w and b important as in any optimization problem.
â€¢ Initialize the parameters randomly, non-zeros.
â€¢ Updating w and b by gradient descent
â€¢ Important to be able to calculate gradient of activation function
 w(l) :=w(l) -a Â¶ J(w,b)
  i, j i, j
Â¶w(l) i, j
b(l) :=b(l) -a Â¶ J(w,b)
 ii
Â¶b(l) i
where a is the learning rate
 
 Notational Convention in Back-propagation
 z(l) denotes the total weighted sum of inputs i
to unit i in layer l including the bias term Thus, z(2) =Ã¥n w(1)x +b(1)
i
j=1
ij j i
so that a(l) = f (z(l) ) ii
  Recall that a(1) = x which denotes the values from the input layer. Without the subcript index,
it is implied both a(1)and x are vectors. Similarly, we can further write compactly as
z(l+1) =w(l)a(l) +b(l) a(l+1) = f(z(l+1))
 z(2) =w(1)x+b(1) a(2) = f(z(2))
z(3) =w(2)a(2) +b(2)
h (x)=a(3) = f(z(3)) W,b
Courtesy of A. Ngâ€™s CS294A Lecture Notes

Partial Derivatives and Chain Rule
 a(2)
a(3)
 L e t q ( l ) = w ( l ) +, b ( l ) ( & ) ğ¿ğ‘’ğ‘¡ ğœƒ(&) = ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥ ğ‘œğ‘“ğ‘¤
( & ) (
i, j i
a(4) =h (x)= f(z(4))= f q(3)a(3)
w,b
= f q(3)g z(3) = f q(3)g(q(2)a(2))
(())( )
(,)
ğ‘ğ‘›ğ‘‘ ğ‘
= f q(3)g q(2) p z(2) = f q(3)g q(2) p q(1)a(1)
( ( ( ))) ( ( ( )))
=f q(3)gq(2)pq(1)x
( ( ( )))
()
   a(1)
a(4)
From Cambridge Mathematics
 Layer 1
Layer 2
Layer 4 Layer 3
 J(w,b;x,y)=1h (x)-y2
     w,b
â€¢ Takingpartialderivativesofacomplexfunction
â€¢ Usingthechainrulefromcalculus
d Ã©f(g(x))Ã¹=fÂ¢(g(x))gÂ¢(x) dx Ã« Ã»
2
Taken from: Why are Russian Dolls so Full of Themselves? | by Sofia Boulamrach | Medium
  â€¢ Using the chain rule is fine and good, but it can be very messy and time consuming.
â€¢ Letâ€™s consider a scheme based on the chain rule but more approachable in getting the partial derivatives
â€¢ Back Propagation

 Multi-Layer Perceptron and Back-propagation
â€¢ For a fixed set of training data { (1) (1) (2) (2) (m) (m) } (x ,y ),(x ,y ),......,(x ,y )
â€¢ Define a loss function for a single sample â€¢ For m examples
 J(w,b;x,y)=1h (x)-y2
    2
w,b
 Ã©1 m Ã¹ J (w , b ) = Ãª 2 Ã¥ J (w , b; x ( i ) , y ( i ) ) Ãº
Ã« i=1 Ã» Ã©1 m Ã¦1 Ã¶Ã¹
=Ãªm 2 h (x(i))-y(i) Ãº Ã¥Ã§ w,b 2Ã·
Ã« i=1Ã¨         Ã¸Ã»
â€¢ Let sigmoid be used for activation
â€¢ For classification, y=0 or 1 data labels.

Multi-Layer Perceptron and Back-propagation
1. Forward pass: Starting from the input layer, the algorithm computes the output of all the neurons in each layer for every instance in the batch until the output of the last layer (prediction value) is computed.
2. All intermediate results are preserved for the backward pass.
3. The networkâ€™s output error (loss function) computed.
4. Chain rule (calculus) applied to analytically compute how much each output connection contributed to the error.
5. The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain ruleâ€”and so on until the algorithm reaches the input layer.
6. The reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).
7. Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.
 
Multi-Layer Perceptron and Back-propagation
1. Forward pass: Starting from the input layer, the algorithm computes the output of all the neurons in each layer for every instance in the batch until the output of the last layer (prediction value) is computed.
2. All intermediate results are preserved for the backward pass.
3. The networkâ€™s output error (loss function) computed.
4. Chain rule (calculus) applied to analytically compute how much each output connection contributed to the error.
5. The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain ruleâ€”and so on until the algorithm reaches the input layer.
6. The reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).
7. Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.
 
Multi-Layer Perceptron and Back-propagation
1. Forward pass: Starting from the input layer, the algorithm computes the output of all the neurons in each layer for every instance in the batch until the output of the last layer (prediction value) is computed.
2. All intermediate results are preserved for the backward pass.
3. The networkâ€™s output error (loss function) computed.
4. Chain rule (calculus) applied to analytically compute how much each output connection contributed to the error.
5. The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain ruleâ€”and so on until the algorithm reaches the input layer.
6. The reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).
7. Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.
 
Multi-Layer Perceptron and Back-propagation
1. Forward pass: Starting from the input layer, the algorithm computes the output of all the neurons in each layer for every instance in the batch until the output of the last layer (prediction value) is computed.
2. All intermediate results are preserved for the backward pass.
3. The networkâ€™s output error (loss function) computed.
4. Chain rule (calculus) applied to analytically compute how much each output connection contributed to the error.
5. The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain ruleâ€”and so on until the algorithm reaches the input layer.
6. The reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).
7. Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.
 
Multi-Layer Perceptron and Back-propagation
1. Forward pass: Starting from the input layer, the algorithm computes the output of all the neurons in each layer for every instance in the batch until the output of the last layer (prediction value) is computed.
2. All intermediate results are preserved for the backward pass.
3. The networkâ€™s output error (loss function) computed.
4. Chain rule (calculus) applied to analytically compute how much each output connection contributed to the error.
5. The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain ruleâ€”and so on until the algorithm reaches the input layer.
6. The reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).
7. Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.
 
Multi-Layer Perceptron and Back-propagation
1. Forward pass: Starting from the input layer, the algorithm computes the output of all the neurons in each layer for every instance in the batch until the output of the last layer (prediction value) is computed.
2. All intermediate results are preserved for the backward pass.
3. The networkâ€™s output error (loss function) computed.
4. Chain rule (calculus) applied to analytically compute how much each output connection contributed to the error.
5. The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain ruleâ€”and so on until the algorithm reaches the input layer.
6. The reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).
7. Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.
 
 Multi-Layer Perceptron and Back-propagation
â€¢ Letâ€™s compute the forward pass of a network with one hidden layer.
z(2) =w(1)x +w(1)x +w(1)x +b(1) 1 11 1 12 2 13 3 1
Ã©a(1)Ã¹ ï² ï² Ãª 1 Ãº
 w(1) 1,3
w(2) 1,3
Ã©z(2)Ã¹ Ã©xÃ¹ ï² Ãª1Ãºï²1
w(1) Ã¹ w(1)
(1) z(2)a(2) w(2) w 11 1,1
z(2) =w(1)x +w(1)x +w(1)x +b(1)
x=a(1)=Ãªa(1)Ãº
   1,1
x (3) 2
   1
x2
x 3
+1 b3(1) Layer L1
21 1 22 2 23 3 2
w1 33113223333 Ã«3Ã»
z(2) a (2) 2
2
Z (3) a1 z(2) =w(1)x +w(1)x +w(1)x +b(1) Ãªa(1)Ãº
(2) ï²
ï²ï²
 1,2 2
z(2) =w(1)x =w(1)a(1) ï€¥ï€¥
 z(2)=z(2) x=ÃªxÃº Ãª2ÃºÃª2Ãº
w(1) = w(1) w(1)
Ã©w(1) w(1)
Ãª11 12 13Ãº
  z(2)a (2)
33 (3)
ï€¥
Ãª21 22 23Ãº Ãªw(1) w(1) w(1) Ãº Ã«31 32 33Ã»
  b1(2)
Z2(3) a2 Layer L3
Ãªz(2)Ãº ÃªxÃº Ã«3Ã»Ã«3Ã»
  +1
Layer L
Ã©z(2) Ã¹ Ã©w(1) w(1) Ãª1ÃºÃª111213Ãº1Ãª1Ãº
2
Ã«3 Ã» Ã«31 32 33Ã»Ã«3Ã» Ã«3 Ã»
w(1) Ã¹Ã©x Ã¹ Ã©b(1) Ã¹ Ãªz(2)Ãº =Ãªw(1) w(1) w(1)ÃºÃªx Ãº+Ãªb(1)Ãº
Ãª2ÃºÃª21 22 23ÃºÃª2ÃºÃª2Ãº z(2) w(1) w(1) w(1) Ãªx Ãº b(1)

 Multi-Layer Perceptron and Back-propagation
â€¢ Continue the forward pass
Ã©Ã¹ Ã©a(2)Ã¹ s(z1 )
w
(1)
1
z(2) a2(2) 2
1,1
()
(2)
 z(2)a w(2)
a(2) =Ãªa(2)Ãº= s(z(2)) Ãª2ÃºÃª2Ãº
1(2)
1,1 Ã«3Ã»
ï²Ãª1ÃºÃªÃº ÃªÃº
a(2) (2)
Ã« 3 Ã» Ãªs z Ãº
      x1 x2
x3 +1
w(2) 1,2
Z1(3) a1(3)
Continue to the final layer
z(3) =w(2)a(2) +w(2)a(2) +w(2)a(2) +b(2) 1 111 122 133 1
z(3) =w(2)a(2) +w(2)a(2) +w(2)a(2) +b(2)
  w(1) (2) 2 21 1 1,3 w ï²ï²
22 2
23 3 2
  b
b1(2)
(1) 3
ï² Ã©z(3) Ã¹ z(3)
z(2)a (2) 33
1,3 z(3) =w(2)a(2) ï€¥
  Z (3) a2(3) 2
Layer L3
Ã©w(2) w(2)
(3) 1 (2) 11 12 13
w(2) Ã¹ wherez =Ãª Ãº wï€¥ =Ãª Ãº
 Ã« 2 Ã»
w(2) w(2) w(2)
 Layer L1
Ãªz(3) Ãº
w(2)
22 23Ã»Ãªa(2)Ãº Ã«2 Ã»
+1
Layer L2
Ã« 21 22 Ã©a(2) Ã¹
23 Ã»
Ã©z(3) Ã¹ 1
Ã©w(2) = 11
w(2) 12
w(2) Ã¹Ãª 1 Ãº Ã©b(2) Ã¹ 13 a(2) + 1
Ãªw(2) Ã«2 Ã» Ã«21
w(2) Ãº Ãª 2 Ãº Ãªb(2) Ãº Ã«3Ã»

 Multi-Layer Perceptron and Back-propagation
â€¢ Loss function J w(1)
z(2) a1(2) 1
w(2) 1,1
    1,1
    x1 x
x3 +1
Z1(3) a1(3)
2 1Ã©22Ã¹
w(2) 1,2
w(2) 1,3
z(2)a (2) 3 3
b3(1) b1(2) +1
Layer L2
Ã¥ i=1
y1
1C 2
 (2) a (2) z2 2
J =
(a(3) - y ) 2 i i
 w(1) 1,3
=a-y+a-y 2Ã«1122Ã»
Ãª((3) ) ((3) )Ãº
     (3) y Z2(3) a2 2
Layer L3
  Layer L1

 Multi-Layer Perceptron and Back-propagation
â€¢ Loss Function
1Ã© (3) 2 (3) 2Ã¹
J=Ãªsz-y+sz-yÃº (())(())
= +
=
2Ã«1122Ã»
s w(2)a(2) +w(2)a(2) +w(2)a(2) +b(2) 1Ã© (
) Ã¹2 - y
2Ã«111 122 133 1 1Ã»
s w(2)a(2) +w(2)a(2) +w(2)a(2) +b(2) - y 1Ã©( ) Ã¹2 2Ã«211 222 233 2 2Ã»
s w s z +w s z +w s z +b -y 1Ã©(() () ())Ã¹2 2 11 1 12 2 13 3 1 1
Ã« (2) (2) (2) (2) (2) (2) (2) Ã»
+ swsz +wsz +wsz +b -y 1Ã©(() () ())Ã¹2 2 21 1 22 2 23 3 2 2
Ã« (2) (2) (2) (2) (2) (2) (2) Ã»

 Multi-Layer Perceptron and Back-propagation
ğ‘§!($)
1Ã©
2Ã«11 1111221331
    (()
J= sw(2)sw(1)x+w(1)x+w(1)x+b(1)
+w s w x +w x +w x +b ()
( 2 ) (1) (1) (1) (1)
12 211 222 233 2
 +
(()
(2) (1) (1) (1) (1) (2)
( ))]2
+wswx+wx+wx+b +b -y
13 311 322 333 3 1
1
1Ã©
2Ã«21 1111221331
s w(2)s w(1)x +w(1)x +w(1)x +b(1)
+w s w x +w x +w x +b ()
( 2 ) (1) (1) (1) (1)
From Cambridge Mathematics
 22 211 222 233 2
(2) (1) (1) (1) (1) (2)
( ))]2
+wswx+wx+wx+b +b -y 23 311 322 333 3 2 2

 Multi-Layer Perceptron and Back-propagation
â€¢ Partial derivative
Â¶J Â¶w(2)
of loss function J 12
1C Ã¥
 2 (a(3) - y )
J =
1Ã© 2 2Ã¹
 w(1) 1,1
x 1
x2
x 3
+1
Layer L1
z(2) a1(2) 1
z(2) a2(2) 2
w(2) 1,3
z(2)a (2) 3 3
b1(2)
w(2) 1,1
2 i=1
Z a1(3) y = Ãª(a(3)-y)+(a(3)-y)Ãº
ii
       1(3)1 2Ã«1122Ã»
   w(1) 1,3
= Ã­ Ãª(a(3)-y)+(a(3)-y)ÃºÃ½ Â¶w(2) Ã®2Ã« 1 1 2 2 Ã»Ã¾
b3(1)
(3) y Z2(3) a2
2
w(2) 1,2
Â¶J Â¶Ã¬1Ã© 2 2Ã¹Ã¼
  Â¶w(2)
12 12
      LayerL
3
Â¶w(2)Ã®2Ã«2 2 Ã»Ã¾ 12
But
 +1
Layer L2
Â¶ Ã¬1Ã© 2Ã¹Ã¼
Ã­ Ãª(a(3) -y )ÃºÃ½=0
 
 Multi-Layer Perceptron and Back-propagation
   w(1) 1,1
(2)
12
Za1(3) y =(a(3)-y)Â¶a(3) Â¶z(3)
z(2) a1 1
w(2)
Â¶w(2) 1,1 12
= Ã­ Ãª(a(3)-y)ÃºÃ½ Â¶w(2) Ã®2Ã« 1 1 Ã»Ã¾
Remaining term
Â¶J Â¶Ã¬1Ã© 2Ã¹Ã¼
       x 1
x2
x3 +1
z(2) a2(2) 2
1(3) 1 1111 Â¶z(3) Â¶w(2)
1 12 Consider the second term
     Â¶a(3) Â¶s (z(3) ) w(1) w(2) 1=1
1,3 1,3 z(2)a (2)
Â¶z(3) Â¶z(3) 11
Recallderivativeofsigmoid
gÂ¢=g(1-g) andg(z(3))=a(3) 11
Thus
Â¶s (z(3) )
       L a y e r L 1
+1
L a y e r L 2
1 = f Â¢ ( z ( 3 ) ) = a ( 3 ) (1 - a ( 3 ) )
b3(1)
b1(2)
3 3
(3) y Z2(3) a2 2
Layer L3
w(2) 1,2
   Â¶z(3) 1
111

 Multi-Layer Perceptron and Back-propagation
Remaining term
Â¶J Â¶Ã¬1Ã© 2Ã¹Ã¼
  w(1) 1,1
x1
x2 x3
+1
Layer L1
Ã«Ã» Â¶a Â¶z
z(2)a(2) w(2) 1 1 1,1
z(2) a2(2) 2
w(2) 1,3
z(2)a (2) 3 3
=   Ã­ Ãª(a(3)-y)ÃºÃ½ Â¶w(2) Â¶w(2)Ã®2 1 1 Ã¾
   12 12
     Z1(3)a1(3)
y1
=(a(3)-y) (3) (3) 11
    w(1) 1,3
1 1 Â¶z(3) Â¶w(2) 1 12
 ()()
=a(3)-y a(3) 1-a(3) a(2)
11112
     b3(1)
b1(2)
(3) y Z2(3) a2 2
Layer L3
  +1
Layer L2
w(2) 1,2

 Multi-Layer Perceptron and Back-propagation
 z(2)a w(2)
Â¶J = Â¶J Â¶a(3) Â¶z(3) 11
(2)
Â¶w Â¶a Â¶z Â¶w 12 1 1 12
    (1) w1,1
1
(2) (3) (3) (2)
1
1,1
w(2) Z1(3)a1(3) y1 = Â¶J Â¶z(3) =d(3) Â¶z(3)
       x1
x2 x3
(2) z2a2
1,2 11 Â¶z(3) Â¶w(2) 1 Â¶w(2)
112 12
where
d(3)o 1
     w(1) 1,3
(2)
w(2) 1,3
  z(2)
3 a3(2)
y2
Â¶J Â¶z(3)
1
    Z (3) a2(3) +1 3 b1(2) 2
b(1)
= Â¶J Â¶a(3) =(a(3) -y)fÂ¢(z(3)) 1
Â¶a(3) Â¶z(3) 1 1 1 11
  +1
Layer L2
Layer L3
  Layer L1

 Multi-Layer Perceptron and Back-propagation
 w(1) 1,1
x 1
x2 x3
+1
Layer L1
z(2) a1(2) 1
z(2) a2(2) 2
w(2) 1,3
(2)
z3 a3(2)
w(2) 1,1
(3)
w(2) Z1(3)a1 y1
The partial derivative
Â¶J =d(3) Â¶z(3) =d(3)a(2)
         1,2
Â¶w(2) 12
1 1 12 Â¶w(2)
12
  w(1) 1,3
Similarly, define
Â¶J
  d(3) o 2
    b3(1)
Z2(3) a2(3) Layer L3
y2
(3) 2
b1(2)
Â¶z
= Â¶J Â¶a(3) =(a(3) -y )fÂ¢(z(3))
    +1
Layer L2
2
Â¶a(3) Â¶z(3) 2 2 2
22

 Multi-Layer Perceptron and Back-propagation
w(1) 1,1
z(2)a1(2) 1
z(2) a2(2) 2
w(2) 1,3
z(2)a (2) 3 3
w(2) 1,1
w(2) 1,2
Â¶J Â¶J Â¶a(N) = i
=(a(N) -y)a(N)(1-a(N)) iiii
When the outermost layer is N
           x 1
(3) Z1(3) a1
y1
Â¶z(N) Â¶a(N) Â¶z(N)
i
and d(N) o
i i
  x 2
w(1) 1,3
x3 +1
Layer L1
Â¶J
 i Â¶z(N) i
 Additionally
Â¶J =(a(N) -y)a(N)(1-a(N))a(N-1) = Â¶J a(N-1)
  Â¶w(N-1) i i i i, j
=d(N)a(N-1) ij
i j
Â¶z(N) j i
     b3(1)
(3) y Z2(3) a2
Layer L3
2
b1(2)
  +1
Layer L2
 Note that these are only applicable at a layer just under the outermost layer N!

 Multi-Layer Perceptron and Back-propagation
â€¢ Similarly for bi(3)
Â¶JÂ¶Ã¬1Ã© 2 2Ã¹Ã¼
= Ã­ Ãª(a(3)-y)+(a(3)-y)ÃºÃ½ Â¶b(2) Ã®2Ã« 1 1 2 2 Ã»Ã¾
   (2) (2)
w(1) za1w(2)
1,1
z(2) a2(2) 2
w(1) (2) i i Â¶b(2) i
1,3 w i ii
1 1,1
Â¶ Ã¬1Ã© 2Ã¹Ã¼ = Ã­ Ãª(a(3)-y)ÃºÃ½
Â¶b(2) ii
       x1
x2 x3
Z(3)a1(3)
1 1 Â¶b(2)Ã®2Ã«iiÃ»Ã¾
y
  i
= (a(3) - y )Â¶a(3) = (a(3) - y )Â¶a(3) Â¶z(3) iii
     1,3
i Â¶z(3) Â¶b(2)
  z(2) (2) 3a3
=(a(3) -y)fÂ¢(z(3))=d(3) (3) y i i i i
   +1 b3(1) Layer L1
b1(2)
Z2(3) a2 2 Layer L3
  For a general case of outerlayer N
= Â¶J =(a(N)-y)fÂ¢(z(N))=d(N)
 Â¶b(N-1) i
iiii
 +1
Layer L2
w(2) 1,2

 Multi-Layer Perceptron and Back-propagation
â€¢ For an output node, the difference between the networkâ€™s activation and the true target value (ai(N) - yi) can be directly measured.
 â€¢Î´i(N)canbeusednowto
computethepartialderivative i Â¶z(N) i i i i
Sinced(N) = Â¶J i
   for weights on layer N-1
=(a(N) -y)a(N)(1-a(N)) Â¶J Â¶J Â¶z(N) Â¶z(N)
   Â¶w(N-1) i,j
i
i Â¶w(N-1)
Â¶J
Â¶b(N-1) i
Â¶z(N) Â¶w(N-1) i i,j
=d(N) i
= i =d(N)
(#%&) ! i,j!
(#) ğ›¿ğ‘
  
 Multi-Layer Perceptron and Back-propagation
â€¢ Consider a weight w(1) in layer 1 2,1
 w(1) z(2) a1(2) (2) 1,1 1 w
1Ã©22Ã¹
   w (1) 2,1
1,1 (2)
Ãª(a(3) -y ) +(a(3) -y )Ãº 2Ã«1 1 2 2Ã»
Â¶a(3) + (a(3) - y ) 2
function
      x1 x2
w (1) 1,3
w y1 1,2
J =
= (a(3) - y ) 1
Z1(3) a1(3)
  z(2) a2(2) 2
Â¶J Â¶w(1)
Â¶a(3) 1 1 Â¶w(1)
    w(2) 1,3
z(2)a (2) 3 3
b3(1) +1
2 2 Â¶w(1) 21 21 21
   x3 +1
(3) Â¶a(3) Â¶z(3) (3) Â¶a(3) Â¶z(3) =(a -y) 1 1 +(a -y) 2 2
       b1(2)
y
2
1 1 Â¶z(3) Â¶w(1) 2 2 Â¶z(3) Â¶w(1) 1 21 2 21
Z2(3) a2(3)
 12 =(a(3) -y)fÂ¢(z(3))Â¶z(3) +(a(3) -y )fÂ¢(z(3))Â¶z(3)
   Layer L1
Layer L2
â€¢ Note the influence of w (1) to the loss
2,1
 1 1 1 Â¶w(1) 2 2 2 Â¶w(1)
LayerL 21 21 3

 Multi-Layer Perceptron and Back-propagation
Â¶z(3) = Â¶ Ã©w(2)a(2) +w(2)a(2) +w(2)a(2) +b(2) Ã¹
1 Â¶w(1)
Â¶w(1)Ã«11 1 12 2 21
13 3 1 Ã» Â¶a(2) Â¶z(2)
=w(2) 2
12 Â¶z(2) Â¶w(1)
   w(1) 1,1
z(2) a1(2) 1
w(2) 1,1
w(2) 1,2
21
=
=w(2) fÂ¢(z(2))x
Â¶ Â¶a(2) Ã©w(2)a(2) Ã¹ =w(2) 2
      x1
x2
Â¶w(1) Ã« 12 2 Ã» 12 Â¶w(1) 12 2 1
2 21 21221
      w (1) 2,1
Z1(3) a1(3)
 z(2) a2(2) 2
 w(1) 1,3
x 3
+1
Layer L1
w(2) 1,3
z(2) (2) 3a3
b1(2)
Similarly
Â¶z(3) Â¶ 2 =
Ã©w(2)a(2) +w(2)a(2) +w(2)a(2) +b(2) Ã¹
   (1) (1)Ã«21 1 Â¶w21 Â¶w21
22 2
23 3 2 Ã» Â¶a(2) Â¶z(2)
    b3(1)
Â¶a(2) 2
Z2(3) a2(3)
 +1
Layer L2
2
LayerL3
Â¶
Â¶w(1) Ã« 22 2 Ã» 22 Â¶w(1)
=w(2) fÂ¢(z(2))x 22 2 1
 =
=w(2) 2
22 Â¶z(2) Â¶w(1)
Ã©w(2)a(2) Ã¹ =w(2)
21 21221
    
 Multi-Layer Perceptron and Back-propagation
 w(1) 1,1
x1 x2
w(1) 1,3
x3 +1
Z1(3)a1(3) y1 w(2)
1,3
(3) a (3) y2 Z2 2
(3) (3) (2) (2)
Â¶J =(a -y)fÂ¢(z )w fÂ¢(z )x
+(a(3) - y ) f Â¢(z(3) )w(2) f Â¢(z(2) )x 2 2 2 22 2 1
=d(3)w(2) fÂ¢(z(2) )x +d(3)w(2) fÂ¢(z(2) )x 1 12 2 1 2 22 2 1
z(2) 1
z(2) a2(2) 2
z(2)a (2) 33
b3(1)
+1 Layer L2
a (2)
1 w(2)
1,1
w(2) 1,2
Putting them together
        w (1) 2,1
  Â¶w
(1) 21
1 1 1 12 2 1
      b1(2)
  Layer L1
Layer L3
( )()
= d(3)w(2) +d(3)w(2) fÂ¢ z(2) x
1 12 2 22 2 1

 Multi-Layer Perceptron and Back-propagation
From the previous derivation
Â¶J Â¶w (1)
(
)Â¢( )
= d(3)w(2) +d(3)w(2) f z(2) x
1 12 2 22 2 1 Can be shown similarly
Â¶J (
= d (3)w(2) +d (3)w(2)
 21
   x1 w(2) 1,2
z(2) a2(2) 2
Z1(3) a1(3)
 w (1) 2,1
   f Â¶b(1) 1 12 2 22 2
z(2) )Â¢( )
   w(2) 2,2
2
Notice the flow of information from upper layer to the lower layer
 b(1) 2
  Z 2(3) (3)a
Let d (2) = d (3)w(2) +d (3)w(2) f z(2)
( )Â¢( )
2
2 1 12 2 22 2
+1 Ã¥
=2 d(3)w(2)fÂ¢(z(2))
j j2 2 j=1
LayerL1 LayerL2 LayerL Thus Â¶J =d(2)x , Â¶J =d(2) 3 Â¶w (1) 2 1 Â¶b(1) 2
  21 2

 Multi-Layer Perceptron and Back-propagation
â€¢ Thegradientdescentupdaterule can be applied now
 w(1) :=w(1) -a Â¶J =w(1) -ad(2)x 21 21 Â¶w(1) 21 21
21
 b(1) :=b(1) -ad(2) 222
   x1
w(2) Z1(3) a1(3) 1,2
    z(2) a2(2) 2
  +1
w (1) 2,1
w(2) 2,2
2
j=1
d ( 2 ) = Ã¥2 d ( 3 ) w ( 2 ) f Â¢ ( z ( 2 ) )
Ã¦ Sl+1 Ã¶
d(l) =Ã§Ã¥d(l+1)w(l)Ã·fÂ¢(z(l))
i Ã¨ j jiÃ¸ i j=1
Where Sl+1 = Number of nodes on layer l +1
j j2 2 More generally
 b(1) 2
  Layer L1
Layer L2
Layer L3
Z2(3) a2(3)

 Multi-Layer Perceptron and Back-propagation
     x1
More generally z(2) a2(2)
2
Â¶J =d(l)a(l-1) , Â¶J =d(l) Â¶w(l-1) i j Â¶b(l-1) i
ij i
  +1
w (1) 2,1
w(2) 2,2
Where Sl+1 = Number of nodes on layer l +1
w(2) 1,2
Z1(3) a1(3)
Ã¦ Sl+1 Ã¶
di(l) =Ã§Ã¥d(l+1)w(l) fÂ¢(z(l))
  j jiÃ·i Ã¨j=1 Ã¸
   b(1) 2
  Layer L1
Layer L2
Layer L3
Z2(3) a2(3)

 Multi-Layer Perceptron and Back-propagation
 w(1) 1,1
â€¢ Letâ€™s train starting the outer layer at N=3 z(2) a1(2) w(2)
1 1,1
    Â¶J
Â¶z(3) i
=d(3) =(a(3) -y)a(3)(1-a(3)) iiiii
     x1
x2 x3
w(2) 1,2
 w (1) 2,1
Â¶J =d(3)a(2), Â¶J =d(3) Â¶w(2) i j Â¶b(2) i
i, j i
    w(1) 1,3
w(2) 1,3
z(2)a (2) 33
b1(2) +1
Layer 2
  Â¶J Â¶w(2)
i, j Â¶J
Â¶b(2) i
=d(3)a(2) =(a(3) -y)a(3)(1-a(3))a(2) ijiiiij
  =(a(3) -y)a(3)(1-a(3)) iiii
 b3(1) +1
z(2) a2(2) 2
Z1(3) a1(3)
    Z2(3) a2(3)
  Layer 1
Layer 3

 Multi-Layer Perceptron and Back-propagation
â€¢ Training the outer layer at N=3
Â¶J Â¶w(2)
11
Â¶J =d(3)a(2) =(a(3) -y)a(3)(1-a(3))a(2)
Â¶w(2) 1 2 1 1 1 1 2 12
= d (3)a(2) = (a(3) - y )a(3) (1- a(3) )a(2) 1111111
  (1) w1,1
z(2) a1(2) 1
w(2) 1,1
w(2) 1,2
        x1
Â¶J Â¶w(2)
13
Â¶J Â¶w(2)
21
Â¶J Â¶w(2)
= d (3)a(2) = (a(3) - y )a(3) (1- a(3) )a(2) 1311113
 w (1) 2,1
Z1(3) a1(3)
  z(2) a2(2) 2
 x 2
w(1) 1,3
x3
(1) +1 b3
Layer 1
w(2) 1,3
=d(3)a(2) =(a(3) -y )a(3)(1-a(3))a(2) 2122221
=d(3)a(2) =(a(3) -y )a(3)(1-a(3))a(2) 2222222
=d(3)a(2) =(a(3) -y )a(3)(1-a(3))a(2) 2322223
=(a(3) -y)a(3)(1-a(3)) 1111
1
 Â¶J Â¶w(2)
    z(2)a (2)
33 22
  +1
Layer 2
Layer 3
b1(2)
23
Â¶J Â¶b(2)
Z2(3) a2(3)
    Â¶J =(a(3) -y )a(3)(1-a(3))
 Â¶b(2) 2
2222

 Multi-Layer Perceptron and Back-propagation
â€¢ Training the inner layer at 2 w(1) z(2) a1(2) w(2)
Â¶J =d(l)a(l-1) , Â¶J =d(l) Â¶w(l-1) i j Â¶b(l-1) i
ij i
Ã¦ Sl+1 Ã¶
d(l) =Ã§Ã¥d(l+1)w(l)Ã·fÂ¢(z(l))
   1,1 1 1,1
       x ijjii
 1
x2 x3
w(2) 1,2
Ã¨ j=1 Ã¸
Where Sl+1 = Number of nodes on layer l +1
w (1) 2,1
z(2) a2(2) 2
Z1(3) a1(3)
  w(1) 1,3
w(2) 1,3
z(2)
3 a3(2)
b1(2)
l = 2, Sl +1 = Nodes at layer (l + 1 = 3) : 2 Ã¦2 Ã¶
d(2) =Ã§Ã¥d(3)w(2)Ã·fÂ¢(z(2)) i Ã¨ j ji Ã¸ i
     b3(1) +1
j=1
d(2) =d(3)w(2) fÂ¢(z(2) )+d(3)w(2) fÂ¢(z(2) )
Z2(3) a2(3)
  LayerL1
+1
LayerL2
Layer L3
1 1 11 1 2 21 1
d(2) =d(3)w(2) fÂ¢(z(2) )+d(3)w(2) fÂ¢(z(2) ) 2 1 12 2 2 22 2
d(2) =d(3)w(2) fÂ¢(z(2) )+d(3)w(2) fÂ¢(z(2) ) 3 1 13 3 2 23 3

 Multi-Layer Perceptron and Back-propagation
â€¢ Training the inner layer at 1
Â¶J =d(2)a(1) , Â¶J =d(2)
   w(1) 1,1
x1 x2
w(1) 1,3
1
1,1 w(2)
z(2)a1(2) w(2)
Â¶w (1) ij
i j Â¶b(1) i i
    = d(2)a(1) , Â¶J = d(2)a(1)
Â¶J
Â¶w(1) 11 Â¶w(1) 21 Â¶w(1) 31
= d(2)a(1) , Â¶J
1,2 11 21 31
    w (1) 2,1
Z1(3) a1(3)
    z(2) a2(2) 2
       x3 +1
w(2) 1,3
Â¶J = d(2)a(1) , Â¶J = d(2)a(1) , Â¶J = d(2)a(1) Â¶w(1) 12 Â¶w(1) 22 Â¶w(1) 32
12 22 32
Â¶J =d(2)a(1) , Â¶J =d(2)a(1) , Â¶J =d(2)a(1) Â¶w(1) 13 Â¶w(1) 23 Â¶w(1) 33
13 23 33 Â¶J =d(2), Â¶J =d(2), Â¶J =d(2)
LayerL3 Â¶b(1) 1 Â¶b(1) 2 Â¶b(1) 3 123
z(2)a (2) 33
  Z2(3) a2(3)
   b3(1)
b1(2)
     Layer L1
+1
Layer L2

Multi-Layer Perceptron and Back-propagation
 â€¢ Update weights from backpropagation
Ã¦ Sl+1 Ã¶
d(l) =Ã§Ã¥d(l+1)w(l)Ã·fÂ¢(z(l))
i Ã¨ j jiÃ¸ i j=1
Where Sl+1 = Number of nodes on layer l +1
w(l) :=w(l) -a Â¶ J(w,b)
  i, j i, j
Â¶w(l) i, j
b(l) :=b(l) -a Â¶ J(w,b)
 ii
Â¶b(l) i
 Since d (l+1) = i
Â¶J
Â¶w(l) i, j
Â¶J Â¶z(l+1)
 i
Â¶J Â¶z(l+1)
Â¶z(l+1) = i =d(l+1) i
    Â¶z(l+1) Â¶w(l) i Â¶w(l) i i, j i, j
 w(l) :=w(l) -a(a(l)d(l+1)) i,j i,j ji
b(l) :=b(l) -ad(l+1) iii
where a = learning rate

Multi-Layer Perceptron and Back-propagation
â€¢ Partial derivatives of the loss function J(w, b) for a single instance (x, y):
 Â¶ Â¶w(l)
Â¶ J(w,b;x(i),y(i)) Â¶b(l)
J(w,b;x(i),y(i)) and
â€¢ As such, we can compute batch gradients as
J (w,b; x(k ) , y(k ) ) J (w,b; x(k ) , y(k ) )
  i, j
i
Â¶1mÂ¶ J (w,b) = Ã¥
  Â¶w(l) m k=1 Â¶w(l) i, j i, j
Â¶ 1Ã¥Â¶ J (w,b) = m
  Â¶b(l) m k=1 Â¶b(l) ii

 Developing an MLP with one hiddern layer for a classification task
â€¢ Asinthepreviousderivation,
N=3, l1 is the input layer and
l isthehiddenlayer. 2
â€¢ InputxhasPdimensions
â€¢ Hidden layer l2 has Q nodes.
â€¢ The output layer N has R nodes.
â€¢ Perform a feedforward pass,
computing the activations for
layersl andN. 2
z(2) =w(1)x +w(1)x +w(1)x +.......+w(1)x +b(1) 1 11 1 12 2 13 3 1P P 1
z(2) =w(1)x +w(1)x +w(1)x +.......+w(1)x +b(1) 2 211 222 233 2PP 2
z(2) =w(1)x +w(1)x +w(1)x +.......+w(1)x +b(1) 3 311 322 333 3PP 3
.........
z(2) =w(1) x +w(1) x +w(1) x +.......+w(1) x +b(1)
Q M11 M22 M33 ï²
QPP Q
ï²ï²
 z(2) =w(1)x+b(1) ï€¥
Ã©w(1) w(1) Ãª1Ãº Ãª11 12
w(1)Ã¹
1PÃº 1
Ã©Ã¹ s(z(2))
Ã©z(2)Ã¹
(2) (1)(1) (1)ÃªÃº(2)Ãª(2)Ãº
Ã©x Ã¹
ï²ÃªÃºï€¥Ãª Ãºï²ÃªÃºï²ÃªÃºÃª Ãº
. .
Ãªz2 Ãº Ãªw21 w22 . . w2PÃº Ãªx2Ãº Ãªa2 Ãº s(z2 )
z(2) =Ãª . Ãº w(1) =Ãª . . . . . Ãº x=Ãª.Ãº a(2) =Ãª . Ãº=Ãª . Ãº Ãª.ÃºÃª.....ÃºÃª.ÃºÃª.ÃºÃª.Ãº
ÃªÃºÃª ÃºÃªÃºÃªÃºÃªÃº
z(2) w(1) . . . w(1) x
a(2) Ãª (2) Ãº Ã«QÃ» Ã«Q1 QPÃ»Ã«PÃ» Ã«QÃ»s(z)
Ã©a(2)Ã¹ Ãª1ÃºÃª1Ãº
Ã«QÃ»

 Developing an MLP with one hiddern layer for a classification task
 z(3) =w(2)a(2) +w(2)a(2) +..+w(2)a(2) +b(2) 1111122 13Q1
z(3) =w(2)a(2) +w(2)a(2) +..+w(2)a(2) +b(2) 2211222 23Q2
. .
z(3) =w(2)a(2) +w(2)a(2) +..+w(2)a(2) +b(2) RR11R22 R3QR
â€¢ Continue the forward pass calculation to the output layer
â€¢ Keep all the activation values for later backpropagation calculations
ï²ï²
ï²
z(3) =w(2)a(2) +b(2) ï€¥
(3) Ã©Ã¹
Ã©z1 Ã¹ ÃªÃº
(3) s (z(3) ) Ã©w(2) w(2) . . w(2)Ã¹ Ã©a Ã¹ Ãª 1 Ãº
Ãª1112 1QÃº 1
(2) (2) (2) ÃªÃºÃª Ãº z(3) w w ..w a(3) s(z(3))
ï² Ãª2ÃºÃ©z(3)Ã¹ Ãª21 22 2QÃº ï² Ãª2ÃºÃª 2 Ãº ÃªÃºï€¥ÃªÃºÃªÃº
z(3)= . Ãª1 Ãº w(2)= . . . . . a(3)= . =Ãª . Ãº ÃªÃºz(3) Ãª Ãº ÃªÃº
Ãª.ÃºÃ«2Ã»Ãª.....Ãº Ãª.ÃºÃª.Ãº (3) Ãª Ãº ÃªÃºÃªÃº
Ãªz Ãº w(2) w(2) . . w(2) a(3) Ãª (3) Ãº
Ã«RÃ» Ã«R1 R2 RQÃ» Ã«RÃ» s(z) Ã«RÃ»

 Developing an MLP with one hiddern layer for a classification task
â€¢ For each output unit i in layer L3 (the output layer), set d(N) =(a(N) -y)a(N)(1-a(N))
d(3) =(a(3) -y)a(3)(1-a(3)) iiiii
â€¢ For the hidden layer l2 , set
Note S3 is the number of nodes at layer N(=3)
iiiii Since N =3
Ã¥S3 Â¢()Ã¥S3 ()
d(2) = d(3)w(2) f z(2) = d(3)w(2)a(2) 1-a(2)
1
j j1 1
j=1 j=1
jj11 1
d (2) = 2
Ã¥S3
d (3)w(2)a(2) 1- a(2) ()
jj22 2 j=1
As noted earlier, you need the activation values for backpropagation

 Developing an MLP with one hiddern layer for a classification task
â€¢ Update the weights â€¢ For layer l2
Â¶J
Â¶w(2) i, j
=d(3)a(2) =(a(3) -y)a(3)(1-a(3))a(2) ijiiiij
  â€¢ For layer l1
Â¶J =d(2)a(1) , Â¶J =d(2) Â¶w (1) i j Â¶b(1) i
ij i
Â¶J
Â¶b(2) i
=d(3) =(a(3) -y)a(3)(1-a(3)) iiiii
   d (2) = i
S3 Ã¥
d (3)w(2)a(2) 1- a(2) ()
jjii i j=1
   
 Developing an MLP with one hiddern layer for a classification task
â€¢ Project 1
â€¢ MNIST digit handwritten dataset
  from sklearn.datasets import fetch_openml mnist = fetch_openml('mnist_784', version=1) mnist.keys()
#%%
import numpy as np
X, y = mnist["data"], mnist["target"] y = y.astype(np.uint8)
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]

 Developing an MLP with one hiddern layer for a classification task
â€¢ 70,000 images of 28 x 28 pixels
â€¢ Each image labeled as 0 - 9
â€¢ Develop an ANN classifier for MNIST dataset with one hidden layer
â€¢ Input dimension = 28 x 28 = 784
â€¢ Output dimension = 10
â€¢ Thus,P=784,Q=?,R=10
â€¢ Thus the output has to be sent to a softmax activation to classify 10 different digit classes.
â€¢ Recommend using stochastic gradient descent instead of batch gradient
 
 Project 1 Specifics
â€¢ Use the MNIST data samples for training and testing. The python code posted on the blackboard will let you download MNIST. Develop the classifier code to run on Google Colab (https://colab.research.google.com/notebooks/intro.ipynb#recent=true ) so that you donâ€™t have to worry about setting up your machine correctly.
â€¢ Develop a code Python using the code provided as the starting point to design a neural network to perform 10 digit classification.
â€¢ Experiment with 3 different hidden units and see the performance differences.
â€¢ Summarize the results and report them. Include the code you ran in the report.
 
 Project Specifics
â€¢ Submit (upload your files to the Blackboard) your code by uploading the original python file (*.py) after changing the extension to â€œ*.py.txtâ€
â€¢ Submit the project report via the Blackboard
â€¢ Use L2 norm as the loss function. Also use 10 dimensional vector as the output for the
digit classification.
â€¢ Key Elements to be included in the report (I will be grading the report based on these elements).
â€¢ Describeyournetworkdesignandhyperparameters â€¢ Number of layers
â€¢ Number of nodes in each layer â€¢ Activation functions used
â€¢ Learning rate
â€¢ Describetheforwardpassofyourcode
â€¢ Describethebackpropagationofyourcode
â€¢ Includelearningcurves:bothtrainingandvalidationlosses
â€¢ Describethevalidationprocessused
â€¢ Describethecriteriayouusedtoterminatethetraining(whenandwhy). â€¢ Predicttheperformanceofthecodeonanunseendataset.
â€¢ Sourcecodetobealsoincludedasapartofthereport.

 Project Specifics
â€¢ To get the full credit of the project, I should be able to run your code on Google Colab. So, please verify that it runs.
â€¢ Fully describe what you have done in the code. Include comments on the code so that I can understand what you did.
â€¢ Do NOT use any of Keras, TensorFlow library classifiers. I would like you to build a neural network classifier from the grounds up. You may use other functions within python or ScikitLearn, but your core code should be done by python including forward pass and backpropagation.
